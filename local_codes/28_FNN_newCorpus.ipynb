{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b58a335c-334c-4774-a47a-881b3352ea12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "../checkpoints/Finetune_ESM_FNN_checkpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alireza_noroozi/Finetune_ESM_FNN/runs/o444wir5?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7b973ae37010>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import json\n",
    "from transformers import EsmModel, AutoTokenizer\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from batch import Batch\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 100_000\n",
    "val_epoch = 100\n",
    "num_val = 10\n",
    "batch_size = 8\n",
    "dataset_name = \"new_corpus\"\n",
    "lr = 0.001\n",
    "model_name = \"Finetune_ESM_FNN\"\n",
    "max_seq_len = 500\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{dataset_name}\", batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": model_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_szie\": batch_size,\n",
    "        \"max_seq_len\": max_seq_len\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e229e48c-c96e-42c6-9468-d296236b0518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67486\n"
     ]
    }
   ],
   "source": [
    "tax_ids_file = \"../data/tax_ids.csv\"\n",
    "\n",
    "tax_ids = pd.read_csv(tax_ids_file)\n",
    "# print(tax_ids)\n",
    "num_classes = len(tax_ids) + 1\n",
    "print(num_classes)\n",
    "id_encoder = {name: idx + 1 for idx, name in enumerate(tax_ids['Taxonomic_lineage_IDs'].values)}\n",
    "\n",
    "id_decoder = {idx + 1: name for idx, name in enumerate(tax_ids['Taxonomic_lineage_IDs'].values)}\n",
    "id_decoder[0] = \"NOT DEFINED\"\n",
    "\n",
    "# Character vocabulary for protein sequences (20 amino acids + 1 padding)\n",
    "vocab = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(vocab)}  # Start index from 1 for padding\n",
    "# Sequence encoder: Convert the protein sequence into integers\n",
    "def encode_sequence(sequence):\n",
    "    return [char_to_idx.get(char, 0) for char in sequence] + [0 for _ in range(max_seq_len - len(sequence))]  # 0 for unknown characters or padding \n",
    "\n",
    "def data_to_tensor_batch(b):\n",
    "    inputs = torch.LongTensor([encode_sequence(e['sequence']) for e in b])\n",
    "    tax_ids = torch.LongTensor([id_encoder.get(e['Taxonomic_lineage_IDs'], 0) for e in b])\n",
    "\n",
    "    return Batch(inputs, tax_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ebcdf2b-8d60-41d4-af40-e8dd0e07e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ESM2(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim=1280, vocab_size=21):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.layer1 = nn.Linear(embedding_dim, 512)\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input `x` is of shape (batch_size, sequence_length)\n",
    "        embeddings = self.embedding(x)  # Shape: (batch_size, sequence_length, embedding_dim)\n",
    "        outputs = self.layer1(embeddings)  # Shape: (batch_size, sequence_length, 512)\n",
    "        outputs = self.layer_norm(outputs)  # Normalize across feature dimensions\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "\n",
    "        # Pooling across the sequence dimension to get a single vector per sequence\n",
    "        pooled_output = torch.mean(outputs, dim=1)  # Shape: (batch_size, 512)\n",
    "\n",
    "        # Final classification layer\n",
    "        logits = self.layer2(pooled_output)  # Shape: (batch_size, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc32d340-2117-4019-b644-2cab007728d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 35.304094 M\n",
      "ESM2(\n",
      "  (embedding): Embedding(21, 1280)\n",
      "  (layer1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layer2): Linear(in_features=512, out_features=67486, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ESM2(num_classes).to(device)\n",
    "\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total parameters: {total/ 1e6} M')\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Initial restart interval\n",
    "    T_mult=2,  # Multiply interval by 2 after each restart\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beb78789-231b-4972-896c-43c06da3c1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([62847,  4563, 59752, 62848, 15365, 41312, 10661, 50520, 58202, 29003,\n",
      "        47514, 33664, 56666, 21453, 30594, 64470, 64471, 58203, 65961, 21454,\n",
      "            1,  9151, 32139,  6074, 35209, 16872, 44417, 15366, 59753, 53560,\n",
      "        44418, 59754, 16873, 16874,  4564, 59755, 18436,  6075, 27445, 62847,\n",
      "        59756, 53561, 24425, 39812, 62849, 13836, 61322,     2, 29004,  3028,\n",
      "        35210,  1509, 65962, 53562,  9152, 27446, 61323, 50521, 47515, 55106,\n",
      "        61324,  4564, 27447, 41313, 50522,  1510, 10662, 33665, 41314, 12229,\n",
      "        42858, 49064, 18437, 32140, 33666, 58204, 32141, 64472, 33667, 32142])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     avg_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m num_val\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, accuracy, f1_micro, f1_macro\n\u001b[0;32m---> 45\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 37\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_labels)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Compute evaluation metrics (example: accuracy, F1 score)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(\u001b[43mall_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, all_preds\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     38\u001b[0m f1_macro \u001b[38;5;241m=\u001b[39m f1_score(all_labels\u001b[38;5;241m.\u001b[39mnumpy(), all_preds\u001b[38;5;241m.\u001b[39mnumpy(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# F1-score for multi-label classification\u001b[39;00m\n\u001b[1;32m     39\u001b[0m f1_micro \u001b[38;5;241m=\u001b[39m f1_score(all_labels\u001b[38;5;241m.\u001b[39mnumpy(), all_preds\u001b[38;5;241m.\u001b[39mnumpy(), average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# F1-score for multi-label classification\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "val_dir = f\"val_results/{model_name}\"\n",
    "if not os.path.exists(val_dir):\n",
    "    os.makedirs(val_dir)\n",
    "    \n",
    "val_batches = [da.get_batch() for _ in range(num_val)]\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "            tensor_batch = data_to_tensor_batch(val_batches[epoch])\n",
    "            tensor_batch.gpu(device)\n",
    "        \n",
    "            labels = tensor_batch.taxes\n",
    "            outputs = model(tensor_batch.seq_ids)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "                \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches into single tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    print(all_labels)\n",
    "    # Compute evaluation metrics (example: accuracy, F1 score)\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    f1_macro = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')  # F1-score for multi-label classification\n",
    "    f1_micro = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')  # F1-score for multi-label classification\n",
    "    # conf_matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "    avg_loss = running_loss / num_val\n",
    "    \n",
    "    return avg_loss, accuracy, f1_micro, f1_macro\n",
    "\n",
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba226938-d105-4ca0-888f-36c42f24ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def load_checkpoint(model, optimizer=None, scheduler=None):\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pt'))        \n",
    "    # Extract epoch numbers and find latest\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load optimizer state if provided (for training)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Move optimizer state to GPU if necessary\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get training metadata\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "    \n",
    "    print(f\"Successfully loaded checkpoint from epoch {epoch}\")\n",
    "    # print(\"Metrics at checkpoint:\", metrics)\n",
    "    \n",
    "    return model, optimizer, scheduler, epoch, metrics\n",
    "        \n",
    "\n",
    "# model, optimizer, scheduler, latest_epoch, metrics = load_checkpoint(model, optimizer, scheduler)\n",
    "latest_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b1591-7164-4cd1-b328-0268291bf092",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 98/100000 [01:23<23:50:12,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100000]\n",
      "Train Loss: 10.9839\n",
      "test Loss: 11.0489, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 198/100000 [03:19<23:45:52,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/100000]\n",
      "Train Loss: 10.7905\n",
      "test Loss: 10.5840, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 298/100000 [05:11<23:42:55,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/100000]\n",
      "Train Loss: 10.6935\n",
      "test Loss: 10.3726, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 398/100000 [06:56<23:38:15,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/100000]\n",
      "Train Loss: 10.6458\n",
      "test Loss: 10.3708, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 498/100000 [08:43<23:42:00,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/100000]\n",
      "Train Loss: 10.5916\n",
      "test Loss: 10.2280, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 698/100000 [12:21<23:36:50,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [700/100000]\n",
      "Train Loss: 10.5770\n",
      "test Loss: 10.2566, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 798/100000 [14:08<23:33:19,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [800/100000]\n",
      "Train Loss: 10.5264\n",
      "test Loss: 10.2511, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 898/100000 [15:59<23:31:48,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [900/100000]\n",
      "Train Loss: 10.5826\n",
      "test Loss: 10.1532, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 998/100000 [17:43<23:38:26,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/100000]\n",
      "Train Loss: 10.4992\n",
      "test Loss: 10.1603, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1098/100000 [19:32<23:21:34,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1100/100000]\n",
      "Train Loss: 10.4366\n",
      "test Loss: 10.1345, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1198/100000 [21:18<23:31:32,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1200/100000]\n",
      "Train Loss: 10.4988\n",
      "test Loss: 10.1922, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1298/100000 [23:05<23:31:24,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1300/100000]\n",
      "Train Loss: 10.4969\n",
      "test Loss: 10.1520, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1398/100000 [24:53<23:14:23,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1400/100000]\n",
      "Train Loss: 10.3082\n",
      "test Loss: 10.1369, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1598/100000 [28:36<23:20:35,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1600/100000]\n",
      "Train Loss: 10.2984\n",
      "test Loss: 10.0398, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2498/100000 [44:58<23:08:20,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2500/100000]\n",
      "Train Loss: 10.2772\n",
      "test Loss: 9.9119, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2598/100000 [46:41<23:08:41,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2600/100000]\n",
      "Train Loss: 10.3002\n",
      "test Loss: 9.9121, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2698/100000 [48:23<23:11:16,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2700/100000]\n",
      "Train Loss: 10.2658\n",
      "test Loss: 9.9387, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2798/100000 [50:07<23:10:33,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2800/100000]\n",
      "Train Loss: 10.2264\n",
      "test Loss: 9.9310, test Accuracy: 0.0125\n",
      "test F1 (micro): 0.0125, test F1 (macro): 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2898/100000 [51:52<23:05:58,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2900/100000]\n",
      "Train Loss: 10.3502\n",
      "test Loss: 9.9088, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2998/100000 [53:43<22:59:48,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3000/100000]\n",
      "Train Loss: 10.3410\n",
      "test Loss: 9.8685, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3098/100000 [55:31<22:52:33,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3100/100000]\n",
      "Train Loss: 10.2105\n",
      "test Loss: 9.9348, test Accuracy: 0.0125\n",
      "test F1 (micro): 0.0125, test F1 (macro): 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3198/100000 [57:22<22:56:18,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3200/100000]\n",
      "Train Loss: 10.2321\n",
      "test Loss: 9.8927, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3298/100000 [59:10<22:59:41,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3300/100000]\n",
      "Train Loss: 10.3060\n",
      "test Loss: 9.9776, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3398/100000 [1:00:57<22:55:42,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3400/100000]\n",
      "Train Loss: 10.1084\n",
      "test Loss: 9.8358, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3498/100000 [1:02:46<22:59:27,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3500/100000]\n",
      "Train Loss: 10.2265\n",
      "test Loss: 9.9052, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3598/100000 [1:04:45<22:59:22,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3600/100000]\n",
      "Train Loss: 10.3402\n",
      "test Loss: 9.9393, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3698/100000 [1:06:34<22:53:06,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3700/100000]\n",
      "Train Loss: 10.1987\n",
      "test Loss: 9.8702, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3798/100000 [1:08:21<22:52:52,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3800/100000]\n",
      "Train Loss: 10.2467\n",
      "test Loss: 9.9193, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3898/100000 [1:10:06<22:54:20,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3900/100000]\n",
      "Train Loss: 10.3373\n",
      "test Loss: 9.9886, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3998/100000 [1:11:54<22:40:42,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4000/100000]\n",
      "Train Loss: 10.2399\n",
      "test Loss: 9.8428, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4098/100000 [1:13:41<22:46:31,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4100/100000]\n",
      "Train Loss: 10.2230\n",
      "test Loss: 9.8572, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4198/100000 [1:15:25<22:36:26,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4200/100000]\n",
      "Train Loss: 10.2443\n",
      "test Loss: 9.8650, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4298/100000 [1:17:11<23:02:30,  1.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4300/100000]\n",
      "Train Loss: 10.2075\n",
      "test Loss: 9.8768, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4398/100000 [1:19:01<22:45:16,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4400/100000]\n",
      "Train Loss: 10.2353\n",
      "test Loss: 9.8379, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4498/100000 [1:20:46<22:39:32,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4500/100000]\n",
      "Train Loss: 10.2091\n",
      "test Loss: 9.8507, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4598/100000 [1:22:37<22:46:02,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4600/100000]\n",
      "Train Loss: 10.1832\n",
      "test Loss: 9.8413, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4698/100000 [1:24:30<22:40:11,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4700/100000]\n",
      "Train Loss: 10.2155\n",
      "test Loss: 9.8500, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4798/100000 [1:26:24<22:40:32,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4800/100000]\n",
      "Train Loss: 10.1529\n",
      "test Loss: 9.8459, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4898/100000 [1:28:09<22:31:48,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4900/100000]\n",
      "Train Loss: 10.2520\n",
      "test Loss: 9.8512, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4998/100000 [1:30:03<22:25:56,  1.18it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5000/100000]\n",
      "Train Loss: 10.2844\n",
      "test Loss: 9.8563, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5098/100000 [1:31:51<22:37:43,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5100/100000]\n",
      "Train Loss: 10.1657\n",
      "test Loss: 9.8549, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5198/100000 [1:33:38<22:31:31,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5200/100000]\n",
      "Train Loss: 10.2654\n",
      "test Loss: 9.8551, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5298/100000 [1:35:25<22:29:26,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5300/100000]\n",
      "Train Loss: 10.2141\n",
      "test Loss: 9.8808, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5398/100000 [1:37:12<22:23:32,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5400/100000]\n",
      "Train Loss: 10.2586\n",
      "test Loss: 9.9591, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5498/100000 [1:38:58<22:24:38,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5500/100000]\n",
      "Train Loss: 10.2305\n",
      "test Loss: 9.8920, test Accuracy: 0.0250\n",
      "test F1 (micro): 0.0250, test F1 (macro): 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5598/100000 [1:40:47<22:24:21,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5600/100000]\n",
      "Train Loss: 10.2416\n",
      "test Loss: 9.8798, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5698/100000 [1:42:35<22:26:40,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5700/100000]\n",
      "Train Loss: 10.0899\n",
      "test Loss: 9.8406, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5798/100000 [1:44:23<22:36:22,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5800/100000]\n",
      "Train Loss: 10.2330\n",
      "test Loss: 9.9097, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5898/100000 [1:46:12<22:24:46,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5900/100000]\n",
      "Train Loss: 10.1574\n",
      "test Loss: 9.9145, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5998/100000 [1:48:06<22:15:16,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6000/100000]\n",
      "Train Loss: 10.2457\n",
      "test Loss: 9.8572, test Accuracy: 0.0250\n",
      "test F1 (micro): 0.0250, test F1 (macro): 0.0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6098/100000 [1:49:55<22:18:36,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6100/100000]\n",
      "Train Loss: 10.0967\n",
      "test Loss: 9.8741, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6198/100000 [1:51:42<22:16:54,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6200/100000]\n",
      "Train Loss: 10.2032\n",
      "test Loss: 9.9327, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6298/100000 [1:53:31<22:17:31,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6300/100000]\n",
      "Train Loss: 10.2815\n",
      "test Loss: 9.9015, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6398/100000 [1:55:18<22:11:22,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6400/100000]\n",
      "Train Loss: 10.2280\n",
      "test Loss: 9.9432, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6498/100000 [1:57:05<22:17:43,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6500/100000]\n",
      "Train Loss: 10.1315\n",
      "test Loss: 9.8989, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6598/100000 [1:58:56<22:15:26,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6600/100000]\n",
      "Train Loss: 10.2961\n",
      "test Loss: 9.9240, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6698/100000 [2:00:42<22:10:21,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6700/100000]\n",
      "Train Loss: 10.2215\n",
      "test Loss: 9.9514, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6798/100000 [2:02:32<22:04:41,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6800/100000]\n",
      "Train Loss: 10.2397\n",
      "test Loss: 9.9354, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6898/100000 [2:04:25<22:05:18,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6900/100000]\n",
      "Train Loss: 10.1755\n",
      "test Loss: 9.9303, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6998/100000 [2:06:09<22:04:58,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7000/100000]\n",
      "Train Loss: 10.1580\n",
      "test Loss: 9.9841, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7098/100000 [2:07:57<22:04:53,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7100/100000]\n",
      "Train Loss: 10.1968\n",
      "test Loss: 9.9468, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 11398/100000 [3:25:49<21:03:39,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11400/100000]\n",
      "Train Loss: 10.1088\n",
      "test Loss: 10.0872, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 11498/100000 [3:27:36<21:05:29,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11500/100000]\n",
      "Train Loss: 10.1869\n",
      "test Loss: 10.1001, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11598/100000 [3:29:22<20:58:33,  1.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11600/100000]\n",
      "Train Loss: 10.2427\n",
      "test Loss: 10.1338, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11698/100000 [3:31:09<21:04:51,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11700/100000]\n",
      "Train Loss: 10.0887\n",
      "test Loss: 10.0980, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11798/100000 [3:32:54<21:10:32,  1.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11800/100000]\n",
      "Train Loss: 10.2298\n",
      "test Loss: 10.0627, test Accuracy: 0.0000\n",
      "test F1 (micro): 0.0000, test F1 (macro): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11809/100000 [3:33:27<25:45:53,  1.05s/it] "
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "current_lr = lr\n",
    "\n",
    "for epoch in tqdm(range(latest_epoch + 1, latest_epoch + epochs + 1)):\n",
    "    model.train()\n",
    "    \n",
    "    tensor_batch = data_to_tensor_batch(da.get_batch())\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    labels = tensor_batch.taxes\n",
    "    outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        train_loss = running_loss / val_epoch\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_accuracy, val_f1_micro, val_f1_macro = evaluate(model)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"test Loss: {val_loss:.4f}, test Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"test F1 (micro): {val_f1_micro:.4f}, test F1 (macro): {val_f1_macro:.4f}\")\n",
    "        \n",
    "        # Create metrics dictionary for saving\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": val_loss,\n",
    "            \"test_accuracy\": val_accuracy,\n",
    "            \"test_f1_micro\": val_f1_micro,\n",
    "            \"test_f1_macro\": val_f1_macro,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"lr\": current_lr\n",
    "        }\n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch + loss.item())\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Reset training metrics\n",
    "        running_loss = 0\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d2b86-4822-422b-a3f3-c6617b3701ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mlight-terrain-11\u001b[0m at: \u001b[34mhttps://wandb.ai/alireza_noroozi/Finetune_ESM/runs/nvokgtnu\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20241205_144714-nvokgtnu/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model, _, _, latest_epoch, metrics = load_checkpoint(model)\n",
    "\n",
    "val_batches_ = [virus_da.get_batch() for _ in range(num_val // 2)] + [cellular_da.get_batch() for _ in range(num_val // 2)]\n",
    "\n",
    "# input_sequences_ = [e['Sequence'] for b in val_batches_ for e in b]\n",
    "# labels_ = [encode_lineage(e['Taxonomic_lineage__ALL_'])  for b in val_batches_ for e in b]\n",
    "\n",
    "input_sequences_ = [\"ACACAD\"]\n",
    "labels_ = [{0: 1}]\n",
    "\n",
    "def evaluate_df(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    df = {\n",
    "        \"sequence\": [],\n",
    "        \"label\": [],\n",
    "        \"pred\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": 0,\n",
    "        \"accuracy\": 0,\n",
    "        \"f1 macro\": 0,\n",
    "        \"f1 micro\": 0\n",
    "    }\n",
    "    \n",
    "    # Process each sequence\n",
    "    for sequence, label in zip(input_sequences_, labels_):\n",
    "        inputs = tokenizer_(\n",
    "            [sequence],\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len\n",
    "        ).to(device)\n",
    "    \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "        pred = output.argmax(dim=-1).cpu().item()\n",
    "        loss = criterion(output, torch.tensor([label[0]]).to(device))\n",
    "        df[\"sequence\"].append(sequence)\n",
    "        df[\"label\"].append(level_decoder[0][label[0]])\n",
    "        df[\"pred\"].append(level_decoder[0][pred])\n",
    "        df[\"loss\"].append(round(loss.cpu().item(), 4))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    new_df = pd.DataFrame(df)\n",
    "    new_df['is_incorrect'] = new_df['label'] != new_df['pred']\n",
    "    new_df = new_df.sort_values(['is_incorrect', 'loss'], ascending=[False, False])\n",
    "    new_df.to_csv(f'classification_results__new_att.csv', index=False)\n",
    "\n",
    "    metrics[\"loss\"] = np.array(df[\"loss\"]).mean()\n",
    "    metrics[\"accuracy\"] = accuracy_score(np.array(df[\"label\"]), np.array(df[\"pred\"]))\n",
    "    metrics[\"f1 macro\"] = f1_score(np.array(df[\"label\"]), np.array(df[\"pred\"]), average='macro')  # F1-score for multi-label classification\n",
    "    metrics[\"f1 micro\"] = f1_score(np.array(df[\"label\"]), np.array(df[\"pred\"]), average='micro') \n",
    "    print(metrics)\n",
    "\n",
    "evaluate_df(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ef5ce-baca-4389-b958-e2f2aff6b4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f9007-8e44-4eff-b37a-27fcd1600262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

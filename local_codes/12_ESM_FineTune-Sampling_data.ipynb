{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cd1b0e-e0e4-4fd8-a680-ccdf0e1918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dictionary.\n",
      "cuda:0\n",
      " WORLD_SIZE=1 , LOCAL_WORLD_SIZE=1,RANK =0,LOCAL_RANK = 0 \n",
      "../checkpoints/Fine Tune ESM uniform sampling_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malirezanor\u001b[0m (\u001b[33malirezanor-310-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241115_081055-w2fx3tdc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM%20uniform%20sampling/runs/w2fx3tdc' target=\"_blank\">still-vortex-12</a></strong> to <a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM%20uniform%20sampling' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM%20uniform%20sampling' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM%20uniform%20sampling</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM%20uniform%20sampling/runs/w2fx3tdc' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM%20uniform%20sampling/runs/w2fx3tdc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM%20uniform%20sampling/runs/w2fx3tdc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x711783a212d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from data_process import mix_data_to_tensor_batch, simple_data_to_tensor_batch\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 100_000\n",
    "val_epoch = 1000\n",
    "num_val = 50\n",
    "batch_size = 16\n",
    "dataset_name = \"corpus_1000_random\"\n",
    "virus_dataset_name = \"corpus_200_500_Viruses_random\"\n",
    "non_virus_dataset_name = \"corpus_200_500_Non_Viruses_random\"\n",
    "lr = 0.001\n",
    "model_name = \"Fine Tune ESM uniform sampling\"\n",
    "max_seq_len = 500\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "virus_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq/{virus_dataset_name}\", batch_size)\n",
    "non_virus_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq/{non_virus_dataset_name}\", batch_size)\n",
    "da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq/{dataset_name}\", 2 * batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"ESM + FNN\",\n",
    "        \"dataset\": virus_dataset_name + \", \" + non_virus_dataset_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_szie\": batch_size,\n",
    "        \"max_seq_len\": max_seq_len\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cb1410-5ec8-4c88-854d-be599e54e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmModel\n",
    "\n",
    "class ESM1b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")\n",
    "        self.layer1 = nn.Linear(1280, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(512, 4)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.esm(x, attention_mask=attention_mask)\n",
    "        # Mean pooling over all residues\n",
    "        outputs = outputs.last_hidden_state.mean(dim=1)\n",
    "        outputs = self.layer1(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.layer2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37fede89-9b77-4f1e-951b-3f6a2600d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 653.014425 M parameters\n",
      "ESM1b(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-32): 33 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (layer1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (layer2): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ESM1b().to(device)\n",
    "print(\"model:\", sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Initial restart interval\n",
    "    T_mult=2,  # Multiply interval by 2 after each restart\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb03ada3-72a3-4e55-a292-7773574ce08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = [da.get_batch() for _ in range(num_val)]\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "            tensor_batch = simple_data_to_tensor_batch(val_batches[epoch], max_seq_len)\n",
    "            tensor_batch.gpu(device)\n",
    "        \n",
    "            labels = tensor_batch.taxes[\"begining root\"]\n",
    "            outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "                \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches into single tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Compute evaluation metrics (example: accuracy, F1 score)\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    f1_macro = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')  # F1-score for multi-label classification\n",
    "    f1_micro = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')  # F1-score for multi-label classification\n",
    "    conf_matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy(), labels= [0, 1, 2, 3])\n",
    "    avg_loss = running_loss / num_val\n",
    "    \n",
    "    return avg_loss, accuracy, f1_micro, f1_macro, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba451553-07c8-4608-821a-28078d720548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def load_checkpoint(model, optimizer=None, scheduler=None):\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pt'))        \n",
    "    # Extract epoch numbers and find latest\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load optimizer state if provided (for training)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Move optimizer state to GPU if necessary\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get training metadata\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "    \n",
    "    print(f\"Successfully loaded checkpoint from epoch {epoch}\")\n",
    "    print(\"Metrics at checkpoint:\", metrics)\n",
    "    \n",
    "    return model, optimizer, scheduler, epoch, metrics\n",
    "        \n",
    "\n",
    "model, optimizer, scheduler, latest_epoch, metrics = load_checkpoint(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8969e5b-acab-4cd4-9822-1018b922491e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b894a867-bf97-4d90-9092-17acfa8cf6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_ratio(epoch, decay_epochs=50000):\n",
    "    \"\"\"\n",
    "    Calculate partition ratio that decreases from 8/16 to 1/16 in steps\n",
    "    \"\"\"\n",
    "    # Calculate how many epochs before each step down\n",
    "    epochs_per_step = decay_epochs // 7  # 7 steps from 8/16 down to 1/16\n",
    "    \n",
    "    # Calculate current step based on epoch\n",
    "    step = min(epoch // epochs_per_step, 7)  # Max 7 steps down from 8\n",
    "    \n",
    "    # Map step to fraction\n",
    "    fraction = (8 - step) / 16\n",
    "    \n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b487e-69ce-4c41-be78-a1b6dcb58e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 999/100000 [25:08<41:40:39,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/100000]\n",
      "Train Loss: 0.6482, Train Accuracy: 0.6358\n",
      "Train F1 (micro): 0.6358, Train F1 (macro): 0.4241\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 5218 2782    0]\n",
      " [   0 3020 4954    0]\n",
      " [   0    8   18    0]]\n",
      "Val Loss: 0.5250, Val Accuracy: 0.7475\n",
      "Val F1 (micro): 0.7475, Val F1 (macro): 0.3141\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   19    6    0]\n",
      " [   0  386 1177    0]\n",
      " [   0    4    8    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1999/100000 [51:28<41:04:17,  1.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2000/100000]\n",
      "Train Loss: 0.5404, Train Accuracy: 0.7392\n",
      "Train F1 (micro): 0.7392, Train F1 (macro): 0.4930\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 6025 1975    0]\n",
      " [   0 2181 5802    0]\n",
      " [   0   10    7    0]]\n",
      "Val Loss: 0.9491, Val Accuracy: 0.5569\n",
      "Val F1 (micro): 0.5569, Val F1 (macro): 0.2570\n",
      "Val Confusion Matrix:\n",
      "[[  0   0   0   0]\n",
      " [  0  22   3   0]\n",
      " [  0 694 869   0]\n",
      " [  0   7   5   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2999/100000 [1:17:45<40:36:17,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3000/100000]\n",
      "Train Loss: 0.5092, Train Accuracy: 0.7689\n",
      "Train F1 (micro): 0.7689, Train F1 (macro): 0.5657\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 6135 1828   37]\n",
      " [   0 1590 6143   79]\n",
      " [   0   29  135   24]]\n",
      "Val Loss: 0.5578, Val Accuracy: 0.7581\n",
      "Val F1 (micro): 0.7581, Val F1 (macro): 0.3218\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   22    3    0]\n",
      " [   0  372 1191    0]\n",
      " [   0    4    8    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3999/100000 [1:44:07<40:18:38,  1.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4000/100000]\n",
      "Train Loss: 0.4300, Train Accuracy: 0.8061\n",
      "Train F1 (micro): 0.8061, Train F1 (macro): 0.5940\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 6239 1758    3]\n",
      " [   0 1275 6651   18]\n",
      " [   0   33   16    7]]\n",
      "Val Loss: 0.8752, Val Accuracy: 0.6562\n",
      "Val F1 (micro): 0.6562, Val F1 (macro): 0.2885\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   22    3    0]\n",
      " [   0  535 1028    0]\n",
      " [   0    6    6    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4999/100000 [2:10:27<39:44:58,  1.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5000/100000]\n",
      "Train Loss: 0.4214, Train Accuracy: 0.8185\n",
      "Train F1 (micro): 0.8185, Train F1 (macro): 0.5468\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 6299 1701    0]\n",
      " [   0 1130 6797    0]\n",
      " [   0   26   47    0]]\n",
      "Val Loss: 0.6682, Val Accuracy: 0.7388\n",
      "Val F1 (micro): 0.7388, Val F1 (macro): 0.3139\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   21    4    0]\n",
      " [   0  402 1161    0]\n",
      " [   0    5    7    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5174/100000 [2:15:58<39:47:16,  1.51s/it] IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "  9%|▉         | 8999/100000 [3:55:42<38:10:05,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9000/100000]\n",
      "Train Loss: 0.4452, Train Accuracy: 0.8066\n",
      "Train F1 (micro): 0.8066, Train F1 (macro): 0.6205\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 5329 1638   33]\n",
      " [   0 1139 7527   92]\n",
      " [   0   50  142   50]]\n",
      "Val Loss: 0.6621, Val Accuracy: 0.7156\n",
      "Val F1 (micro): 0.7156, Val F1 (macro): 0.3063\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   21    4    0]\n",
      " [   0  439 1124    0]\n",
      " [   0    5    7    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 9999/100000 [4:22:05<37:50:58,  1.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10000/100000]\n",
      "Train Loss: 0.3717, Train Accuracy: 0.8433\n",
      "Train F1 (micro): 0.8433, Train F1 (macro): 0.5610\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 5544 1456    0]\n",
      " [   0  988 7949    0]\n",
      " [   0    7   56    0]]\n",
      "Val Loss: 0.5433, Val Accuracy: 0.7619\n",
      "Val F1 (micro): 0.7619, Val F1 (macro): 0.3204\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   20    5    0]\n",
      " [   0  364 1199    0]\n",
      " [   0    5    7    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10999/100000 [4:48:16<37:16:34,  1.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11000/100000]\n",
      "Train Loss: 0.4054, Train Accuracy: 0.8195\n",
      "Train F1 (micro): 0.8195, Train F1 (macro): 0.5841\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 5290 1704    6]\n",
      " [   0 1094 7816   14]\n",
      " [   0   21   49    6]]\n",
      "Val Loss: 0.3899, Val Accuracy: 0.8550\n",
      "Val F1 (micro): 0.8550, Val F1 (macro): 0.3491\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   16    9    0]\n",
      " [   0  211 1352    0]\n",
      " [   0    3    9    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 11999/100000 [5:14:29<36:43:44,  1.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12000/100000]\n",
      "Train Loss: 0.6197, Train Accuracy: 0.6613\n",
      "Train F1 (micro): 0.6613, Train F1 (macro): 0.4213\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 3029 3971    0]\n",
      " [   0 1442 7551    0]\n",
      " [   0    0    7    0]]\n",
      "Val Loss: 0.5118, Val Accuracy: 0.9762\n",
      "Val F1 (micro): 0.9762, Val F1 (macro): 0.3293\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0    0   25    0]\n",
      " [   0    1 1562    0]\n",
      " [   0    0   12    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 12999/100000 [5:40:35<36:22:23,  1.51s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13000/100000]\n",
      "Train Loss: 0.5739, Train Accuracy: 0.7114\n",
      "Train F1 (micro): 0.7114, Train F1 (macro): 0.4650\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 3962 3038    0]\n",
      " [   0 1568 7421    0]\n",
      " [   0    2    9    0]]\n",
      "Val Loss: 0.5634, Val Accuracy: 0.7863\n",
      "Val F1 (micro): 0.7863, Val F1 (macro): 0.3209\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   15   10    0]\n",
      " [   0  320 1243    0]\n",
      " [   0    5    7    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 13999/100000 [6:07:28<35:50:33,  1.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14000/100000]\n",
      "Train Loss: 0.4995, Train Accuracy: 0.7711\n",
      "Train F1 (micro): 0.7711, Train F1 (macro): 0.5089\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 4609 2391    0]\n",
      " [   0 1225 7729    7]\n",
      " [   0    6   33    0]]\n",
      "Val Loss: 0.6428, Val Accuracy: 0.7600\n",
      "Val F1 (micro): 0.7600, Val F1 (macro): 0.3169\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   18    7    0]\n",
      " [   0  365 1198    0]\n",
      " [   0    6    6    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 14999/100000 [6:34:35<38:57:50,  1.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15000/100000]\n",
      "Train Loss: 0.5756, Train Accuracy: 0.7282\n",
      "Train F1 (micro): 0.7282, Train F1 (macro): 0.4994\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 4033 2140  111]\n",
      " [   0 1559 7605  292]\n",
      " [   0   98  149   13]]\n",
      "Val Loss: 0.4167, Val Accuracy: 0.8788\n",
      "Val F1 (micro): 0.8788, Val F1 (macro): 0.3474\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0   11   14    0]\n",
      " [   0  168 1395    0]\n",
      " [   0    3    9    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 15999/100000 [7:00:51<34:59:25,  1.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16000/100000]\n",
      "Train Loss: 0.4419, Train Accuracy: 0.8162\n",
      "Train F1 (micro): 0.8162, Train F1 (macro): 0.5314\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 3948 2052    0]\n",
      " [   0  820 9111    0]\n",
      " [   0    1   68    0]]\n",
      "Val Loss: 0.3129, Val Accuracy: 0.9213\n",
      "Val F1 (micro): 0.9213, Val F1 (macro): 0.3641\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0    9   16    0]\n",
      " [   0   98 1465    0]\n",
      " [   0    4    8    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 16999/100000 [7:27:15<35:40:03,  1.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17000/100000]\n",
      "Train Loss: 0.4157, Train Accuracy: 0.8217\n",
      "Train F1 (micro): 0.8217, Train F1 (macro): 0.5354\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 4053 1947    0]\n",
      " [   0  867 9094    0]\n",
      " [   0   19   20    0]]\n",
      "Val Loss: 0.2660, Val Accuracy: 0.9250\n",
      "Val F1 (micro): 0.9250, Val F1 (macro): 0.3628\n",
      "Val Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0    8   17    0]\n",
      " [   0   91 1472    0]\n",
      " [   0    2   10    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 17512/100000 [7:41:55<36:01:09,  1.57s/it] "
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "train_preds = []\n",
    "train_labels = []\n",
    "\n",
    "for epoch in tqdm(range(latest_epoch, latest_epoch + epochs)):\n",
    "    model.train()\n",
    "\n",
    "    current_partition = get_partition_ratio(epoch)\n",
    "    \n",
    "    tensor_batch = mix_data_to_tensor_batch(\n",
    "        virus_da.get_batch(),\n",
    "        non_virus_da.get_batch(),\n",
    "        max_seq_len,\n",
    "        partition=current_partition\n",
    "    )\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    labels = tensor_batch.taxes[\"begining root\"]\n",
    "    outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "\n",
    "\n",
    "    \n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    train_preds.append(preds.cpu())\n",
    "    train_labels.append(labels.cpu())\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        # Calculate training metrics\n",
    "        all_train_preds = torch.cat(train_preds)\n",
    "        all_train_labels = torch.cat(train_labels)\n",
    "        \n",
    "        train_accuracy = accuracy_score(all_train_labels.numpy(), all_train_preds.numpy())\n",
    "        train_f1_micro = f1_score(all_train_labels.numpy(), all_train_preds.numpy(), average='micro')\n",
    "        train_f1_macro = f1_score(all_train_labels.numpy(), all_train_preds.numpy(), average='macro')\n",
    "        train_cm = confusion_matrix(all_train_labels.numpy(), all_train_preds.numpy(), labels=[0, 1, 2, 3])\n",
    "        train_loss = running_loss / val_epoch\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Train F1 (micro): {train_f1_micro:.4f}, Train F1 (macro): {train_f1_macro:.4f}\")\n",
    "        print(\"Train Confusion Matrix:\")\n",
    "        print(train_cm)\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_accuracy, val_f1_micro, val_f1_macro, val_cm = evaluate(model)\n",
    "        \n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Val F1 (micro): {val_f1_micro:.4f}, Val F1 (macro): {val_f1_macro:.4f}\")\n",
    "        print(\"Val Confusion Matrix:\")\n",
    "        print(val_cm)\n",
    "        \n",
    "        # Create metrics dictionary for saving\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"train_f1_micro\": train_f1_micro,\n",
    "            \"train_f1_macro\": train_f1_macro,\n",
    "            \"train_confusion_matrix\": train_cm,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_f1_micro\": val_f1_micro,\n",
    "            \"val_f1_macro\": val_f1_macro,\n",
    "            \"val_confusion_matrix\": val_cm,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"current_portion\": current_partition,\n",
    "            \"lr\": current_lr\n",
    "        }\n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch + loss.item())\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Reset training metrics\n",
    "        running_loss = 0\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38a288-ea78-424e-9f90-995c9b435655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

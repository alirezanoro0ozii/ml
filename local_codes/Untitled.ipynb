{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44541519-5260-4b41-bdf7-489e26124cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:330: UserWarning: 1Torch was not compiled with memory efficient attention. (Triggered internally at /var/lib/jenkins/pytorch/aten/src/ATen/native/transformers/hip/sdp_utils.cpp:505.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BioGptModel\n",
    "import torch\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "model = BioGptModel.from_pretrained(\"microsoft/biogpt\").to(device)\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(device)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d1b22d-46b1-4b53-85be-2a497ba6fa5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36f2e6a-14bb-4728-956b-1e4e8231560d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioGptModel(\n",
       "  (embed_tokens): BioGptScaledWordEmbedding(42384, 1024, padding_idx=1)\n",
       "  (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x BioGptDecoderLayer(\n",
       "      (self_attn): BioGptSdpaAttention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd21c80-4aba-47e8-b2e0-ff9921822c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.tokens of {'input_ids': tensor([[    2,   313,  3666,   399,     7, 10241,  4316,    21,   704,  9070]],\n",
       "       device='cuda:1'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:1')}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da321b82-4d6e-41bc-b392-9d42039cd079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viruses Riboviria Pararnavirae Artverviricota Revtraviricetes Ortervirales Retroviridae Orthoretrovirinae Lentivirus Primate lentivirus group unclassified Primate lentivirus group Human immunodeficiency virus\n",
      "29386 \t Viruses\n",
      "13699 \t Rib\n",
      "33030 \t ovi\n",
      "12347 \t ria\n",
      "3587 \t Par\n",
      "1162 \t ar\n",
      "790 \t n\n",
      "13583 \t avi\n",
      "36460 \t rae\n",
      "2897 \t Ar\n",
      "765 \t t\n",
      "3238 \t ver\n",
      "8493 \t vir\n",
      "1356 \t ic\n",
      "15373 \t ota\n",
      "12924 \t Rev\n",
      "9499 \t tra\n",
      "8493 \t vir\n",
      "1356 \t ic\n",
      "10474 \t etes\n",
      "6530 \t Or\n",
      "2543 \t ter\n",
      "8493 \t vir\n",
      "9545 \t ales\n",
      "16218 \t Retro\n",
      "22269 \t viridae\n",
      "11523 \t Orth\n",
      "5065 \t ore\n",
      "5230 \t tro\n",
      "8493 \t vir\n",
      "14739 \t inae\n",
      "237 \t L\n",
      "5735 \t enti\n",
      "350 \t virus\n",
      "11584 \t Prim\n",
      "786 \t ate\n",
      "28524 \t lentivirus\n",
      "64 \t group\n",
      "446 \t un\n",
      "1856 \t classified\n",
      "11584 \t Prim\n",
      "786 \t ate\n",
      "28524 \t lentivirus\n",
      "64 \t group\n",
      "1969 \t Human\n",
      "3317 \t immunodeficiency\n",
      "350 \t virus\n"
     ]
    }
   ],
   "source": [
    "str_input = \"Viruses, Riboviria, Pararnavirae, Artverviricota, Revtraviricetes, Ortervirales, Retroviridae, Orthoretrovirinae, Lentivirus, Primate lentivirus group, unclassified Primate lentivirus group, Human immunodeficiency virus\"\n",
    "\n",
    "taxa = [taxon.strip() for taxon in str_input.split(',')]\n",
    "processed_text =\" \".join(taxa)\n",
    "\n",
    "str_input = processed_text\n",
    "print(str_input)\n",
    "inputs = tokenizer(str_input, return_tensors=\"pt\", add_special_tokens=False)\n",
    "input_ids = inputs.input_ids[0].tolist()\n",
    "# print(input_ids)\n",
    "for _ in range(len(input_ids)):\n",
    "    print(input_ids[_], \"\\t\", tokenizer.decode(input_ids[_]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7e431b5-c346-4a84-8787-ee85392b7d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "print('allocated', torch.cuda.memory_allocated() / 1024**2)  # MB\n",
    "print('cached', torch.cuda.memory_reserved() / 1024**2)      # MB\n",
    "\n",
    "model = EsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "# Force CUDA to sync\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print('allocated', torch.cuda.memory_allocated() / 1024**2)  # MB\n",
    "print('cached', torch.cuda.memory_reserved() / 1024**2)      # MB\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63eaaa13-fc77-453c-a368-119c295277f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 21,  3,  3,  3,  3,  3,  2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8da7386-7f12-4dfe-aee3-1b215a07f980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 320])\n",
      "torch.Size([1, 320])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)\n",
    "print(outputs.pooler_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de1ae358-5392-46d5-92ac-de57ad4404b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "def data_tensor_batch(b, max_seq_len):\n",
    "    return tokenizer(\n",
    "        [e['Sequence'] for e in b],\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "class ESM2(nn.Module):\n",
    "    def __init__(self,model_name, num_classes):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(model_name)\n",
    "        # for param in self.esm.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.layer1 = nn.Linear(1280, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.esm(x, attention_mask=attention_mask).pooler_output\n",
    "        outputs = self.layer1(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.layer2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b5357d8-4139-45b2-a2ee-87c7b431b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def eval_model(model, max_seq_len, num_val=10):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    timing_stats = {\n",
    "        'batch_times': [],\n",
    "        'total_time': 0\n",
    "    }\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():\n",
    "            batch_start_time = time.time()\n",
    "    \n",
    "            tensor_batch = data_tensor_batch(da.get_batch(), max_seq_len)\n",
    "            outputs = model(tensor_batch['input_ids'], tensor_batch['attention_mask'])\n",
    "    \n",
    "            batch_end_time = time.time()\n",
    "            \n",
    "            batch_duration = batch_end_time - batch_start_time\n",
    "            timing_stats['batch_times'].append(batch_duration)\n",
    "            \n",
    "            print(f\"Batch {epoch + 1}/{num_val} took {batch_duration:.3f} seconds\")\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    total_duration = total_end_time - total_start_time\n",
    "    timing_stats['total_time'] = total_duration\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_batch_time = sum(timing_stats['batch_times']) / len(timing_stats['batch_times'])\n",
    "    print(f\"\\nEvaluation Summary:\")\n",
    "    print(f\"Total time: {total_duration:.3f} seconds\")\n",
    "    print(f\"Average batch time: {avg_batch_time:.3f} seconds\")\n",
    "\n",
    "\n",
    "def train_model(model, max_seq_len, num_train=10):\n",
    "    model.train()  # Set model to evaluation mode\n",
    "    \n",
    "    timing_stats = {\n",
    "        'batch_times': [],\n",
    "        'total_time': 0\n",
    "    }\n",
    "    \n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for epoch in range(num_train):\n",
    "        batch_start_time = time.time()\n",
    "    \n",
    "        tensor_batch = data_tensor_batch(da.get_batch(), max_seq_len)\n",
    "        outputs = model(tensor_batch['input_ids'], tensor_batch['attention_mask'])\n",
    "    \n",
    "        loss = criterion(outputs, torch.zeros((batch_size, )).to(device).long())\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_end_time = time.time()\n",
    "        batch_duration = batch_end_time - batch_start_time\n",
    "        timing_stats['batch_times'].append(batch_duration)\n",
    "        \n",
    "        print(f\"Batch {epoch + 1}/{num_train} took {batch_duration:.3f} seconds\")\n",
    "    \n",
    "    total_end_time = time.time()\n",
    "    total_duration = total_end_time - total_start_time\n",
    "    timing_stats['total_time'] = total_duration\n",
    "    \n",
    "    # Calculate statistics\n",
    "    avg_batch_time = sum(timing_stats['batch_times']) / len(timing_stats['batch_times'])\n",
    "    print(f\"\\nEvaluation Summary:\")\n",
    "    print(f\"Total time: {total_duration:.3f} seconds\")\n",
    "    print(f\"Average batch time: {avg_batch_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d8ef847-31c3-4bdd-a000-70f3a57ec28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated 0.0\n",
      "cached 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allocated 0.0\n",
      "cached 0.0\n",
      "Trainable parameters: 653.010326 M\n",
      "Total parameters: 653.010326 M\n",
      "Batch 1/10 took 1.423 seconds\n",
      "Batch 2/10 took 0.532 seconds\n",
      "Batch 3/10 took 0.522 seconds\n",
      "Batch 4/10 took 0.521 seconds\n",
      "Batch 5/10 took 0.521 seconds\n",
      "Batch 6/10 took 0.523 seconds\n",
      "Batch 7/10 took 0.530 seconds\n",
      "Batch 8/10 took 0.525 seconds\n",
      "Batch 9/10 took 0.523 seconds\n",
      "Batch 10/10 took 0.522 seconds\n",
      "\n",
      "Evaluation Summary:\n",
      "Total time: 6.144 seconds\n",
      "Average batch time: 0.614 seconds\n",
      "---------------------------------------\n",
      "Batch 1/10 took 0.097 seconds\n",
      "Batch 2/10 took 0.168 seconds\n",
      "Batch 3/10 took 0.166 seconds\n",
      "Batch 4/10 took 0.169 seconds\n",
      "Batch 5/10 took 0.169 seconds\n",
      "Batch 6/10 took 0.169 seconds\n",
      "Batch 7/10 took 0.169 seconds\n",
      "Batch 8/10 took 0.169 seconds\n",
      "Batch 9/10 took 0.169 seconds\n",
      "Batch 10/10 took 0.170 seconds\n",
      "\n",
      "Evaluation Summary:\n",
      "Total time: 1.616 seconds\n",
      "Average batch time: 0.162 seconds\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/esm2_t33_650M_UR50D\"\n",
    "dataset_name = \"corpus_200_500_random\"\n",
    "batch_size = 2\n",
    "from data_access import PQDataAccess\n",
    "da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{dataset_name}\", batch_size)\n",
    "max_seq_len = 1022\n",
    "\n",
    "print('allocated', torch.cuda.memory_allocated() / 1024**2)  # MB\n",
    "print('cached', torch.cuda.memory_reserved() / 1024**2)      # MB\n",
    "\n",
    "model = ESM2(model_name, 1).to(device)\n",
    "# Force CUDA to sync\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print('allocated', torch.cuda.memory_allocated() / 1024**2)  # MB\n",
    "print('cached', torch.cuda.memory_reserved() / 1024**2)      # MB\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable parameters: {trainable/ 1e6} M')\n",
    "print(f'Total parameters: {total/ 1e6} M')\n",
    "# print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Initial restart interval\n",
    "    T_mult=2,  # Multiply interval by 2 after each restart\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")\n",
    "\n",
    "train_model(model, max_seq_len)\n",
    "print(\"---------------------------------------\")\n",
    "eval_model(model, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed1f6b-a677-40fa-910b-d2af4e5cfd84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

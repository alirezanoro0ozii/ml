{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29691e21-2220-42da-9ff5-c2b75bf41211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "../checkpoints/ESM_T5_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel, T5Tokenizer, T5EncoderModel\n",
    "from transformers import T5Tokenizer, AutoTokenizer, EsmModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "\n",
    "# sys.path.insert(0, '../dlp')\n",
    "# from data_process import *\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs= 10_000\n",
    "val_epoch = 100\n",
    "num_val = 25\n",
    "\n",
    "model_name = \"ESM_T5\"\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89b10832-468f-4164-818d-7ed931e3fee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, '../dlp')\n",
    "from batch import Batch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "len_tokenizer = len(tokenizer.vocab)\n",
    "print(len_tokenizer)\n",
    "\n",
    "\n",
    "def encode_lineage_tokenizer(tax_lineage):\n",
    "    encoded = tokenizer.encode(tax_lineage.split(\", \"), add_special_tokens=True, padding='max_length', truncation=True, max_length=150, is_split_into_words= True)\n",
    "    # print(encoded)\n",
    "    return encoded\n",
    "\n",
    "def embedding_batch(split, i):\n",
    "    sequences, lineage_str = torch.load(f'../embeddings/esm_embeddings/{split}/{i}.pt')\n",
    "    tax_ids = [encode_lineage_tokenizer(s) for s in lineage_str]\n",
    "    \n",
    "    return Batch(sequences, torch.LongTensor(tax_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db0481a8-5bad-48da-a199-5b76bd0ab8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding_Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        max_seq_len=1000,\n",
    "        max_tax_len=150,\n",
    "        d_model=512\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_linear = nn.Linear(320, d_model)\n",
    "        self.decoder_embedding = AutoModelForMaskedLM.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead=4,\n",
    "            num_encoder_layers=3,\n",
    "            num_decoder_layers=3,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "\n",
    "        self.lm_head = nn.Linear(d_model, output_dim, bias=False)\n",
    "\n",
    "    def forward(self, src, tgt, tgt_mask=None, src_padding_mask=None, tgt_padding_mask=None):\n",
    "        src = self.encoder_linear(src)\n",
    "        tgt = self.decoder_embedding(tgt)\n",
    "        \n",
    "        output = self.transformer(\n",
    "            src,\n",
    "            tgt,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_padding_mask=tgt_padding_mask\n",
    "        )\n",
    "        \n",
    "        return self.lm_head(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a52dfb7a-ac56-43cd-8c60-c58efee82939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 137.93081 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = embedding_Transformer(output_dim=len_tokenizer).to(device)\n",
    "print(\"model:\", sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "449c485f-0d3a-4af7-afea-5e79400006bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"esm2_t6_8M_UR50D\"\n",
    "_model = None\n",
    "_tokenizer = None\n",
    "_device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_esm_model():\n",
    "    global _model, _tokenizer\n",
    "    if _model is None:\n",
    "        _tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\n",
    "        _model = EsmModel.from_pretrained(f\"facebook/{model_name}\").to(_device)\n",
    "    return _model, _tokenizer\n",
    "\n",
    "def esm_embedding_sequence(sequences):\n",
    "    model, tokenizer = get_esm_model()\n",
    "    inputs = tokenizer(\n",
    "        sequences,\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len\n",
    "    ).to(_device)\n",
    "    \n",
    "    with torch.no_grad():  # Add this to reduce memory usage\n",
    "        # Forward pass through the model\n",
    "        outputs = model(**inputs).last_hidden_state\n",
    "        # Compute mean across the sequence dimension (or any other pooling method)\n",
    "        output_embeddings = outputs.cpu()  # Move back to CPU\n",
    "    return output_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ef87958-0e53-4e20-a343-7a536e89ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(epoch):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get batch and convert to tensor\n",
    "    tensor_batch = embedding_batch('train', epoch)\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    src = tensor_batch.seq_ids\n",
    "    tgt = tensor_batch.taxes\n",
    "    \n",
    "    # Create masks\n",
    "    tgt_mask = model.transformer.generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
    "\n",
    "    output = model(\n",
    "        src,\n",
    "        tgt,\n",
    "        tgt_mask=tgt_mask,\n",
    "        tgt_padding_mask=(tgt == 0)\n",
    "    )\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "    \n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea8637ba-6c64-404e-96f9-a0eeecd569d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(max_len=50, start_token=2, end_token=3, device='cuda'):    \n",
    "    test_protein_sequence = [\"MKTAYIAKQRQISFVKSHFSRQDIL\"]\n",
    "    src = esm_embedding_sequence(test_protein_sequence).to(device)\n",
    "    \n",
    "    # Initialize target sequence with start token\n",
    "    # Shape: (batch_size, 1)\n",
    "    tgt = torch.ones(src.size(0), 1).long().to(device) * start_token\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            # Generate next token probabilities\n",
    "            output = model(src, tgt)\n",
    "            # Get the next token prediction\n",
    "            # output shape: (batch_size, seq_len, vocab_size)\n",
    "            # We only need the last token prediction\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            # print(_, next_token_logits)\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Append the predicted token to target sequence\n",
    "            tgt = torch.cat([tgt, next_token], dim=1)\n",
    "            \n",
    "            # Check if end token is generated\n",
    "            if (next_token == end_token).all():\n",
    "                break\n",
    "\n",
    "    print(tokenizer.decode(tgt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea5c51f9-b292-43bb-adcb-458db794fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might also want to add beam search for better generation\n",
    "def generate_with_beam_search(beam_width=5, max_len=100, start_token=101, end_token=102, device='cuda'):\n",
    "    test_protein_sequence = [\"MKTAYIAKQRQISFVKSHFSRQDIL\"]\n",
    "    src = torch.LongTensor([encode_sequence(*test_protein_sequence)]).to(device)\n",
    "\n",
    "    model.eval()\n",
    "    batch_size = src.size(0)\n",
    "    \n",
    "    # Initialize beams with start tokens\n",
    "    beams = [(torch.ones(batch_size, 1).long().to(device) * start_token, 0.0)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            candidates = []\n",
    "            \n",
    "            # Expand each beam\n",
    "            for sequence, score in beams:\n",
    "                # tgt_mask = model.transformer.generate_square_subsequent_mask(sequence.size(1)).to(device)\n",
    "                # output = model(src, sequence, tgt_mask=tgt_mask)\n",
    "                output = model(src, sequence)\n",
    "                \n",
    "                next_token_logits = output[:, -1, :]\n",
    "                \n",
    "                # Get top k next tokens for each beam\n",
    "                probs = nn.Softmax(dim=-1)(next_token_logits)\n",
    "                top_probs, top_tokens = probs.topk(beam_width)\n",
    "                \n",
    "                for prob, token in zip(top_probs[0], top_tokens[0]):\n",
    "                    new_sequence = torch.cat([sequence, token.unsqueeze(0).unsqueeze(1)], dim=1)\n",
    "                    new_score = score - torch.log(prob).item()  # Convert to log probability\n",
    "                    candidates.append((new_sequence, new_score))\n",
    "            \n",
    "            # Select top beam_width candidates\n",
    "            candidates.sort(key=lambda x: x[1])  # Sort by score\n",
    "            beams = candidates[:beam_width]\n",
    "            \n",
    "            # Check if best beam ended\n",
    "            if beams[0][0][:, -1].item() == end_token:\n",
    "                break\n",
    "\n",
    "    print(tokenizer.decode(beams[0][0][0]))\n",
    "    # Return the best beam\n",
    "    return beams[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83fd445d-a2f9-40fc-8a71-158f8df168ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:9vfvculs) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-deluge-15</strong> at: <a href='https://wandb.ai/alirezanor-310-ai/ESM_T5/runs/9vfvculs' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/ESM_T5/runs/9vfvculs</a><br/> View project at: <a href='https://wandb.ai/alirezanor-310-ai/ESM_T5' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/ESM_T5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241112_081506-9vfvculs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:9vfvculs). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241112_081533-1ljv74wi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alirezanor-310-ai/ESM_T5/runs/1ljv74wi' target=\"_blank\">light-snowflake-16</a></strong> to <a href='https://wandb.ai/alirezanor-310-ai/ESM_T5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alirezanor-310-ai/ESM_T5' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/ESM_T5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alirezanor-310-ai/ESM_T5/runs/1ljv74wi' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/ESM_T5/runs/1ljv74wi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'MaskedLMOutput' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 20\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m val_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create masks\u001b[39;00m\n\u001b[1;32m     13\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mgenerate_square_subsequent_mask(tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 32\u001b[0m, in \u001b[0;36membedding_Transformer.forward\u001b[0;34m(self, src, tgt, tgt_mask, src_padding_mask, tgt_padding_mask)\u001b[0m\n\u001b[1;32m     29\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_linear(src)\n\u001b[1;32m     30\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_embedding(tgt)\n\u001b[0;32m---> 32\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(output)\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:211\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[43mtgt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe batch number of src and tgt must be equal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model \u001b[38;5;129;01mor\u001b[39;00m tgt\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MaskedLMOutput' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"ESM_T5\",\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"Transformer\",\n",
    "    \"dataset\": \"seqtext_1000\",\n",
    "    \"epochs\": epochs,\n",
    "    } \n",
    ")\n",
    "\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_step(epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        mean_train_loss = sum(train_losses[-val_epoch:]) / val_epoch\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {mean_train_loss:.4f}\")\n",
    "        generate()\n",
    "        # log metrics to wandb\n",
    "        wandb.log({\"train loss\": train_loss})\n",
    "        # wandb.log_artifact(model)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def23b4-8285-44df-b3e3-76e107eacb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_with_beam_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82589d6-b30d-4c93-929e-a42d8f671c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c217af6-e89a-4ec6-be30-7e1d72a1124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dictionary.\n",
      "30522\n",
      "cuda:1\n",
      "../checkpoints/esm_hierarchy_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malirezanor\u001b[0m (\u001b[33malirezanor-310-ai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241112_183307-1gj4c6uc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alirezanor-310-ai/test%20classification/runs/1gj4c6uc' target=\"_blank\">polar-bird-6</a></strong> to <a href='https://wandb.ai/alirezanor-310-ai/test%20classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alirezanor-310-ai/test%20classification' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/test%20classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alirezanor-310-ai/test%20classification/runs/1gj4c6uc' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/test%20classification/runs/1gj4c6uc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from data_process import *\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs= 10_000\n",
    "val_epoch = 500\n",
    "num_val = 1000\n",
    "\n",
    "model_name = \"esm_hierarchy\"\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"test classification\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    \"config\": \"onehot\" \n",
    "    }\n",
    ")\n",
    "\n",
    "config = \"embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b17e331f-93bc-46c5-b849-cdcbd9f192ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMFNNClassifier(nn.Module):\n",
    "    def __init__(self, model, embedding_dim, num_classes, max_seq_len, hidden_dim):\n",
    "        super(FNNClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(embedding_dim * max_seq_len, hidden_dim)  # Output size depends on conv and pooling layers\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes, bias=False)\n",
    "        self.fc3 = nn.Linear(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model == \"Flat\":\n",
    "            x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "            x = torch.relu(self.fc1(x))\n",
    "\n",
    "        elif self.model == \"Mean\":\n",
    "            x = x.mean(dim=1)\n",
    "            x = torch.relu(self.fc3(x))\n",
    "\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc645d81-5c2a-4a02-9578-3b41322133b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 1000\n",
    "max_tax_len = 150\n",
    "\n",
    "# Character vocabulary for protein sequences (20 amino acids + 1 padding)\n",
    "vocab = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(vocab)}  # Start index from 1 for padding\n",
    "# Sequence encoder: Convert the protein sequence into integers\n",
    "def encode_sequence(sequence):\n",
    "    return [char_to_idx.get(char, 0) for char in sequence] + [0 for _ in range(max_seq_len - len(sequence))]  # 0 for unknown characters or padding \n",
    "\n",
    "def encode_sequence_batch(sequences):\n",
    "    return torch.Tensor([encode_sequence(s) for s in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0482858d-4781-4bb9-83c0-2134a6b1901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 164.006912 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_dim = 21\n",
    "embedding_dim = 320\n",
    "hidden_dim = 512\n",
    "num_taxonomy_ids = 4  # Example: Assuming 14,680 possible taxonomy classes\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "ESM_model = FNNClassifier(\n",
    "    \"Mean\",\n",
    "    embedding_dim,\n",
    "    num_taxonomy_ids,\n",
    "    1000,\n",
    "    hidden_dim\n",
    ").to(device)\n",
    "\n",
    "onehot_model = FNNClassifier(\n",
    "    \"Flat\",\n",
    "    21,\n",
    "    num_taxonomy_ids,\n",
    "    1000,\n",
    "    hidden_dim\n",
    ").to(device)\n",
    "\n",
    "\n",
    "model = ESM_model if config == \"embedding\" else onehot_model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"model:\", sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bd613b7-6f9c-47e9-9587-f329de508780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(split='val'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "            tensor_batch, sequences = esm_hierarchy_data_to_tensor_batch(split, epoch)\n",
    "            tensor_batch.gpu(device)\n",
    "        \n",
    "            labels = tensor_batch.taxes[\"clades\"]\n",
    "            onehot_tensors = encode_sequence_batch(sequences).to(device)\n",
    "            \n",
    "            if config == \"embedding\":\n",
    "                outputs = model(tensor_batch.seq_ids)\n",
    "            else:\n",
    "                outputs = model(onehot_tensors)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "                \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches into single tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Compute evaluation metrics (example: accuracy, F1 score)\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    f1_macro = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')  # F1-score for multi-label classification\n",
    "    f1_micro = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')  # F1-score for multi-label classification\n",
    "    conf_matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "    avg_loss = running_loss / num_val\n",
    "    \n",
    "    return avg_loss, accuracy, f1_micro, f1_macro, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de31fff-9adf-4d46-a058-08ccdeaa11b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mepochs\u001b[49m):\n\u001b[1;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    tensor_batch, sequences = esm_hierarchy_data_to_tensor_batch('train', epoch)\n",
    "    tensor_batch.gpu(device)\n",
    "\n",
    "    labels = tensor_batch.taxes[\"clade\"]\n",
    "    onehot_tensors = encode_sequence_batch(sequences).to(device)\n",
    "    \n",
    "    if config == \"embedding\":\n",
    "        outputs = model(tensor_batch.seq_ids)\n",
    "    else:\n",
    "        outputs = model(onehot_tensors)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backpropagation: Zero the gradients, compute the backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the loss\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        # Print loss for this epoch\n",
    "        epoch_loss = running_loss / (epoch + 1)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        val_loss, val_accuracy, val_f1_micro, val_f1_macro, cm = evaluate()\n",
    "        print(cm)\n",
    "        print(f\"val Loss: {val_loss:.4f}, val Accuracy: {val_accuracy:.4f}, val F1 Score (micro): {val_f1_micro:.4f}, , val F1 Score (macro): {val_f1_macro:.4f}\")\n",
    "\n",
    "        wandb.log({\"train loss\": epoch_loss, \"val acc\": val_accuracy, \"val loss\": val_loss})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a637234-a1a9-472f-ba0c-d81360967828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14237267754599453,\n",
       " 0.9738125,\n",
       " 0.9738125,\n",
       " 0.32891084301742607,\n",
       " array([[    0,   237,     0],\n",
       "        [    0, 15581,     0],\n",
       "        [    0,   182,     0]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

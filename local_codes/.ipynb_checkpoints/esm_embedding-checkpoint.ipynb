{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6815f4c0-e3a9-4a6e-bf3a-4a57a117f79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, EsmModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "model_name = \"esm2_t6_8M_UR50D\"\n",
    "# os.mkdir(f\"embeddings/{model_name}\")\n",
    "max_seq_len = 2000\n",
    "max_tax_len = 40\n",
    "num_taxonomy_ids = 4118  # Example: Assuming 14,680 possible taxonomy classes\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d9571e-7bca-4d33-af02-ae102a1f69c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9640, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"taxonomy_data.csv\")[:10000]\n",
    "lens = df['Sequence'].apply(len)\n",
    "df = df[lens < max_seq_len]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d381ebb-81cb-4c19-99be-10ce419997d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set Len:  4118\n"
     ]
    }
   ],
   "source": [
    "def list_encoder(s):\n",
    "    return [int(i) for i in s.replace(\"[\", \"\").replace(\"]\", \"\").split(\", \")]\n",
    "\n",
    "# Example list of taxonomy IDs for multiple sequences\n",
    "taxonomy_ids_list = [list_encoder(tax_id_str) for tax_id_str in df['tree trace'].values]\n",
    "\n",
    "set_list = [set(t) for t in taxonomy_ids_list]\n",
    "union_set = set().union(*set_list)\n",
    "print(\"Set Len: \", len(union_set))\n",
    "\n",
    "# Flatten the list of taxonomy IDs and get unique taxonomy IDs\n",
    "all_taxonomy_ids = set([tax_id for sublist in taxonomy_ids_list for tax_id in sublist])\n",
    "# Create a mapping from taxonomy ID to index\n",
    "taxonomy_id_to_idx = {tax_id: idx for idx, tax_id in enumerate(all_taxonomy_ids)}\n",
    "taxonomy_idx_to_id = {idx: tax_id for idx, tax_id in enumerate(all_taxonomy_ids)}\n",
    "\n",
    "# Apply the mapping to each list of taxonomy IDs\n",
    "mapped_taxonomy_ids_list = [[taxonomy_id_to_idx[tax_id] for tax_id in tax_ids] + [0 for _ in range(max_tax_len - len(tax_ids))] for tax_ids in taxonomy_ids_list]\n",
    "\n",
    "# Character vocabulary for protein sequences (20 amino acids + 1 padding)\n",
    "vocab = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(vocab)}  # Start index from 1 for padding\n",
    "\n",
    "# Sequence encoder: Convert the protein sequence into integers\n",
    "def encode_sequence(sequence):\n",
    "    return [char_to_idx.get(char, 0) for char in sequence] + [0 for _ in range(max_seq_len - len(sequence))]  # 0 for unknown characters or padding \n",
    "\n",
    "def encode_taxonomy(taxonomy):\n",
    "    return [1 if _ in taxonomy else 0 for _ in range(num_taxonomy_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032e3077-2ffd-4226-8ad9-4746090a02ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, sequences, taxonomy_ids):\n",
    "        self.sequences = sequences\n",
    "        self.taxonomy_ids = taxonomy_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        taxonomy = self.taxonomy_ids[idx]\n",
    "\n",
    "        # Encode sequence and taxonomy (example encoding for demonstration)\n",
    "        sequence_encoded = torch.tensor(encode_sequence(sequence), dtype=torch.long)\n",
    "        taxonomy_encoded = torch.tensor(encode_taxonomy(taxonomy), dtype=torch.long)\n",
    "\n",
    "        return sequence_encoded, taxonomy_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04c730b5-0b2b-4412-9d33-63e2edc32df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences batch shape: torch.Size([64, 2000])\n",
      "Taxonomy batch shape: torch.Size([64, 4118])\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "train_sequences, test_sequences, train_taxonomy, test_taxonomy = train_test_split(df['Sequence'].values, mapped_taxonomy_ids_list, test_size=0.2, random_state=42)\n",
    "train_sequences, val_sequences, train_taxonomy, val_taxonomy = train_test_split(train_sequences, train_taxonomy, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create Dataset objects\n",
    "train_dataset = ProteinDataset(train_sequences, train_taxonomy)\n",
    "val_dataset = ProteinDataset(val_sequences, val_taxonomy)\n",
    "test_dataset = ProteinDataset(test_sequences, test_taxonomy)\n",
    "\n",
    "# Create DataLoader objects for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Example usage of the DataLoader\n",
    "for sequences_batch, taxonomy_batch in train_loader:\n",
    "    print(f\"Sequences batch shape: {sequences_batch.shape}\")\n",
    "    print(f\"Taxonomy batch shape: {taxonomy_batch.shape}\")\n",
    "    break  # Print one batch and exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546931d-e078-4b5c-ad9e-a292066f9a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.data.iloc[idx]['Sequence']\n",
    "        entry = self.data.iloc[idx]['Entry']\n",
    "        return entry, sequence\n",
    "\n",
    "# Instantiate dataset and dataloader with batch size\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "dataset = ProteinDataset(train_df)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"facebook/{model_name}\")\n",
    "model = EsmModel.from_pretrained(f\"facebook/{model_name}\").to(\"cuda:1\")\n",
    "\n",
    "# Iterate through batches of data\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    entries, sequences = batch\n",
    "    \n",
    "    # Check if the embeddings already exist, and if not, process them\n",
    "    # if f\"{batch_idx}.pt\" not in os.listdir(f\"embeddings/{model_name}\"):\n",
    "    print(f\"Processing: {batch_idx}\")\n",
    "\n",
    "    # Tokenize the batch of sequences and move inputs to GPU\n",
    "    inputs = tokenizer(sequences, return_tensors=\"pt\", padding=True).to(\"cuda:1\")\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model(**inputs).last_hidden_state\n",
    "\n",
    "    # Compute mean across the sequence dimension (or any other pooling method)\n",
    "    output_embeddings = outputs.mean(dim=1).cpu()  # Move back to CPU\n",
    "    print(output_embeddings.shape)\n",
    "\n",
    "    # Save the embeddings\n",
    "    torch.save(output_embeddings, f\"embeddings/{model_name}/{batch_idx}.pt\")\n",
    "    \n",
    "    # Clear cache after each batch to avoid memory overflow\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Batch {batch_idx + 1}/{len(dataloader)} processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e69591ca-a7bc-45eb-91c3-4c20aa83af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with an Attention Layer\n",
    "class SimpleAttentionClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_taxonomy_ids):\n",
    "        super(SimpleAttentionClassifier, self).__init__()\n",
    "        # Embedding layer for sequences\n",
    "        self.sequence_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=4, batch_first=True)\n",
    "        # Fully connected layer for predicting taxonomy\n",
    "        self.fc = nn.Linear(embedding_dim, num_taxonomy_ids)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Embed the input sequences\n",
    "        embedded_seq = self.sequence_embedding(sequences)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Attention mechanism (self-attention here)\n",
    "        attn_output, _ = self.attention(embedded_seq, embedded_seq, embedded_seq)  # (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Mean pooling across the sequence length dimension\n",
    "        attn_output = attn_output.mean(dim=1)  # (batch_size, embed_dim)\n",
    "        \n",
    "        # Pass through a fully connected layer to predict taxonomy IDs\n",
    "        output = self.fc(attn_output)  # (batch_size, num_taxonomy_ids)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0482858d-4781-4bb9-83c0-2134a6b1901a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size = len(vocab) + 1  # +1 for padding\n",
    "embedding_dim = 16\n",
    "hidden_dim = 64\n",
    "num_taxonomy_ids = 4118  # Example: Assuming 14,680 possible taxonomy classes\n",
    "num_epochs = 10\n",
    "device = \"cuda:1\"\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "model = SimpleAttentionClassifier(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim, num_taxonomy_ids=num_taxonomy_ids).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Multi-label classification requires BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3de31fff-9adf-4d46-a058-08ccdeaa11b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.3698\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0319, val Accuracy: 0.9949, val F1 Score: 0.3289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.0202\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0168, val Accuracy: 0.9949, val F1 Score: 0.3376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.0162\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0158, val Accuracy: 0.9949, val F1 Score: 0.3406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:14,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.0156\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0155, val Accuracy: 0.9949, val F1 Score: 0.3408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.0153\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0154, val Accuracy: 0.9949, val F1 Score: 0.3381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.0152\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0153, val Accuracy: 0.9949, val F1 Score: 0.3420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.0151\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0152, val Accuracy: 0.9949, val F1 Score: 0.3415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 0.0151\n",
      "2 33154 33208 33213 131567 33511 2759 117570 117571 6072 89593 7711 7742 7776 32523\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0153, val Accuracy: 0.9951, val F1 Score: 0.4784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 0.0150\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0152, val Accuracy: 0.9949, val F1 Score: 0.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [00:15,  5.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.0150\n",
      "2 33154 33208 33213 131567 2759 6072\n",
      "2 131567 2157 2258 2259 2260 183968 28890 53953 70601\n",
      "val Loss: 0.0152, val Accuracy: 0.9949, val F1 Score: 0.3422\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (sequences, taxonomy_ids) in tqdm(enumerate(train_loader)):\n",
    "        # print(\"Running Batch idx:\", batch_idx, len(train_loader))\n",
    "        \n",
    "        sequences = sequences.to(device)\n",
    "        taxonomy_ids = taxonomy_ids.to(device)\n",
    "\n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, taxonomy_ids.float())\n",
    "\n",
    "        # Backpropagation: Zero the gradients, compute the backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print loss for this epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    val_loss, val_accuracy, val_f1 = evaluate(model, val_loader, criterion)\n",
    "    print(f\"val Loss: {val_loss:.4f}, val Accuracy: {val_accuracy:.4f}, val F1 Score: {val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "761053d9-9b70-451c-9fbc-e3347390cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device='cuda:1'):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for batch_idx, (sequences, taxonomy_ids) in enumerate(test_loader):\n",
    "            sequences = sequences.to(device)\n",
    "            taxonomy_ids = taxonomy_ids.to(device)\n",
    "\n",
    "            outputs = model(sequences)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, taxonomy_ids.float())\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Convert model outputs to binary predictions (e.g., threshold = 0.5)\n",
    "            preds = torch.sigmoid(outputs) > 0.6  # Binary predictions\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(taxonomy_ids.cpu())\n",
    "    \n",
    "    # Concatenate all batches into single tensors\n",
    "    all_preds = torch.cat(all_preds).int()\n",
    "    # print(all_preds[0])\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    # print(all_labels[0])\n",
    "    \n",
    "    for p, l in zip(all_preds, all_labels):\n",
    "        print(*[taxonomy_idx_to_id[i] for i, p_ in enumerate(p) if p_])\n",
    "        print(*[taxonomy_idx_to_id[i] for i, l_ in enumerate(l) if l_])\n",
    "        break\n",
    "    \n",
    "    # Compute evaluation metrics (example: accuracy, F1 score)\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    \n",
    "    accuracies = [accuracy_score(p, l) for p, l in zip(all_preds, all_labels)]\n",
    "    accuracy = np.mean(accuracies)\n",
    "    \n",
    "    f1 = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')  # F1-score for multi-label classification\n",
    "    avg_loss = running_loss / len(test_loader)\n",
    "    \n",
    "    return avg_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65314d6-fd1e-408d-80ba-fdbb248c7541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

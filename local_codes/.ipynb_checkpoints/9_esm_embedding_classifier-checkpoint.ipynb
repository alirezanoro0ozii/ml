{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c217af6-e89a-4ec6-be30-7e1d72a1124b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "../checkpoints/esm_hierarchy_checkpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wcdbwxd2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">eager-donkey-1</strong> at: <a href='https://wandb.ai/alirezanor-310-ai/test%20classification/runs/wcdbwxd2' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/test%20classification/runs/wcdbwxd2</a><br/> View project at: <a href='https://wandb.ai/alirezanor-310-ai/test%20classification' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/test%20classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241112_153836-wcdbwxd2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wcdbwxd2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241112_154304-yntibh92</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alirezanor-310-ai/test%20classification/runs/yntibh92' target=\"_blank\">efficient-tree-2</a></strong> to <a href='https://wandb.ai/alirezanor-310-ai/test%20classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alirezanor-310-ai/test%20classification' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/test%20classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alirezanor-310-ai/test%20classification/runs/yntibh92' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/test%20classification/runs/yntibh92</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from data_process import *\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs= 10_000\n",
    "val_epoch = 500\n",
    "num_val = 1000\n",
    "\n",
    "model_name = \"esm_hierarchy\"\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"test classification\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.02,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"CIFAR-100\",\n",
    "    \"epochs\": 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "config = \"embedding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b17e331f-93bc-46c5-b849-cdcbd9f192ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNClassifier(nn.Module):\n",
    "    def __init__(self, model, embedding_dim, num_classes, max_seq_len, hidden_dim):\n",
    "        super(FNNClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.fc1 = nn.Linear(embedding_dim * max_seq_len, hidden_dim)  # Output size depends on conv and pooling layers\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes, bias=False)\n",
    "        self.fc3 = nn.Linear(embedding_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model == \"Flat\":\n",
    "            x = x.view(x.size(0), -1)  # Flatten the output for the fully connected layer\n",
    "            x = torch.relu(self.fc1(x))\n",
    "\n",
    "        elif self.model == \"Mean\":\n",
    "            x = x.mean(dim=1)\n",
    "            x = torch.relu(self.fc3(x))\n",
    "\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc645d81-5c2a-4a02-9578-3b41322133b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 1000\n",
    "max_tax_len = 150\n",
    "\n",
    "# Character vocabulary for protein sequences (20 amino acids + 1 padding)\n",
    "vocab = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(vocab)}  # Start index from 1 for padding\n",
    "# Sequence encoder: Convert the protein sequence into integers\n",
    "def encode_sequence(sequence):\n",
    "    return [char_to_idx.get(char, 0) for char in sequence] + [0 for _ in range(max_seq_len - len(sequence))]  # 0 for unknown characters or padding \n",
    "\n",
    "def encode_sequence_batch(sequences):\n",
    "    return torch.Tensor([encode_sequence(s) for s in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0482858d-4781-4bb9-83c0-2134a6b1901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 164.006912 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "vocab_dim = 21\n",
    "embedding_dim = 320\n",
    "hidden_dim = 512\n",
    "num_taxonomy_ids = 4  # Example: Assuming 14,680 possible taxonomy classes\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "ESM_model = FNNClassifier(\n",
    "    \"Flat\",\n",
    "    embedding_dim,\n",
    "    num_taxonomy_ids,\n",
    "    1000,\n",
    "    hidden_dim\n",
    ").to(device)\n",
    "\n",
    "onehot_model = FNNClassifier(\n",
    "    \"Flat\",\n",
    "    21,\n",
    "    num_taxonomy_ids,\n",
    "    1000,\n",
    "    hidden_dim\n",
    ").to(device)\n",
    "\n",
    "\n",
    "model = ESM_model if config == \"embedding\" else onehot_model\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"model:\", sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bd613b7-6f9c-47e9-9587-f329de508780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "            tensor_batch = esm_hierarchy_data_to_tensor_batch('val', epoch)\n",
    "            tensor_batch.gpu(device)\n",
    "                \n",
    "            sequences = tensor_batch.seq_ids\n",
    "            labels = tensor_batch.taxes[\"begining root\"]\n",
    "            \n",
    "            outputs = model(sequences)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "                \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches into single tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Compute evaluation metrics (example: accuracy, F1 score)\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    f1_macro = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')  # F1-score for multi-label classification\n",
    "    f1_micro = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')  # F1-score for multi-label classification\n",
    "    conf_matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "    avg_loss = running_loss / num_val\n",
    "    \n",
    "    return avg_loss, accuracy, f1_micro, f1_macro, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3de31fff-9adf-4d46-a058-08ccdeaa11b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m      5\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tensor_batch, sequences \u001b[38;5;241m=\u001b[39m \u001b[43mesm_hierarchy_data_to_tensor_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tensor_batch\u001b[38;5;241m.\u001b[39mgpu(device)\n\u001b[1;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m tensor_batch\u001b[38;5;241m.\u001b[39mtaxes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegining root\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/home/aac/Alireza/local_codes/../dlp/data_process.py:246\u001b[0m, in \u001b[0;36mesm_hierarchy_data_to_tensor_batch\u001b[0;34m(split, i)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    tensor_batch, sequences = esm_hierarchy_data_to_tensor_batch('train', epoch)\n",
    "    tensor_batch.gpu(device)\n",
    "\n",
    "    labels = tensor_batch.taxes[\"begining root\"]\n",
    "    onehot_tensors = encode_sequence_batch(sequences).to(device)\n",
    "    \n",
    "    if config == \"embedding\":\n",
    "        print(tensor_batch.seq_ids[0])\n",
    "        outputs = model(tensor_batch.seq_ids)\n",
    "    else:\n",
    "        outputs = model(onehot_tensors)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backpropagation: Zero the gradients, compute the backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the loss\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        # Print loss for this epoch\n",
    "        epoch_loss = running_loss / (epoch + 1)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        val_loss, val_accuracy, val_f1_micro, val_f1_macro, cm = evaluate()\n",
    "        print(cm)\n",
    "        print(f\"val Loss: {val_loss:.4f}, val Accuracy: {val_accuracy:.4f}, val F1 Score (micro): {val_f1_micro:.4f}, , val F1 Score (macro): {val_f1_macro:.4f}\")\n",
    "\n",
    "        wandb.log({\"train loss\": epoch_loss, \"val acc\": val_accuracy, \"val loss\": val_loss})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a637234-a1a9-472f-ba0c-d81360967828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

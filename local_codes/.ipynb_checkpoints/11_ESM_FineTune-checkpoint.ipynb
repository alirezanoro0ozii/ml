{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b39f58-2269-4752-b40c-f09a62fea9f0",
   "metadata": {},
   "source": [
    "Fine Tune ESM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7cd1b0e-e0e4-4fd8-a680-ccdf0e1918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "../checkpoints/Fine Tune ESM_checkpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:41gufcke) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">celestial-dragon-1</strong> at: <a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM/runs/41gufcke' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM/runs/41gufcke</a><br/> View project at: <a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241113_194131-41gufcke/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:41gufcke). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241113_194216-py9gsol4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM/runs/py9gsol4' target=\"_blank\">young-butterfly-2</a></strong> to <a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM/runs/py9gsol4' target=\"_blank\">https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM/runs/py9gsol4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alirezanor-310-ai/Fine%20Tune%20ESM/runs/py9gsol4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7a4b0ade4100>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from data_process import simple_data_to_tensor_batch\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 10_000\n",
    "val_epoch = 500\n",
    "num_val = 1000\n",
    "batch_size = 64\n",
    "dataset_name = \"corpus_1000_random\"\n",
    "lr = 0.001\n",
    "model_name = \"Fine Tune ESM\"\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq/{dataset_name}\", batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": \"ESM + FNN\",\n",
    "    \"dataset\": dataset_name,\n",
    "    \"epochs\": epochs,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2cb1410-5ec8-4c88-854d-be599e54e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmModel\n",
    "\n",
    "class ESM1b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")\n",
    "        self.layer1 = nn.Linear(1280, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(512, 4)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.esm(x, attention_mask=attention_mask)\n",
    "        # Mean pooling over all residues\n",
    "        outputs = outputs.last_hidden_state.mean(dim=1)\n",
    "        outputs = self.layer1(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.layer2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37fede89-9b77-4f1e-951b-3f6a2600d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 653.014425 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = ESM1b().to(device)\n",
    "print(\"model:\", sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb03ada3-72a3-4e55-a292-7773574ce08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "            tensor_batch = simple_data_to_tensor_batch(da.get_batch())\n",
    "            tensor_batch.gpu(device)\n",
    "        \n",
    "            labels = tensor_batch.taxes[\"begining root\"]\n",
    "            \n",
    "            outputs = model(**tensor_batch.seq_ids)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "                \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches into single tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Compute evaluation metrics (example: accuracy, F1 score)\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    f1_macro = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')  # F1-score for multi-label classification\n",
    "    f1_micro = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')  # F1-score for multi-label classification\n",
    "    conf_matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "    avg_loss = running_loss / num_val\n",
    "    \n",
    "    return avg_loss, accuracy, f1_micro, f1_macro, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc83e16a-38c2-4b78-b15e-22bccd5fb378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Set model to training mode\u001b[39;00m\n\u001b[1;32m      5\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m tensor_batch \u001b[38;5;241m=\u001b[39m \u001b[43msimple_data_to_tensor_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tensor_batch\u001b[38;5;241m.\u001b[39mgpu(device)\n\u001b[1;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m tensor_batch\u001b[38;5;241m.\u001b[39mtaxes[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegining root\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/home/aac/Alireza/local_codes/../dlp/data_process.py:253\u001b[0m, in \u001b[0;36msimple_data_to_tensor_batch\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    251\u001b[0m combined_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m tax_ids:\n\u001b[0;32m--> 253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m    254\u001b[0m         combined_dict\u001b[38;5;241m.\u001b[39msetdefault(key, [])\u001b[38;5;241m.\u001b[39mextend(value)\n\u001b[1;32m    256\u001b[0m tensor_encoded \u001b[38;5;241m=\u001b[39m {k: torch\u001b[38;5;241m.\u001b[39mLongTensor(v) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m combined_dict\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    tensor_batch = simple_data_to_tensor_batch(da.get_batch())\n",
    "    tensor_batch.gpu(device)\n",
    "\n",
    "    labels = tensor_batch.taxes[\"begining root\"]\n",
    "    \n",
    "    outputs = model(**tensor_batch.seq_ids)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backpropagation: Zero the gradients, compute the backward pass, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the loss\n",
    "    running_loss += loss.item()\n",
    "\n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        # Print loss for this epoch\n",
    "        epoch_loss = running_loss / (epoch + 1)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Evaluate the model on the test set\n",
    "        val_loss, val_accuracy, val_f1_micro, val_f1_macro, cm = evaluate()\n",
    "        print(cm)\n",
    "        print(f\"val Loss: {val_loss:.4f}, val Accuracy: {val_accuracy:.4f}, val F1 Score (micro): {val_f1_micro:.4f}, , val F1 Score (macro): {val_f1_macro:.4f}\")\n",
    "\n",
    "        wandb.log({\"train loss\": epoch_loss, \"val acc\": val_accuracy, \"val loss\": val_loss})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b487e-69ce-4c41-be78-a1b6dcb58e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

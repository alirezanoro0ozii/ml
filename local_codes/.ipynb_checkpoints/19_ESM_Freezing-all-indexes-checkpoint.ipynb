{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7cd1b0e-e0e4-4fd8-a680-ccdf0e1918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "../checkpoints/Freeze ESM Multiple head_checkpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1xdj84ly) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">playful-paper-4</strong> at: <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head/runs/1xdj84ly' target=\"_blank\">https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head/runs/1xdj84ly</a><br/> View project at: <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head' target=\"_blank\">https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241126_085929-1xdj84ly/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1xdj84ly). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241126_090032-llvorwcw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head/runs/llvorwcw' target=\"_blank\">comfy-dew-5</a></strong> to <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head' target=\"_blank\">https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head/runs/llvorwcw' target=\"_blank\">https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head/runs/llvorwcw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20Multiple%20head/runs/llvorwcw?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7e9e794176d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "import json\n",
    "from transformers import EsmModel, AutoTokenizer\n",
    "\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from batch import Batch\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 100_000\n",
    "val_epoch = 1000\n",
    "num_val = 500\n",
    "batch_size = 16\n",
    "dataset_name = \"corpus_200_500_random\"\n",
    "lr = 0.001\n",
    "model_name = \"Freeze ESM Multiple head\"\n",
    "max_seq_len = 500\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{dataset_name}\", 2 * batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"Freeze ESM Multiple Heads\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_szie\": batch_size,\n",
    "        \"max_seq_len\": max_seq_len\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b8f22e-a680-496e-973a-a7b7c552913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 4, '1': 35, '2': 1368, '3': 6024, '4': 4330, '5': 5265, '6': 5453, '7': 15592, '8': 65895, '9': 59786, '10': 54221, '11': 141660, '12': 20431, '13': 13635, '14': 17392, '15': 32172, '16': 35581, '17': 34356, '18': 33115, '19': 43504, '20': 61510, '21': 57903, '22': 31214, '23': 32835, '24': 107520, '25': 82492, '26': 96471, '27': 90666, '28': 85207, '29': 70508, '30': 71902, '31': 26728, '32': 11716, '33': 6444, '34': 2510, '35': 872, '36': 4}\n",
      "\n",
      "Taxonomic ranks sorted by number of taxa:\n",
      "11: 141660\n",
      "24: 107520\n",
      "26: 96471\n",
      "27: 90666\n",
      "28: 85207\n",
      "25: 82492\n",
      "30: 71902\n",
      "29: 70508\n",
      "8: 65895\n",
      "20: 61510\n",
      "9: 59786\n",
      "21: 57903\n",
      "10: 54221\n",
      "19: 43504\n",
      "16: 35581\n",
      "17: 34356\n",
      "18: 33115\n",
      "23: 32835\n",
      "15: 32172\n",
      "22: 31214\n",
      "31: 26728\n",
      "12: 20431\n",
      "14: 17392\n",
      "7: 15592\n",
      "13: 13635\n",
      "32: 11716\n",
      "33: 6444\n",
      "3: 6024\n",
      "6: 5453\n",
      "5: 5265\n",
      "4: 4330\n",
      "34: 2510\n",
      "2: 1368\n",
      "35: 872\n",
      "1: 35\n",
      "0: 4\n",
      "36: 4\n"
     ]
    }
   ],
   "source": [
    "index2name_file = \"../data/index2name.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(index2name_file):\n",
    "    with open(index2name_file, \"rb\") as f:\n",
    "        index2name = json.load(f)\n",
    "\n",
    "tax_vocab_sizes = {\n",
    "    k: len(v) for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "print(tax_vocab_sizes)\n",
    "# Print tax_vocab_sizes sorted by value (number of taxa per rank)\n",
    "sorted_sizes = dict(sorted(tax_vocab_sizes.items(), key=lambda x: x[1], reverse=True))\n",
    "print(\"\\nTaxonomic ranks sorted by number of taxa:\")\n",
    "for rank, size in sorted_sizes.items():\n",
    "    print(f\"{rank}: {size}\")\n",
    "\n",
    "level_encoder = {\n",
    "    k: {name: idx + 1 for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "level_decoder = {\n",
    "    k: {idx + 1: name for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "def encode_lineage(lineage_str):\n",
    "    taxes_str = lineage_str.split(\", \")\n",
    "    # print(taxes_str)\n",
    "\n",
    "    encoded = {k: [0] for k in rank2name.keys()}\n",
    "    \n",
    "    for i, tax_str in enumerate(taxes_str):\n",
    "        encoded[i][0] = level_encoder[i].get(tax_str, 0)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(f\"facebook/esm1b_t33_650M_UR50S\")\n",
    "\n",
    "def data_to_tensor_batch(b):\n",
    "    inputs = tokenizer_(\n",
    "        [e['Sequence'] for e in b],\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len\n",
    "    )\n",
    "\n",
    "    tax_ids = [encode_lineage(e['Taxonomic_lineage__ALL_']) for e in b]\n",
    "    combined_dict = {}\n",
    "    for d in tax_ids:\n",
    "        for key, value in d.items():\n",
    "            combined_dict.setdefault(key, []).extend(value)\n",
    "\n",
    "    tensor_encoded = {k: torch.LongTensor(v) for k,v in combined_dict.items()}\n",
    "    return Batch(inputs, tensor_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2cb1410-5ec8-4c88-854d-be599e54e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM2(nn.Module):\n",
    "    def __init__(self, num_classes_dict):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "        \n",
    "        # Freeze ESM parameters\n",
    "        for param in self.esm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Create separate classification heads for each task\n",
    "        self.heads = nn.ModuleDict()\n",
    "        for index_name, num_classes in num_classes_dict.items():\n",
    "            self.heads[index_name] = nn.Sequential(\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, index=None):\n",
    "        # Get ESM embeddings\n",
    "        embeddings = self.esm(x, attention_mask=attention_mask).pooler_output\n",
    "        \n",
    "        # If specific task requested, return only that output\n",
    "        if index is not None:\n",
    "            if index not in self.heads:\n",
    "                raise ValueError(f\"Task {index} not found in model heads\")\n",
    "            return self.heads[index](embeddings)\n",
    "        \n",
    "        # Otherwise return all task outputs\n",
    "        return {index: head(embeddings) for index, head in self.heads.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37fede89-9b77-4f1e-951b-3f6a2600d5cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 755.969937 M\n",
      "Total parameters: 1408.323878 M\n",
      "ESM2(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-32): 33 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (rotary_embeddings): RotaryEmbedding()\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=35, bias=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1368, bias=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=6024, bias=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=4330, bias=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=5265, bias=True)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=5453, bias=True)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=15592, bias=True)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=65895, bias=True)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=59786, bias=True)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=54221, bias=True)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=141660, bias=True)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=20431, bias=True)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=13635, bias=True)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=17392, bias=True)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=32172, bias=True)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=35581, bias=True)\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=34356, bias=True)\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=33115, bias=True)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=43504, bias=True)\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=61510, bias=True)\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=57903, bias=True)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=31214, bias=True)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=32835, bias=True)\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=107520, bias=True)\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=82492, bias=True)\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=96471, bias=True)\n",
      "    )\n",
      "    (27): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=90666, bias=True)\n",
      "    )\n",
      "    (28): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=85207, bias=True)\n",
      "    )\n",
      "    (29): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=70508, bias=True)\n",
      "    )\n",
      "    (30): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=71902, bias=True)\n",
      "    )\n",
      "    (31): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=26728, bias=True)\n",
      "    )\n",
      "    (32): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=11716, bias=True)\n",
      "    )\n",
      "    (33): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=6444, bias=True)\n",
      "    )\n",
      "    (34): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=2510, bias=True)\n",
      "    )\n",
      "    (35): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=872, bias=True)\n",
      "    )\n",
      "    (36): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ESM2(tax_vocab_sizes).to(device)\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable parameters: {trainable/ 1e6} M')\n",
    "print(f'Total parameters: {total/ 1e6} M')\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Initial restart interval\n",
    "    T_mult=2,  # Multiply interval by 2 after each restart\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb03ada3-72a3-4e55-a292-7773574ce08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = [da.get_batch() for _ in range(num_val)]\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    losses = {k: 0.0 for k in tax_vocab_sizes.keys()}\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "            tensor_batch = data_to_tensor_batch(val_batches[epoch])\n",
    "            tensor_batch.gpu(device)\n",
    "        \n",
    "            labels = tensor_batch.taxes\n",
    "            outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "\n",
    "            # Calculate individual task losses\n",
    "            batch_loss = 0\n",
    "            for k in tax_vocab_sizes.keys():\n",
    "                task_loss = criterion(outputs[k], labels[k])\n",
    "                losses[k] += task_loss.item()\n",
    "                batch_loss += task_loss\n",
    "            \n",
    "            running_loss += batch_loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / num_val\n",
    "    avg_index_losses = {k: losses[k]/num_val for k in losses}\n",
    "    \n",
    "    return avg_index_losses, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba451553-07c8-4608-821a-28078d720548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def load_checkpoint(model, optimizer=None, scheduler=None):\n",
    "    checkpoints = glob.glob(os.path.join(\"new data Fine Tune ESM uniform sampling_checkpoints\", 'checkpoint_epoch_*.pt'))        \n",
    "    # Extract epoch numbers and find latest\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Cosine annealing with warm restarts\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer,\n",
    "        T_0=10,  # Initial restart interval\n",
    "        T_mult=2,  # Multiply interval by 2 after each restart\n",
    "        eta_min=1e-6  # Minimum learning rate\n",
    "    )\n",
    "    \n",
    "    # Load optimizer state if provided (for training)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Move optimizer state to GPU if necessary\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get training metadata\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "    \n",
    "    print(f\"Successfully loaded checkpoint from epoch {epoch}\")\n",
    "    print(\"Metrics at checkpoint:\", metrics)\n",
    "    \n",
    "    return model, optimizer, scheduler, epoch, metrics\n",
    "        \n",
    "\n",
    "# model, optimizer, scheduler, latest_epoch, metrics = load_checkpoint(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b487e-69ce-4c41-be78-a1b6dcb58e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 999/100000 [08:28<13:55:31,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/100000]\n",
      "Train Loss: 0.1842, Train Accuracy: 0.9389\n",
      "Train F1 (micro): 0.9389, Train F1 (macro): 0.6274\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 7618  382    0]\n",
      " [   0  520 7404    0]\n",
      " [   0   14   62    0]]\n",
      "Val Loss: 0.1754, Val Accuracy: 0.9381\n",
      "Val F1 (micro): 0.9381, Val F1 (macro): 0.4357\n",
      "Val Confusion Matrix:\n",
      "[[    0     0     0     0]\n",
      " [    0   230     8     0]\n",
      " [    0   868 14780     0]\n",
      " [    0    22    92     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 1999/100000 [25:13<13:47:04,  1.97it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2000/100000]\n",
      "Train Loss: 0.1410, Train Accuracy: 0.9526\n",
      "Train F1 (micro): 0.9526, Train F1 (macro): 0.6364\n",
      "Train Confusion Matrix:\n",
      "[[   0    0    0    0]\n",
      " [   0 7719  281    0]\n",
      " [   0  408 7523    0]\n",
      " [   0   10   59    0]]\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "current_lr = lr\n",
    "losses = {k: 0.0 for k in tax_vocab_sizes.keys()}\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "\n",
    "    tensor_batch = data_to_tensor_batch(da.get_batch())\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    labels = tensor_batch.taxes\n",
    "    outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "    \n",
    "    batch_loss = 0\n",
    "    for k in tax_vocab_sizes.keys():\n",
    "        task_loss = criterion(outputs[k], labels[k])\n",
    "        losses[k] += task_loss.item()\n",
    "        batch_loss += task_loss\n",
    "    \n",
    "    running_loss += batch_loss.item()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        train_loss = running_loss / val_epoch\n",
    "        val_losses, val_loss = evaluate(model)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Create metrics dictionary for saving\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"lr\": current_lr\n",
    "        }\n",
    "        for k in tax_vocab_sizes.keys():\n",
    "            metrics[f\"val_loss_{k}\"] = val_losses[k]\n",
    "            metrics[f\"train_loss_{k}\"] = losses[k]\n",
    "            \n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch + loss.item())\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Reset training metrics\n",
    "        running_loss = 0\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

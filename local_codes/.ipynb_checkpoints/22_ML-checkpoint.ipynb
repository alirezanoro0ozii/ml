{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cd1b0e-e0e4-4fd8-a680-ccdf0e1918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      " WORLD_SIZE=1 , LOCAL_WORLD_SIZE=1,RANK =0,LOCAL_RANK = 0 \n",
      "../checkpoints/Freeze ESM All head_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malirezanor\u001b[0m (\u001b[33malireza_noroozi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241128_053345-5m63ku52</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head/runs/5m63ku52' target=\"_blank\">northern-feather-6</a></strong> to <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head' target=\"_blank\">https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head/runs/5m63ku52' target=\"_blank\">https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head/runs/5m63ku52</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head/runs/5m63ku52?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x71e71cbd98a0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "import json\n",
    "from transformers import EsmModel, AutoTokenizer\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from batch import Batch\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 100_000\n",
    "val_epoch = 1_000\n",
    "num_val = 20\n",
    "batch_size = 16\n",
    "virus_dataset_name = \"corpus_1000_Viruses\"\n",
    "cellular_dataset_name = \"corpus_1000_cellular\"\n",
    "lr = 0.001\n",
    "model_name = \"Freeze ESM All head\"\n",
    "max_seq_len = 1000\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "virus_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{virus_dataset_name}\", batch_size)\n",
    "cellular_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{cellular_dataset_name}\", batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": model_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_szie\": batch_size,\n",
    "        \"max_seq_len\": max_seq_len\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b8f22e-a680-496e-973a-a7b7c552913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 33, 2: 1298, 3: 5633, 4: 3733, 5: 5181, 6: 5444, 7: 15383, 8: 65228, 9: 59006, 10: 53852, 11: 141571, 12: 20369, 13: 13615, 14: 17388, 15: 32162, 16: 35564, 17: 34333, 18: 33100, 19: 43499, 20: 61507, 21: 57902, 22: 31211, 23: 32827, 24: 107510, 25: 82470, 26: 96454, 27: 90631, 28: 85202, 29: 70506, 30: 71899, 31: 26726, 32: 11716, 33: 6444, 34: 2510, 35: 872, 36: 4}\n",
      "\n",
      "Taxonomic ranks sorted by number of taxa:\n",
      "11: 141571\n",
      "24: 107510\n",
      "26: 96454\n",
      "27: 90631\n",
      "28: 85202\n",
      "25: 82470\n",
      "30: 71899\n",
      "29: 70506\n",
      "8: 65228\n",
      "20: 61507\n",
      "9: 59006\n",
      "21: 57902\n",
      "10: 53852\n",
      "19: 43499\n",
      "16: 35564\n",
      "17: 34333\n",
      "18: 33100\n",
      "23: 32827\n",
      "15: 32162\n",
      "22: 31211\n",
      "31: 26726\n",
      "12: 20369\n",
      "14: 17388\n",
      "7: 15383\n",
      "13: 13615\n",
      "32: 11716\n",
      "33: 6444\n",
      "3: 5633\n",
      "6: 5444\n",
      "5: 5181\n",
      "4: 3733\n",
      "34: 2510\n",
      "2: 1298\n",
      "35: 872\n",
      "1: 33\n",
      "36: 4\n",
      "0: 2\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index2name_file = \"../data/taxonomy_index.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(index2name_file):\n",
    "    with open(index2name_file, \"rb\") as f:\n",
    "        index2name = json.load(f)\n",
    "\n",
    "tax_vocab_sizes = {\n",
    "    int(k): len(v) for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "print(tax_vocab_sizes)\n",
    "# Print tax_vocab_sizes sorted by value (number of taxa per rank)\n",
    "sorted_sizes = dict(sorted(tax_vocab_sizes.items(), key=lambda x: x[1], reverse=True))\n",
    "print(\"\\nTaxonomic ranks sorted by number of taxa:\")\n",
    "for rank, size in sorted_sizes.items():\n",
    "    print(f\"{rank}: {size}\")\n",
    "\n",
    "level_encoder = {\n",
    "    int(k): {name: idx + 1 for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "level_decoder = {\n",
    "    int(k): {idx + 1: name for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "for k, v in level_decoder.items():\n",
    "    level_decoder[k][0] = \"NOT DEFINED\"\n",
    "\n",
    "\n",
    "def encode_lineage(lineage_str):\n",
    "    taxes_str = lineage_str.split(\", \")\n",
    "\n",
    "    encoded = {int(k): 0 for k in index2name.keys()}\n",
    "    \n",
    "    for i, tax_str in enumerate(taxes_str):\n",
    "        encoded[i] = level_encoder[i][tax_str]\n",
    "\n",
    "    return encoded\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(f\"facebook/esm1b_t33_650M_UR50S\")\n",
    "\n",
    "def mix_data_to_tensor_batch(b_virues, b_cellular, max_seq_len=max_seq_len, partition=0.25):\n",
    "    split_point = int(len(b_virues) * partition)\n",
    "    b = b_virues[:split_point] + b_cellular[-len(b_virues) + split_point:]\n",
    "    random.shuffle(b)  # In-place shuffle\n",
    "    \n",
    "    inputs = tokenizer_(\n",
    "        [e['Sequence'] for e in b],\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len\n",
    "    )\n",
    "\n",
    "    tax_ids = [encode_lineage(e['Taxonomic_lineage__ALL_']) for e in b]\n",
    "    combined_dict = {}\n",
    "    for d in tax_ids:\n",
    "        for key, value in d.items():\n",
    "            combined_dict.setdefault(key, []).append(value)\n",
    "\n",
    "    tensor_encoded = {k: torch.LongTensor(v) for k,v in combined_dict.items()}\n",
    "    return Batch(inputs, tensor_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2cb1410-5ec8-4c88-854d-be599e54e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class ESM1b(nn.Module):\n",
    "    def __init__(self, num_classes_dict):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")\n",
    "        \n",
    "        # Freeze ESM parameters\n",
    "        for param in self.esm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.heads = nn.ModuleDict()\n",
    "        for index_name, num_classes in num_classes_dict.items():\n",
    "            self.heads[str(index_name)] = nn.Sequential(\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "    \n",
    "    def get_optimizers(self, base_lr=1e-4):\n",
    "        optimizers = {}\n",
    "        for index, head in self.heads.items():\n",
    "            optimizers[index] = torch.optim.Adam(head.parameters(), lr=base_lr)\n",
    "        return optimizers\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, index=None):\n",
    "        embeddings = self.esm(x, attention_mask=attention_mask)\n",
    "        outputs = embeddings.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        if index is not None:\n",
    "            if index not in self.heads:\n",
    "                raise ValueError(f\"Task {index} not found in model heads\")\n",
    "            return self.heads[index](outputs)\n",
    "        \n",
    "        return {index: head(outputs) for index, head in self.heads.items()}\n",
    "\n",
    "# Usage example\n",
    "model = ESM1b(num_classes_dict={0: 10, 1: 20})\n",
    "optimizers = model.get_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37fede89-9b77-4f1e-951b-3f6a2600d5cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 754.155969 M\n",
      "Total parameters: 1406.51247 M\n",
      "ESM1b(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-32): 33 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=33, bias=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=1298, bias=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=5633, bias=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=3733, bias=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=5181, bias=True)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=5444, bias=True)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=15383, bias=True)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=65228, bias=True)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=59006, bias=True)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=53852, bias=True)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=141571, bias=True)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=20369, bias=True)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=13615, bias=True)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=17388, bias=True)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=32162, bias=True)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=35564, bias=True)\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=34333, bias=True)\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=33100, bias=True)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=43499, bias=True)\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=61507, bias=True)\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=57902, bias=True)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=31211, bias=True)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=32827, bias=True)\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=107510, bias=True)\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=82470, bias=True)\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=96454, bias=True)\n",
      "    )\n",
      "    (27): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=90631, bias=True)\n",
      "    )\n",
      "    (28): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=85202, bias=True)\n",
      "    )\n",
      "    (29): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=70506, bias=True)\n",
      "    )\n",
      "    (30): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=71899, bias=True)\n",
      "    )\n",
      "    (31): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=26726, bias=True)\n",
      "    )\n",
      "    (32): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=11716, bias=True)\n",
      "    )\n",
      "    (33): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=6444, bias=True)\n",
      "    )\n",
      "    (34): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=2510, bias=True)\n",
      "    )\n",
      "    (35): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=872, bias=True)\n",
      "    )\n",
      "    (36): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ESM1b(tax_vocab_sizes).to(device)\n",
    "optimizers = model.get_optimizers()\n",
    "\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable parameters: {trainable/ 1e6} M')\n",
    "print(f'Total parameters: {total/ 1e6} M')\n",
    "print(model)\n",
    "\n",
    "# optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizers['0'],\n",
    "    T_0=10,  # Initial restart interval\n",
    "    T_mult=2,  # Multiply interval by 2 after each restart\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33d5aae5-876d-48ad-be0c-31c7acba9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "val_dir = f\"val_results/{model_name}\"\n",
    "if not os.path.exists(val_dir):\n",
    "    os.makedirs(val_dir)\n",
    "    \n",
    "val_batches = [virus_da.get_batch() for _ in range(num_val // 2)] + [cellular_da.get_batch() for _ in range(num_val // 2)]\n",
    "\n",
    "input_sequences = [e['Sequence'] for b in val_batches for e in b]\n",
    "labels_ = [encode_lineage(e['Taxonomic_lineage__ALL_'])  for b in val_batches for e in b]\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    df = {\n",
    "        i : {\n",
    "            \"sequence\": [],\n",
    "            \"label\": [],\n",
    "            \"pred\": [],\n",
    "            \"loss\": []\n",
    "        } for i in tax_vocab_sizes.keys()\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        i : {\n",
    "            \"loss\": 0,\n",
    "            \"accuracy\": 0,\n",
    "            \"f1 macro\": 0,\n",
    "            \"f1 micro\": 0\n",
    "        } for i in tax_vocab_sizes.keys()\n",
    "    }\n",
    "    \n",
    "    # Process each sequence\n",
    "    for sequence, label in zip(input_sequences, labels_):\n",
    "        inputs = tokenizer_(\n",
    "            [sequence],\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len\n",
    "        ).to(device)\n",
    "    \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "        for k in tax_vocab_sizes.keys():\n",
    "            pred = output[str(k)].argmax(dim=-1).cpu().item()\n",
    "            loss = criterion(output[str(k)], torch.tensor([label[k]]).to(device))\n",
    "            df[k][\"sequence\"].append(sequence)\n",
    "            df[k][\"label\"].append(level_decoder[k][label[k]])\n",
    "            df[k][\"pred\"].append(level_decoder[k][pred])\n",
    "            df[k][\"loss\"].append(round(loss.cpu().item(), 4))\n",
    "\n",
    "    for k in tax_vocab_sizes.keys():\n",
    "        # Convert to DataFrame\n",
    "        new_df = pd.DataFrame(df[k])\n",
    "        new_df['is_incorrect'] = new_df['label'] != new_df['pred']\n",
    "        new_df = new_df.sort_values(['is_incorrect', 'loss'], ascending=[False, False])\n",
    "        new_df.to_csv(f'val_results/{model_name}/classification_results_{k}.csv', index=False)\n",
    "\n",
    "        metrics[k][\"loss\"] = np.array(df[k][\"loss\"]).mean()\n",
    "        metrics[k][\"accuracy\"] = accuracy_score(np.array(df[k][\"label\"]), np.array(df[k][\"pred\"]))\n",
    "        metrics[k][\"f1 macro\"] = f1_score(np.array(df[k][\"label\"]), np.array(df[k][\"pred\"]), average='macro')  # F1-score for multi-label classification\n",
    "        metrics[k][\"f1 micro\"] = f1_score(np.array(df[k][\"label\"]), np.array(df[k][\"pred\"]), average='micro') \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686acc3d-51ee-42a4-908f-cd988f426daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'loss': 0.5130515625000001,\n",
       "  'accuracy': 0.5,\n",
       "  'f1 macro': 0.3333333333333333,\n",
       "  'f1 micro': 0.5},\n",
       " 1: {'loss': 3.52185875, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 2: {'loss': 7.239747812500001,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 3: {'loss': 8.6302459375, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 4: {'loss': 8.2487228125, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 5: {'loss': 8.5836009375, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 6: {'loss': 8.657818125, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 7: {'loss': 9.632887812500002,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 8: {'loss': 11.102313125, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 9: {'loss': 10.909422187500002,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 10: {'loss': 10.911146562499999,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 11: {'loss': 12.0065621875,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 12: {'loss': 10.023724375, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 13: {'loss': 9.399950624999999,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 14: {'loss': 9.743676875, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 15: {'loss': 10.3315053125,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 16: {'loss': 10.54398375, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 17: {'loss': 10.5244028125,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 18: {'loss': 10.397404374999999,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 19: {'loss': 10.7125115625,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 20: {'loss': 11.0797884375,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 21: {'loss': 10.856150625, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 22: {'loss': 10.431981562499999,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 23: {'loss': 10.37791625, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 24: {'loss': 11.57188625, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 25: {'loss': 11.3338703125,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 26: {'loss': 11.320951249999998,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 27: {'loss': 11.4151128125,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 28: {'loss': 11.4102784375,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 29: {'loss': 11.388939375000001,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 30: {'loss': 11.126828125, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 31: {'loss': 10.3457840625,\n",
       "  'accuracy': 0.0,\n",
       "  'f1 macro': 0.0,\n",
       "  'f1 micro': 0.0},\n",
       " 32: {'loss': 9.18782625, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 33: {'loss': 8.8048671875, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 34: {'loss': 7.87959125, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 35: {'loss': 6.5692059375, 'accuracy': 0.0, 'f1 macro': 0.0, 'f1 micro': 0.0},\n",
       " 36: {'loss': 1.3868665625,\n",
       "  'accuracy': 0.003125,\n",
       "  'f1 macro': 0.003115264797507788,\n",
       "  'f1 micro': 0.003125}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5cfdd29-cb15-4367-8804-5e730c99d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_ratio(epoch, decay_epochs=100000):\n",
    "    \"\"\"\n",
    "    Calculate partition ratio that decreases from 8/16 to 1/16 in steps\n",
    "    \"\"\"\n",
    "    # Calculate how many epochs before each step down\n",
    "    epochs_per_step = decay_epochs // 7  # 7 steps from 8/16 down to 1/16\n",
    "    \n",
    "    # Calculate current step based on epoch\n",
    "    step = min(epoch // epochs_per_step, 7)  # Max 7 steps down from 8\n",
    "    \n",
    "    # Map step to fraction\n",
    "    fraction = (8 - step) / 16\n",
    "    \n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b487e-69ce-4c41-be78-a1b6dcb58e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 457/100000 [5:00:38<1090:48:51, 39.45s/it]"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "current_lr = lr\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "\n",
    "    tensor_batch = mix_data_to_tensor_batch(virus_da.get_batch(), cellular_da.get_batch(), partition = get_partition_ratio(epoch+1))\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    labels = tensor_batch.taxes\n",
    "\n",
    "    batch_loss = 0\n",
    "    for index, optimizer in optimizers.items():\n",
    "        optimizer.zero_grad()\n",
    "        output = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'], index=index)\n",
    "        loss = criterion(output, labels[int(index)])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss += loss\n",
    "    \n",
    "    running_loss += batch_loss.item()\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        train_loss = running_loss / val_epoch\n",
    "        val_metrics = evaluate(model)\n",
    "        val_losses = {k: v[\"loss\"] for k, v in val_metrics.items()} \n",
    "        val_loss = sum([entry['loss'] for entry in val_metrics.values()]) \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(val_losses)\n",
    "        \n",
    "        # Create metrics dictionary for saving\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"lr\": current_lr,\n",
    "            \"partition\": get_partition_ratio(epoch+1)\n",
    "        }\n",
    "        metrics.update(val_losses)\n",
    "            \n",
    "        # Save periodic checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizers,\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch + batch_loss.item())\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Reset training metrics\n",
    "        running_loss = 0\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11fb7d-2d37-4e03-be2d-52d1faac30b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4777ee-67b9-405f-b633-5e2320984387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f48a67-61c5-434e-a532-501b19620031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WORLD_SIZE=1 , LOCAL_WORLD_SIZE=1,RANK =0,LOCAL_RANK = 0 \n",
      "Loaded dictionary.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "../checkpoints/Own_T5_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from data_access import PQDataAccess\n",
    "from data_process import *\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "batch_size = 64\n",
    "da = PQDataAccess(\"/home/aac/Alireza/datasets/taxseq/corpus_1000\", batch_size)\n",
    "epochs= 10_000\n",
    "val_epoch = 100\n",
    "num_val = 25\n",
    "\n",
    "model_name = \"T5\"\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81b7d4ef-04f8-45c4-a2de-d793188027e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "T5Decoder.__init__() missing 1 required keyword-only argument: 'max_seq_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(t5_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# model = T5ForConditionalGeneration(config).to(device)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_num_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt5_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m/home/aac/Alireza/local_codes/models/T5Model.py:377\u001b[0m, in \u001b[0;36mT5.__init__\u001b[0;34m(self, dec_num_tokens, dim, max_seq_len, enc_num_tokens, enc_depth, enc_heads, enc_dim_head, enc_mlp_mult, dec_depth, dec_heads, dec_dim_head, dec_mlp_mult, dropout, tie_token_emb)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(max_seq_len, dim)\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m T5Encoder(\n\u001b[1;32m    367\u001b[0m     dim \u001b[38;5;241m=\u001b[39m dim,\n\u001b[1;32m    368\u001b[0m     max_seq_len \u001b[38;5;241m=\u001b[39m max_seq_len, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m     dropout \u001b[38;5;241m=\u001b[39m dropout\n\u001b[1;32m    375\u001b[0m )\n\u001b[0;32m--> 377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m \u001b[43mT5Decoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#max_seq_len= max_seq_len, \u001b[39;49;00m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdec_num_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdec_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheads\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdec_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim_head\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdec_dim_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmlp_mult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdec_mlp_mult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_logits \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(dim, dec_num_tokens)\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# tie weights\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: T5Decoder.__init__() missing 1 required keyword-only argument: 'max_seq_len'"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print(model.config)\n",
    "print(\"model:\", sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f837a4-a5d2-48d4-9ff9-91d70c0ee07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_step(model, optimizer, da, device):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get batch and convert to tensor\n",
    "    tensor_batch = T5_data_to_tensor_batch(da.get_batch())\n",
    "    tensor_batch.gpu(device)\n",
    "    labels = tensor_batch.taxes\n",
    "    \n",
    "    outputs = model(input_ids=tensor_batch.seq_ids, labels=labels)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "        \n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70fbb55-5af3-4aae-a912-81539a97c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    test_protein_sequence = \"MKTAYIAKQRQISFVKSHFSRQDIL\"\n",
    "    input_tensor = torch.LongTensor([encode_sequence(test_protein_sequence)]).to(device)\n",
    "    \n",
    "    # Generate output using the model\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_tensor,\n",
    "            max_length=200,  # Adjust based on expected taxonomy length\n",
    "        )\n",
    "    \n",
    "    # Decode the generated output to a string\n",
    "    print(t5_tokenizer.decode(output_sequences[0], skip_special_tokens=True))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d670768-512e-49e2-acb6-660e1447aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_step(model, optimizer, da, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        mean_train_loss = sum(train_losses[-val_epoch:]) / val_epoch\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {mean_train_loss:.4f}\")\n",
    "        generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1f764-70de-4bdd-afc8-e02167e3538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7cd1b0e-e0e4-4fd8-a680-ccdf0e1918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "../checkpoints/OnlySecond_checkpoints\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:stb4v7gr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">swift-sponge-3</strong> at: <a href='https://wandb.ai/alireza_noroozi/OnlySecond/runs/stb4v7gr' target=\"_blank\">https://wandb.ai/alireza_noroozi/OnlySecond/runs/stb4v7gr</a><br/> View project at: <a href='https://wandb.ai/alireza_noroozi/OnlySecond' target=\"_blank\">https://wandb.ai/alireza_noroozi/OnlySecond</a><br/>Synced 4 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241226_085552-stb4v7gr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:stb4v7gr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241226_085807-o2ao2ghv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alireza_noroozi/OnlySecond/runs/o2ao2ghv' target=\"_blank\">generous-breeze-4</a></strong> to <a href='https://wandb.ai/alireza_noroozi/OnlySecond' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alireza_noroozi/OnlySecond' target=\"_blank\">https://wandb.ai/alireza_noroozi/OnlySecond</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alireza_noroozi/OnlySecond/runs/o2ao2ghv' target=\"_blank\">https://wandb.ai/alireza_noroozi/OnlySecond/runs/o2ao2ghv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alireza_noroozi/OnlySecond/runs/o2ao2ghv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x75cc556080d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import json\n",
    "from transformers import EsmModel, AutoTokenizer, AutoModel\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from batch import Batch\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 100_000\n",
    "val_epoch = 100\n",
    "num_val = 10\n",
    "batch_size = 64\n",
    "virus_dataset_name = \"corpus_1000_Viruses\"\n",
    "cellular_dataset_name = \"corpus_1000_cellular\"\n",
    "lr = 0.001\n",
    "model_name = \"OnlySecond\"\n",
    "max_seq_len = 1000\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "virus_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{virus_dataset_name}\", batch_size)\n",
    "cellular_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{cellular_dataset_name}\", batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": model_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_szie\": batch_size,\n",
    "        \"max_seq_len\": max_seq_len\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2cb1410-5ec8-4c88-854d-be599e54e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM1b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm = torch.load(\"esm_msa1_t12_100M_UR50S.pt\")['model']\n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = nn.Linear(1280, 512)\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer2 = nn.Linear(512, 34)\n",
    "\n",
    "    def attention_pooling(self, x):\n",
    "        # x shape: (batch_size, seq_length, embedding_dim)\n",
    "        attention_weights = self.attention(x)  # (batch_size, seq_length, 1)\n",
    "        attention_weights = torch.softmax(attention_weights.squeeze(-1), dim=1)  # (batch_size, seq_length)\n",
    "        attention_weights = attention_weights.unsqueeze(-1)  # (batch_size, seq_length, 1)\n",
    "        pooled = torch.sum(x * attention_weights, dim=1)  # (batch_size, embedding_dim)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.esm(x, attention_mask=attention_mask).last_hidden_state\n",
    "        outputs = self.attention_pooling(outputs)\n",
    "        \n",
    "        outputs = self.layer1(outputs)\n",
    "        outputs = self.layer_norm(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.layer2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71797635-27f5-4b83-8725-62fe5cd47594",
   "metadata": {},
   "outputs": [],
   "source": [
    "esm = torch.load(\"esm_msa1_t12_100M_UR50S.pt\")['model']\n",
    "esm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37fede89-9b77-4f1e-951b-3f6a2600d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1.002531 M\n",
      "Total parameters: 1.002531 M\n"
     ]
    }
   ],
   "source": [
    "model = ESM1b().to(device)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable parameters: {trainable/ 1e6} M')\n",
    "print(f'Total parameters: {total/ 1e6} M')\n",
    "# print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Initial restart interval\n",
    "    T_mult=2,  # Multiply interval by 2 after each restart\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d31e550-ed8f-41a5-942f-90d86c3afe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index2name_file = \"../data/taxonomy_index.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(index2name_file):\n",
    "    with open(index2name_file, \"rb\") as f:\n",
    "        index2name = json.load(f)\n",
    "\n",
    "tax_vocab_sizes = {\n",
    "    int(k): len(v) for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "# print(tax_vocab_sizes)\n",
    "# # Print tax_vocab_sizes sorted by value (number of taxa per rank)\n",
    "# sorted_sizes = dict(sorted(tax_vocab_sizes.items(), key=lambda x: x[1], reverse=True))\n",
    "# print(\"\\nTaxonomic ranks sorted by number of taxa:\")\n",
    "# for rank, size in sorted_sizes.items():\n",
    "#     print(f\"{rank}: {size}\")\n",
    "\n",
    "level_encoder = {\n",
    "    int(k): {name: idx + 1 for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "level_decoder = {\n",
    "    int(k): {idx + 1: name for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "for k, v in level_decoder.items():\n",
    "    level_decoder[k][0] = \"NOT DEFINED\"\n",
    "\n",
    "\n",
    "def encode_lineage(lineage_str):\n",
    "    taxes_str = lineage_str.split(\", \")\n",
    "\n",
    "    encoded = {int(k): 0 for k in index2name.keys()}\n",
    "    \n",
    "    for i, tax_str in enumerate(taxes_str):\n",
    "        encoded[i] = level_encoder[i][tax_str]\n",
    "\n",
    "    return encoded\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(f\"facebook/esm1b_t33_650M_UR50S\")\n",
    "\n",
    "def mix_data_to_tensor_batch(b_virues, b_cellular, max_seq_len=max_seq_len, partition=0.25):\n",
    "    if partition == -1:\n",
    "        b = b_virues + b_cellular\n",
    "    else:\n",
    "        split_point = int(len(b_virues) * partition)\n",
    "        b = b_virues[:split_point] + b_cellular[-len(b_virues) + split_point:]\n",
    "        random.shuffle(b)  # In-place shuffle\n",
    "    \n",
    "    inputs = tokenizer_(\n",
    "        [e['Sequence'] for e in b],\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len\n",
    "    )\n",
    "\n",
    "    tax_ids = [encode_lineage(e['Taxonomic_lineage__ALL_']) for e in b]\n",
    "    combined_dict = {}\n",
    "    for d in tax_ids:\n",
    "        for key, value in d.items():\n",
    "            combined_dict.setdefault(key, []).append(value)\n",
    "\n",
    "    tensor_encoded = {k: torch.LongTensor(v) for k,v in combined_dict.items()}\n",
    "    return Batch(inputs, tensor_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb03ada3-72a3-4e55-a292-7773574ce08b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 71\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics calculation time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtiming[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, accuracy, f1_micro, f1_macro, conf_matrix\n\u001b[0;32m---> 71\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# print(f\"Data loading time: {data_load_time:.2f} seconds\")\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# start_time = time.time()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# print(f\"F1 Score (Micro): {f1_micro:.4f}\")\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# print(f\"F1 Score (Macro): {f1_macro:.4f}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Time model inference\u001b[39;00m\n\u001b[1;32m     35\u001b[0m model_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     38\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 27\u001b[0m, in \u001b[0;36mESM1b.forward\u001b[0;34m(self, x, attention_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 27\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     28\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_pooling(outputs)\n\u001b[1;32m     30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(outputs)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "val_dir = f\"val_results/{model_name}\"\n",
    "if not os.path.exists(val_dir):\n",
    "    os.makedirs(val_dir)\n",
    "\n",
    "# Time data loading\n",
    "data_load_start = time.time()    \n",
    "val_batches = [virus_da.get_batch() for _ in range(num_val)]\n",
    "cell_val_batches = [cellular_da.get_batch() for _ in range(num_val)]\n",
    "data_load_time = time.time() - data_load_start\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    batch_times = []\n",
    "    model_times = []\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        # Time batch preparation\n",
    "        batch_start = time.time()\n",
    "        with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "            tensor_batch = mix_data_to_tensor_batch(val_batches[epoch], cell_val_batches[epoch], max_seq_len, partition=-1)\n",
    "            tensor_batch.gpu(device)\n",
    "            labels = tensor_batch.taxes[1]\n",
    "        batch_time = time.time() - batch_start\n",
    "        batch_times.append(batch_time)\n",
    "            \n",
    "        # Time model inference\n",
    "        model_start = time.time()\n",
    "        outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "        loss = criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        model_time = time.time() - model_start\n",
    "        model_times.append(model_time)\n",
    "    \n",
    "        running_loss += loss.item()\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Time metrics calculation\n",
    "    metrics_start = time.time()\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    f1_macro = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')\n",
    "    f1_micro = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')\n",
    "    conf_matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "    avg_loss = running_loss / num_val\n",
    "    metrics_time = time.time() - metrics_start\n",
    "    \n",
    "    timing = {\n",
    "        'avg_batch_time': sum(batch_times)/len(batch_times),\n",
    "        'avg_model_time': sum(model_times)/len(model_times),\n",
    "        'metrics_time': metrics_time\n",
    "    }\n",
    "    print(f\"\\nTiming breakdown:\")\n",
    "    print(f\"Average batch preparation time: {timing['avg_batch_time']:.2f} seconds\")\n",
    "    print(f\"Average model inference time: {timing['avg_model_time']:.2f} seconds\") \n",
    "    print(f\"Metrics calculation time: {timing['metrics_time']:.2f} seconds\")\n",
    "    \n",
    "    return avg_loss, accuracy, f1_micro, f1_macro, conf_matrix\n",
    "\n",
    "\n",
    "evaluate(model)\n",
    "# print(f\"Data loading time: {data_load_time:.2f} seconds\")\n",
    "\n",
    "# start_time = time.time()\n",
    "# avg_loss, accuracy, f1_micro, f1_macro, conf_matrix = evaluate(model)\n",
    "# end_time = time.time()\n",
    "# eval_time = end_time - start_time\n",
    "# print(f\"Total evaluation time: {eval_time:.2f} seconds\")\n",
    "\n",
    "# print(f\"\\nResults:\")\n",
    "# print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "# print(f\"Accuracy: {accuracy:.4f}\")\n",
    "# print(f\"F1 Score (Micro): {f1_micro:.4f}\")\n",
    "# print(f\"F1 Score (Macro): {f1_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba451553-07c8-4608-821a-28078d720548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def load_checkpoint(model, optimizer=None, scheduler=None):\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pt'))        \n",
    "    # Extract epoch numbers and find latest\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load optimizer state if provided (for training)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Move optimizer state to GPU if necessary\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get training metadata\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "    \n",
    "    print(f\"Successfully loaded checkpoint from epoch {epoch}\")\n",
    "    # print(\"Metrics at checkpoint:\", metrics)\n",
    "    \n",
    "    return model, optimizer, scheduler, epoch, metrics\n",
    "        \n",
    "\n",
    "# model, optimizer, scheduler, latest_epoch, metrics = load_checkpoint(model, optimizer, scheduler)\n",
    "latest_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b894a867-bf97-4d90-9092-17acfa8cf6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_ratio(epoch, decay_epochs=100000):\n",
    "    \"\"\"\n",
    "    Calculate partition ratio that decreases from 8/16 to 1/16 in steps\n",
    "    \"\"\"\n",
    "    # Calculate how many epochs before each step down\n",
    "    epochs_per_step = decay_epochs // 7  # 7 steps from 8/16 down to 1/16\n",
    "    \n",
    "    # Calculate current step based on epoch\n",
    "    step = min(epoch // epochs_per_step, 7)  # Max 7 steps down from 8\n",
    "    \n",
    "    # Map step to fraction\n",
    "    fraction = (8 - step) / 16\n",
    "    \n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b487e-69ce-4c41-be78-a1b6dcb58e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 99/100000 [07:07<119:51:52,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing breakdown:\n",
      "Average batch preparation time: 0.12 seconds\n",
      "Average model inference time: 0.02 seconds\n",
      "Metrics calculation time: 0.01 seconds\n",
      "Epoch [100/100000]\n",
      "Train Loss: 0.5036\n",
      "Val Loss: 0.4689, Val Accuracy: 0.8547\n",
      "Val F1 (micro): 0.8547, Val F1 (macro): 0.3560\n",
      "Val Confusion Matrix:\n",
      "[[  7   2   0   0   0   0   0   0   0   6   3   0   0]\n",
      " [  0 360   0   1   0   0   0   0   2   0  13   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1  10   0   0]\n",
      " [  0   8   0   5   0   0   0   0   3   0   4   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0  13   0   0   0   0   0   6   6   0  12   0   0]\n",
      " [  0  15   0   0   0   0   0   1 146   2   2   0   0]\n",
      " [  2   1   0   0   0   0   0   0   2 410  41   0   0]\n",
      " [  0  17   0   0   0   0   0   1   0  12 160   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   2   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 199/100000 [15:51<119:40:35,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing breakdown:\n",
      "Average batch preparation time: 0.12 seconds\n",
      "Average model inference time: 0.02 seconds\n",
      "Metrics calculation time: 0.00 seconds\n",
      "Epoch [200/100000]\n",
      "Train Loss: 0.4849\n",
      "Val Loss: 0.4529, Val Accuracy: 0.8555\n",
      "Val F1 (micro): 0.8555, Val F1 (macro): 0.3739\n",
      "Val Confusion Matrix:\n",
      "[[  7   2   0   0   0   0   0   0   0   6   3   0   0]\n",
      " [  0 346   0   1   0   0   0   0  10   5  14   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   2   9   0   0]\n",
      " [  0   3   0  10   0   0   0   0   4   0   3   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   8   0   0   0   0   0   6  10   1  12   0   0]\n",
      " [  0   6   0   0   0   0   0   0 154   4   2   0   0]\n",
      " [  2   0   0   0   0   0   0   0   2 417  35   0   0]\n",
      " [  0   6   0   1   0   0   0   1   2  25 155   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   2   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 299/100000 [24:34<119:27:36,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing breakdown:\n",
      "Average batch preparation time: 0.12 seconds\n",
      "Average model inference time: 0.02 seconds\n",
      "Metrics calculation time: 0.01 seconds\n",
      "Epoch [300/100000]\n",
      "Train Loss: 0.4678\n",
      "Val Loss: 0.4508, Val Accuracy: 0.8641\n",
      "Val F1 (micro): 0.8641, Val F1 (macro): 0.3648\n",
      "Val Confusion Matrix:\n",
      "[[  7   2   0   0   0   0   0   0   0   6   3   0   0]\n",
      " [  0 350   0   1   0   0   0   0   9   3  13   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   1   9   0   0]\n",
      " [  0   3   0  12   0   0   0   0   2   0   3   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   8   0   1   0   0   0   1  13   2  12   0   0]\n",
      " [  0   5   0   0   0   0   0   0 154   4   3   0   0]\n",
      " [  2   0   0   0   0   0   0   0   2 422  30   0   0]\n",
      " [  1   6   0   0   0   0   0   0   2  20 160   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   1   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 399/100000 [33:18<119:30:19,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing breakdown:\n",
      "Average batch preparation time: 0.12 seconds\n",
      "Average model inference time: 0.02 seconds\n",
      "Metrics calculation time: 0.00 seconds\n",
      "Epoch [400/100000]\n",
      "Train Loss: 0.4704\n",
      "Val Loss: 0.4318, Val Accuracy: 0.8664\n",
      "Val F1 (micro): 0.8664, Val F1 (macro): 0.3780\n",
      "Val Confusion Matrix:\n",
      "[[  7   2   0   0   0   0   0   0   0   6   3   0   0]\n",
      " [  0 354   0   1   0   0   0   0   7   3  11   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   1   9   0   0]\n",
      " [  0   5   0  11   0   0   0   0   1   0   3   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0  10   0   0   0   0   0   5   9   1  12   0   0]\n",
      " [  0   9   0   0   0   0   0   0 153   2   2   0   0]\n",
      " [  2   0   0   0   0   0   0   0   2 420  32   0   0]\n",
      " [  1   8   0   0   0   0   0   1   2  18 159   0   1]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   1   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 499/100000 [42:03<119:51:57,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing breakdown:\n",
      "Average batch preparation time: 0.12 seconds\n",
      "Average model inference time: 0.02 seconds\n",
      "Metrics calculation time: 0.00 seconds\n",
      "Epoch [500/100000]\n",
      "Train Loss: 0.4464\n",
      "Val Loss: 0.4337, Val Accuracy: 0.8594\n",
      "Val F1 (micro): 0.8594, Val F1 (macro): 0.3881\n",
      "Val Confusion Matrix:\n",
      "[[  7   2   0   0   0   0   0   0   0   7   2   0   0]\n",
      " [  0 356   0   1   0   0   0   2   6   7   4   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   1   9   0   0]\n",
      " [  0   3   0  14   0   0   0   0   1   2   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   8   0   0   0   0   0   8   9   4   8   0   0]\n",
      " [  0   5   0   0   0   0   0   1 153   6   1   0   0]\n",
      " [  2   1   0   0   0   0   0   0   2 430  21   0   0]\n",
      " [  0  11   0   1   0   0   0   3   2  41 132   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   2   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 599/100000 [50:46<119:06:25,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing breakdown:\n",
      "Average batch preparation time: 0.12 seconds\n",
      "Average model inference time: 0.02 seconds\n",
      "Metrics calculation time: 0.00 seconds\n",
      "Epoch [600/100000]\n",
      "Train Loss: 0.4691\n",
      "Val Loss: 0.4131, Val Accuracy: 0.8695\n",
      "Val F1 (micro): 0.8695, Val F1 (macro): 0.4009\n",
      "Val Confusion Matrix:\n",
      "[[  7   2   0   0   0   0   0   0   0   6   3   0   0]\n",
      " [  0 360   0   2   0   0   0   1   4   1   8   0   0]\n",
      " [  0   0   1   0   0   0   0   1   0   1   8   0   0]\n",
      " [  0   3   0  15   0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   8   0   1   0   0   0   7   7   0  14   0   0]\n",
      " [  0   9   0   0   0   0   0   3 150   2   2   0   0]\n",
      " [  2   1   0   0   0   0   0   1   2 413  37   0   0]\n",
      " [  0  10   0   2   0   0   0   2   2  14 160   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   1   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 699/100000 [59:29<119:04:08,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Timing breakdown:\n",
      "Average batch preparation time: 0.12 seconds\n",
      "Average model inference time: 0.02 seconds\n",
      "Metrics calculation time: 0.00 seconds\n",
      "Epoch [700/100000]\n",
      "Train Loss: 0.4120\n",
      "Val Loss: 0.4100, Val Accuracy: 0.8695\n",
      "Val F1 (micro): 0.8695, Val F1 (macro): 0.4061\n",
      "Val Confusion Matrix:\n",
      "[[  7   2   0   0   0   0   0   0   0   6   3   0   0]\n",
      " [  0 357   0   1   0   0   0   0   4   2  12   0   0]\n",
      " [  0   0   1   0   0   0   0   1   0   1   8   0   0]\n",
      " [  0   3   0  15   0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0]\n",
      " [  0   8   0   0   0   0   0   7   8   1  13   0   0]\n",
      " [  0   7   0   0   0   0   0   1 153   2   3   0   0]\n",
      " [  2   1   0   0   0   0   0   1   2 416  34   0   0]\n",
      " [  0   8   0   1   0   0   0   2   3  19 157   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   1   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 713/100000 [1:02:02<126:30:22,  4.59s/it]"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "current_lr = lr\n",
    "\n",
    "for epoch in tqdm(range(latest_epoch, latest_epoch + epochs)):\n",
    "    model.train()\n",
    "\n",
    "    current_partition = get_partition_ratio(epoch)\n",
    "    \n",
    "    tensor_batch = mix_data_to_tensor_batch(\n",
    "        virus_da.get_batch(),\n",
    "        cellular_da.get_batch(),\n",
    "        max_seq_len,\n",
    "        partition=current_partition\n",
    "    )\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    labels = tensor_batch.taxes[1]\n",
    "    outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        train_loss = running_loss / val_epoch\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_accuracy, val_f1_micro, val_f1_macro, val_cm = evaluate(model)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Val F1 (micro): {val_f1_micro:.4f}, Val F1 (macro): {val_f1_macro:.4f}\")\n",
    "        print(\"Val Confusion Matrix:\")\n",
    "        print(val_cm)\n",
    "        \n",
    "        # Create metrics dictionary for saving\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\":val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_f1_micro\": val_f1_micro,\n",
    "            \"val_f1_macro\": val_f1_macro,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"current_portion\": current_partition,\n",
    "            \"lr\": current_lr\n",
    "        }\n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch + loss.item())\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Reset training metrics\n",
    "        running_loss = 0\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38a288-ea78-424e-9f90-995c9b435655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _, latest_epoch, metrics = load_checkpoint(model)\n",
    "\n",
    "val_batches_ = [virus_da.get_batch() for _ in range(num_val // 2)] + [cellular_da.get_batch() for _ in range(num_val // 2)]\n",
    "\n",
    "# input_sequences_ = [e['Sequence'] for b in val_batches_ for e in b]\n",
    "# labels_ = [encode_lineage(e['Taxonomic_lineage__ALL_'])  for b in val_batches_ for e in b]\n",
    "\n",
    "input_sequences_ = [\"ACACAD\"]\n",
    "labels_ = [{0: 1}]\n",
    "\n",
    "def evaluate_df(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    df = {\n",
    "        \"sequence\": [],\n",
    "        \"label\": [],\n",
    "        \"pred\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": 0,\n",
    "        \"accuracy\": 0,\n",
    "        \"f1 macro\": 0,\n",
    "        \"f1 micro\": 0\n",
    "    }\n",
    "    \n",
    "    # Process each sequence\n",
    "    for sequence, label in zip(input_sequences_, labels_):\n",
    "        inputs = tokenizer_(\n",
    "            [sequence],\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len\n",
    "        ).to(device)\n",
    "    \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "        pred = output.argmax(dim=-1).cpu().item()\n",
    "        loss = criterion(output, torch.tensor([label[0]]).to(device))\n",
    "        df[\"sequence\"].append(sequence)\n",
    "        df[\"label\"].append(level_decoder[0][label[0]])\n",
    "        df[\"pred\"].append(level_decoder[0][pred])\n",
    "        df[\"loss\"].append(round(loss.cpu().item(), 4))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    new_df = pd.DataFrame(df)\n",
    "    new_df['is_incorrect'] = new_df['label'] != new_df['pred']\n",
    "    new_df = new_df.sort_values(['is_incorrect', 'loss'], ascending=[False, False])\n",
    "    new_df.to_csv(f'classification_results__new_att.csv', index=False)\n",
    "\n",
    "    metrics[\"loss\"] = np.array(df[\"loss\"]).mean()\n",
    "    metrics[\"accuracy\"] = accuracy_score(np.array(df[\"label\"]), np.array(df[\"pred\"]))\n",
    "    metrics[\"f1 macro\"] = f1_score(np.array(df[\"label\"]), np.array(df[\"pred\"]), average='macro')  # F1-score for multi-label classification\n",
    "    metrics[\"f1 micro\"] = f1_score(np.array(df[\"label\"]), np.array(df[\"pred\"]), average='micro') \n",
    "    print(metrics)\n",
    "\n",
    "evaluate_df(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb06be5-7f3f-4771-802d-086267ec2df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

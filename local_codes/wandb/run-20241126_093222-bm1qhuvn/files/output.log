{0: 4, 1: 35, 2: 1368, 3: 6024, 4: 4330, 5: 5265, 6: 5453, 7: 15592, 8: 65895, 9: 59786, 10: 54221, 11: 141660, 12: 20431, 13: 13635, 14: 17392, 15: 32172, 16: 35581, 17: 34356, 18: 33115, 19: 43504, 20: 61510, 21: 57903, 22: 31214, 23: 32835, 24: 107520, 25: 82492, 26: 96471, 27: 90666, 28: 85207, 29: 70508, 30: 71902, 31: 26728, 32: 11716, 33: 6444, 34: 2510, 35: 872, 36: 4}

Taxonomic ranks sorted by number of taxa:
11: 141660
24: 107520
26: 96471
27: 90666
28: 85207
25: 82492
30: 71902
29: 70508
8: 65895
20: 61510
9: 59786
21: 57903
10: 54221
19: 43504
16: 35581
17: 34356
18: 33115
23: 32835
15: 32172
22: 31214
31: 26728
12: 20431
14: 17392
7: 15592
13: 13635
32: 11716
33: 6444
3: 6024
6: 5453
5: 5265
4: 4330
34: 2510
2: 1368
35: 872
1: 35
0: 4
36: 4
Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Trainable parameters: 755.969937 M
Total parameters: 1408.323878 M
ESM2(
  (esm): EsmModel(
    (embeddings): EsmEmbeddings(
      (word_embeddings): Embedding(33, 1280, padding_idx=1)
      (dropout): Dropout(p=0.0, inplace=False)
      (position_embeddings): Embedding(1026, 1280, padding_idx=1)
    )
    (encoder): EsmEncoder(
      (layer): ModuleList(
        (0-32): 33 x EsmLayer(
          (attention): EsmAttention(
            (self): EsmSelfAttention(
              (query): Linear(in_features=1280, out_features=1280, bias=True)
              (key): Linear(in_features=1280, out_features=1280, bias=True)
              (value): Linear(in_features=1280, out_features=1280, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (rotary_embeddings): RotaryEmbedding()
            )
            (output): EsmSelfOutput(
              (dense): Linear(in_features=1280, out_features=1280, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          )
          (intermediate): EsmIntermediate(
            (dense): Linear(in_features=1280, out_features=5120, bias=True)
          )
          (output): EsmOutput(
            (dense): Linear(in_features=5120, out_features=1280, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    )
    (pooler): EsmPooler(
      (dense): Linear(in_features=1280, out_features=1280, bias=True)
      (activation): Tanh()
    )
    (contact_head): EsmContactPredictionHead(
      (regression): Linear(in_features=660, out_features=1, bias=True)
      (activation): Sigmoid()
    )
  )
  (heads): ModuleDict(
    (0): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=4, bias=True)
    )
    (1): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=35, bias=True)
    )
    (2): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=1368, bias=True)
    )
    (3): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=6024, bias=True)
    )
    (4): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=4330, bias=True)
    )
    (5): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=5265, bias=True)
    )
    (6): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=5453, bias=True)
    )
    (7): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=15592, bias=True)
    )
    (8): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=65895, bias=True)
    )
    (9): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=59786, bias=True)
    )
    (10): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=54221, bias=True)
    )
    (11): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=141660, bias=True)
    )
    (12): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=20431, bias=True)
    )
    (13): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=13635, bias=True)
    )
    (14): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=17392, bias=True)
    )
    (15): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=32172, bias=True)
    )
    (16): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=35581, bias=True)
    )
    (17): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=34356, bias=True)
    )
    (18): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=33115, bias=True)
    )
    (19): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=43504, bias=True)
    )
    (20): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=61510, bias=True)
    )
    (21): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=57903, bias=True)
    )
    (22): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=31214, bias=True)
    )
    (23): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=32835, bias=True)
    )
    (24): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=107520, bias=True)
    )
    (25): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=82492, bias=True)
    )
    (26): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=96471, bias=True)
    )
    (27): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=90666, bias=True)
    )
    (28): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=85207, bias=True)
    )
    (29): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=70508, bias=True)
    )
    (30): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=71902, bias=True)
    )
    (31): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=26728, bias=True)
    )
    (32): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=11716, bias=True)
    )
    (33): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=6444, bias=True)
    )
    (34): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=2510, bias=True)
    )
    (35): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=872, bias=True)
    )
    (36): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=4, bias=True)
    )
  )
)
  0%|          | 9/100000 [00:55<170:03:58,  6.12s/it]
Epoch [10/100000]
Train Loss: 223.3565
Val Loss: 83.5547
  0%|          | 109/100000 [08:27<43:16:27,  1.56s/it] 
Epoch [10/100000]
Train Loss: 78.0940
Val Loss: 92.0773
Epoch [20/100000]
Train Loss: 80.7164
Val Loss: 83.9021
Epoch [30/100000]
Train Loss: 76.8288
Val Loss: 76.6419
Epoch [40/100000]
Train Loss: 73.2459
Val Loss: 71.6006
Epoch [50/100000]
Train Loss: 64.5393
Val Loss: 68.7409
Epoch [60/100000]
Train Loss: 60.9445
Val Loss: 66.6537
Epoch [70/100000]
Train Loss: 58.2144
Val Loss: 65.5568
Epoch [80/100000]
Train Loss: 62.3541
Val Loss: 65.0225
Epoch [90/100000]
Train Loss: 61.6253
Val Loss: 64.9260
Epoch [100/100000]
Train Loss: 62.0464
Val Loss: 62.5626
Epoch [110/100000]
Train Loss: 58.7089
Val Loss: 61.3197
Epoch [120/100000]
Train Loss: 58.8777
Val Loss: 60.8121
Epoch [130/100000]
Train Loss: 55.4497
Val Loss: 59.3688
Epoch [140/100000]
Train Loss: 57.2654
Val Loss: 58.9901
Epoch [150/100000]
Train Loss: 53.4854
Val Loss: 59.2750
Epoch [160/100000]
Train Loss: 57.8913
Val Loss: 58.1375
Epoch [170/100000]
Train Loss: 57.5465
Val Loss: 57.8609

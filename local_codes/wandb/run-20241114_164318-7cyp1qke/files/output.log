Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
model: 653.014425 M parameters
ESM1b(
  (esm): EsmModel(
    (embeddings): EsmEmbeddings(
      (word_embeddings): Embedding(33, 1280, padding_idx=1)
      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (position_embeddings): Embedding(1026, 1280, padding_idx=1)
    )
    (encoder): EsmEncoder(
      (layer): ModuleList(
        (0-32): 33 x EsmLayer(
          (attention): EsmAttention(
            (self): EsmSelfAttention(
              (query): Linear(in_features=1280, out_features=1280, bias=True)
              (key): Linear(in_features=1280, out_features=1280, bias=True)
              (value): Linear(in_features=1280, out_features=1280, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): EsmSelfOutput(
              (dense): Linear(in_features=1280, out_features=1280, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          )
          (intermediate): EsmIntermediate(
            (dense): Linear(in_features=1280, out_features=5120, bias=True)
          )
          (output): EsmOutput(
            (dense): Linear(in_features=5120, out_features=1280, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    )
    (pooler): EsmPooler(
      (dense): Linear(in_features=1280, out_features=1280, bias=True)
      (activation): Tanh()
    )
    (contact_head): EsmContactPredictionHead(
      (regression): Linear(in_features=660, out_features=1, bias=True)
      (activation): Sigmoid()
    )
  )
  (layer1): Linear(in_features=1280, out_features=512, bias=True)
  (relu): ReLU()
  (layer2): Linear(in_features=512, out_features=4, bias=True)
)
  0%|          | 14/10000 [01:49<14:46:11,  5.32s/it]
Epoch [5/10000]
Train Loss: 1.2641, Train Accuracy: 0.3000
Train F1 (micro): 0.3000, Train F1 (macro): 0.2434
Train Confusion Matrix:
[[ 0  0  0]
 [16 16  8]
 [16 16  8]]
Val Loss: 1.1087, Val Accuracy: 0.0187
Val F1 (micro): 0.0187, Val F1 (macro): 0.0123
Val Confusion Matrix:
[[ 14   0   0]
 [779   1   0]
 [  6   0   0]]
Epoch [10/10000]
Train Loss: 0.9671, Train Accuracy: 0.5000
Train F1 (micro): 0.5000, Train F1 (macro): 0.4949
Train Confusion Matrix:
[[24 16]
 [24 16]]
Val Loss: 0.5387, Val Accuracy: 0.9750
Val F1 (micro): 0.9750, Val F1 (macro): 0.3291
Val Confusion Matrix:
[[  0  14   0]
 [  0 780   0]
 [  0   6   0]]
Epoch [15/10000]
Train Loss: 0.7210, Train Accuracy: 0.5000
Train F1 (micro): 0.5000, Train F1 (macro): 0.4505
Train Confusion Matrix:
[[32  8]
 [32  8]]
Val Loss: 0.6646, Val Accuracy: 0.9750
Val F1 (micro): 0.9750, Val F1 (macro): 0.3291
Val Confusion Matrix:
[[  0  14   0]
 [  0 780   0]
 [  0   6   0]]

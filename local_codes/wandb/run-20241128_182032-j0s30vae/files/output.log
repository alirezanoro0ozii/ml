Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Trainable parameters: 766.336998 M
Total parameters: 1418.693499 M
ESM1b(
  (esm): EsmModel(
    (embeddings): EsmEmbeddings(
      (word_embeddings): Embedding(33, 1280, padding_idx=1)
      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.0, inplace=False)
      (position_embeddings): Embedding(1026, 1280, padding_idx=1)
    )
    (encoder): EsmEncoder(
      (layer): ModuleList(
        (0-32): 33 x EsmLayer(
          (attention): EsmAttention(
            (self): EsmSelfAttention(
              (query): Linear(in_features=1280, out_features=1280, bias=True)
              (key): Linear(in_features=1280, out_features=1280, bias=True)
              (value): Linear(in_features=1280, out_features=1280, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (output): EsmSelfOutput(
              (dense): Linear(in_features=1280, out_features=1280, bias=True)
              (dropout): Dropout(p=0.0, inplace=False)
            )
            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          )
          (intermediate): EsmIntermediate(
            (dense): Linear(in_features=1280, out_features=5120, bias=True)
          )
          (output): EsmOutput(
            (dense): Linear(in_features=5120, out_features=1280, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    )
    (pooler): EsmPooler(
      (dense): Linear(in_features=1280, out_features=1280, bias=True)
      (activation): Tanh()
    )
    (contact_head): EsmContactPredictionHead(
      (regression): Linear(in_features=660, out_features=1, bias=True)
      (activation): Sigmoid()
    )
  )
  (heads): ModuleDict(
    (0): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=2, bias=True)
    )
    (1): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=33, bias=True)
    )
    (2): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=1298, bias=True)
    )
    (3): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=5633, bias=True)
    )
    (4): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=3733, bias=True)
    )
    (5): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=5181, bias=True)
    )
    (6): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=5444, bias=True)
    )
    (7): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=15383, bias=True)
    )
    (8): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=65228, bias=True)
    )
    (9): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=59006, bias=True)
    )
    (10): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=53852, bias=True)
    )
    (11): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=141571, bias=True)
    )
    (12): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=20369, bias=True)
    )
    (13): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=13615, bias=True)
    )
    (14): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=17388, bias=True)
    )
    (15): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=32162, bias=True)
    )
    (16): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=35564, bias=True)
    )
    (17): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=34333, bias=True)
    )
    (18): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=33100, bias=True)
    )
    (19): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=43499, bias=True)
    )
    (20): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=61507, bias=True)
    )
    (21): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=57902, bias=True)
    )
    (22): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=31211, bias=True)
    )
    (23): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=32827, bias=True)
    )
    (24): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=107510, bias=True)
    )
    (25): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=82470, bias=True)
    )
    (26): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=96454, bias=True)
    )
    (27): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=90631, bias=True)
    )
    (28): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=85202, bias=True)
    )
    (29): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=70506, bias=True)
    )
    (30): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=71899, bias=True)
    )
    (31): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=26726, bias=True)
    )
    (32): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=11716, bias=True)
    )
    (33): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=6444, bias=True)
    )
    (34): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=2510, bias=True)
    )
    (35): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=872, bias=True)
    )
    (36): Sequential(
      (0): Linear(in_features=1280, out_features=512, bias=True)
      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (2): ReLU()
      (3): Dropout(p=0.1, inplace=False)
      (4): Linear(in_features=512, out_features=4, bias=True)
    )
  )
  (attentions): ModuleDict(
    (0): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (1): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (2): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (3): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (4): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (5): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (6): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (7): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (8): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (9): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (10): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (11): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (12): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (13): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (14): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (15): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (16): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (17): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (18): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (19): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (20): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (21): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (22): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (23): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (24): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (25): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (26): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (27): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (28): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (29): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (30): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (31): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (32): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (33): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (34): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (35): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
    (36): Sequential(
      (0): Linear(in_features=1280, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=1, bias=True)
    )
  )
)
  1%|▏         | 1399/100000 [37:24<36:05:52,  1.32s/it] 
Epoch [200/100000]
Train Loss: 57.2345
Val Loss: 45.5851
{0: 2.8985, 1: 0.4674, 2: 0.9905, 3: 1.5665, 4: 2.0715, 5: 2.6607, 6: 3.2483, 7: 4.4959, 8: 5.6275, 9: 4.3652, 10: 2.5532, 11: 2.0157, 12: 0.8898, 13: 0.852, 14: 0.8094, 15: 0.808, 16: 0.6794, 17: 0.7341, 18: 0.7761, 19: 0.8093, 20: 0.7883, 21: 0.8302, 22: 0.7332, 23: 0.5961, 24: 0.5347, 25: 0.5451, 26: 0.449, 27: 0.4818, 28: 0.3968, 29: 0.3775, 30: 0.2898, 31: 0.1423, 32: 0.0963, 33: 0.0039, 34: 0.0009, 35: 0.0001, 36: 0.0001}
Epoch [400/100000]
Train Loss: 41.3255
Val Loss: 43.3288
{0: 3.8725, 1: 0.4358, 2: 0.8479, 3: 1.3272, 4: 1.8291, 5: 2.3748, 6: 2.9434, 7: 4.1658, 8: 5.2943, 9: 4.2358, 10: 2.4412, 11: 1.9265, 12: 0.8063, 13: 0.7607, 14: 0.7486, 15: 0.7592, 16: 0.6334, 17: 0.6453, 18: 0.6986, 19: 0.7177, 20: 0.6982, 21: 0.7574, 22: 0.6856, 23: 0.5541, 24: 0.5082, 25: 0.5095, 26: 0.4214, 27: 0.4566, 28: 0.3809, 29: 0.367, 30: 0.2894, 31: 0.1416, 32: 0.092, 33: 0.0021, 34: 0.0006, 35: 0.0001, 36: 0.0}
Epoch [600/100000]
Train Loss: 39.2807
Val Loss: 41.8149
{0: 4.1397, 1: 0.4336, 2: 0.8546, 3: 1.3674, 4: 1.8533, 5: 2.305, 6: 2.8843, 7: 4.029, 8: 5.0816, 9: 4.1286, 10: 2.3739, 11: 1.8479, 12: 0.7536, 13: 0.706, 14: 0.698, 15: 0.7226, 16: 0.5622, 17: 0.5717, 18: 0.6549, 19: 0.6509, 20: 0.6329, 21: 0.6576, 22: 0.5895, 23: 0.4668, 24: 0.4096, 25: 0.4401, 26: 0.3679, 27: 0.406, 28: 0.363, 29: 0.3566, 30: 0.2785, 31: 0.1336, 32: 0.0901, 33: 0.0024, 34: 0.0012, 35: 0.0003, 36: 0.0}
Epoch [800/100000]
Train Loss: 38.6702
Val Loss: 42.4349
{0: 5.4021, 1: 0.4523, 2: 0.8887, 3: 1.3269, 4: 1.7663, 5: 2.2407, 6: 2.7953, 7: 3.9614, 8: 5.0403, 9: 4.1548, 10: 2.333, 11: 1.7782, 12: 0.7186, 13: 0.6844, 14: 0.6652, 15: 0.7184, 16: 0.5487, 17: 0.5524, 18: 0.6123, 19: 0.5984, 20: 0.6291, 21: 0.6727, 22: 0.5986, 23: 0.475, 24: 0.4093, 25: 0.442, 26: 0.3862, 27: 0.4172, 28: 0.3414, 29: 0.3432, 30: 0.2669, 31: 0.1187, 32: 0.0887, 33: 0.0057, 34: 0.0013, 35: 0.0005, 36: 0.0}
Epoch [1000/100000]
Train Loss: 36.6787
Val Loss: 38.2178
{0: 3.1063, 1: 0.4172, 2: 0.8363, 3: 1.2727, 4: 1.7062, 5: 2.1648, 6: 2.7158, 7: 3.8015, 8: 4.8759, 9: 4.0554, 10: 2.2155, 11: 1.7142, 12: 0.6322, 13: 0.6207, 14: 0.6159, 15: 0.6266, 16: 0.4941, 17: 0.4998, 18: 0.6219, 19: 0.5569, 20: 0.5622, 21: 0.6145, 22: 0.5404, 23: 0.415, 24: 0.3667, 25: 0.4036, 26: 0.3058, 27: 0.3707, 28: 0.3083, 29: 0.3137, 30: 0.2573, 31: 0.1174, 32: 0.0918, 33: 0.0003, 34: 0.0002, 35: 0.0, 36: 0.0}
Epoch [1200/100000]
Train Loss: 35.3446
Val Loss: 37.6746
{0: 3.644, 1: 0.3878, 2: 0.7922, 3: 1.1989, 4: 1.5885, 5: 2.0473, 6: 2.6033, 7: 3.7307, 8: 4.8197, 9: 4.0141, 10: 2.1574, 11: 1.6996, 12: 0.6093, 13: 0.5959, 14: 0.5897, 15: 0.614, 16: 0.4683, 17: 0.4869, 18: 0.5739, 19: 0.5375, 20: 0.5428, 21: 0.5736, 22: 0.5174, 23: 0.3953, 24: 0.348, 25: 0.3807, 26: 0.3025, 27: 0.3569, 28: 0.3011, 29: 0.3188, 30: 0.2686, 31: 0.1168, 32: 0.0899, 33: 0.0026, 34: 0.0006, 35: 0.0, 36: 0.0}
Epoch [1400/100000]
Train Loss: 34.8070
Val Loss: 37.7740
{0: 3.8831, 1: 0.3855, 2: 0.7825, 3: 1.1876, 4: 1.5816, 5: 2.0398, 6: 2.5933, 7: 3.7188, 8: 4.8092, 9: 4.0007, 10: 2.148, 11: 1.6917, 12: 0.6054, 13: 0.591, 14: 0.5841, 15: 0.6097, 16: 0.4656, 17: 0.4853, 18: 0.5745, 19: 0.5371, 20: 0.5431, 21: 0.5727, 22: 0.5174, 23: 0.3942, 24: 0.3468, 25: 0.3789, 26: 0.3011, 27: 0.3554, 28: 0.2999, 29: 0.316, 30: 0.2633, 31: 0.1169, 32: 0.0898, 33: 0.0033, 34: 0.0007, 35: 0.0, 36: 0.0}

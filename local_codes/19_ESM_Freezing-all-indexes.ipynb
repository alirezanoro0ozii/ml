{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cd1b0e-e0e4-4fd8-a680-ccdf0e1918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      " WORLD_SIZE=1 , LOCAL_WORLD_SIZE=1,RANK =0,LOCAL_RANK = 0 \n",
      "../checkpoints/Freeze ESM All head_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malirezanor\u001b[0m (\u001b[33malireza_noroozi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241128_182032-j0s30vae</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head/runs/j0s30vae' target=\"_blank\">sunny-planet-9</a></strong> to <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head' target=\"_blank\">https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head/runs/j0s30vae' target=\"_blank\">https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head/runs/j0s30vae</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alireza_noroozi/Freeze%20ESM%20All%20head/runs/j0s30vae?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x797fe73159f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "import json\n",
    "from transformers import EsmModel, AutoTokenizer\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from batch import Batch\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 100_000\n",
    "val_epoch = 200\n",
    "num_val = 20\n",
    "batch_size = 16\n",
    "virus_dataset_name = \"corpus_1000_Viruses\"\n",
    "cellular_dataset_name = \"corpus_1000_cellular\"\n",
    "lr = 0.001\n",
    "model_name = \"Freeze ESM All head\"\n",
    "max_seq_len = 1000\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "virus_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{virus_dataset_name}\", batch_size)\n",
    "cellular_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{cellular_dataset_name}\", batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": model_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_szie\": batch_size,\n",
    "        \"max_seq_len\": max_seq_len\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9b8f22e-a680-496e-973a-a7b7c552913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index2name_file = \"../data/taxonomy_index.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(index2name_file):\n",
    "    with open(index2name_file, \"rb\") as f:\n",
    "        index2name = json.load(f)\n",
    "\n",
    "tax_vocab_sizes = {\n",
    "    int(k): len(v) for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "# print(tax_vocab_sizes)\n",
    "# # Print tax_vocab_sizes sorted by value (number of taxa per rank)\n",
    "# sorted_sizes = dict(sorted(tax_vocab_sizes.items(), key=lambda x: x[1], reverse=True))\n",
    "# print(\"\\nTaxonomic ranks sorted by number of taxa:\")\n",
    "# for rank, size in sorted_sizes.items():\n",
    "#     print(f\"{rank}: {size}\")\n",
    "\n",
    "level_encoder = {\n",
    "    int(k): {name: idx + 1 for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "level_decoder = {\n",
    "    int(k): {idx + 1: name for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "for k, v in level_decoder.items():\n",
    "    level_decoder[k][0] = \"NOT DEFINED\"\n",
    "\n",
    "\n",
    "def encode_lineage(lineage_str):\n",
    "    taxes_str = lineage_str.split(\", \")\n",
    "\n",
    "    encoded = {int(k): 0 for k in index2name.keys()}\n",
    "    \n",
    "    for i, tax_str in enumerate(taxes_str):\n",
    "        encoded[i] = level_encoder[i][tax_str]\n",
    "\n",
    "    return encoded\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(f\"facebook/esm1b_t33_650M_UR50S\")\n",
    "\n",
    "def mix_data_to_tensor_batch(b_virues, b_cellular, max_seq_len=max_seq_len, partition=0.25):\n",
    "    split_point = int(len(b_virues) * partition)\n",
    "    b = b_virues[:split_point] + b_cellular[-len(b_virues) + split_point:]\n",
    "    random.shuffle(b)  # In-place shuffle\n",
    "    \n",
    "    inputs = tokenizer_(\n",
    "        [e['Sequence'] for e in b],\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len\n",
    "    )\n",
    "\n",
    "    tax_ids = [encode_lineage(e['Taxonomic_lineage__ALL_']) for e in b]\n",
    "    combined_dict = {}\n",
    "    for d in tax_ids:\n",
    "        for key, value in d.items():\n",
    "            combined_dict.setdefault(key, []).append(value)\n",
    "\n",
    "    tensor_encoded = {k: torch.LongTensor(v) for k,v in combined_dict.items()}\n",
    "    return Batch(inputs, tensor_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cb1410-5ec8-4c88-854d-be599e54e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM1b(nn.Module):\n",
    "    def __init__(self, num_classes_dict):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")\n",
    "        \n",
    "        # Freeze ESM parameters\n",
    "        for param in self.esm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.heads = nn.ModuleDict()\n",
    "        self.attentions = nn.ModuleDict()\n",
    "        \n",
    "        for index_name, num_classes in num_classes_dict.items():\n",
    "            self.heads[str(index_name)] = nn.Sequential(\n",
    "                nn.Linear(1280, 512),\n",
    "                nn.LayerNorm(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "\n",
    "            self.attentions[str(index_name)] = nn.Sequential(\n",
    "                nn.Linear(1280, 256),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(256, 1)\n",
    "            )\n",
    "        \n",
    "\n",
    "    def attention_pooling(self, x, index):\n",
    "        # x shape: (batch_size, seq_length, embedding_dim)\n",
    "        attention_weights = self.attentions[index](x)  # (batch_size, seq_length, 1)\n",
    "        attention_weights = torch.softmax(attention_weights.squeeze(-1), dim=1)  # (batch_size, seq_length)\n",
    "        attention_weights = attention_weights.unsqueeze(-1)  # (batch_size, seq_length, 1)\n",
    "        pooled = torch.sum(x * attention_weights, dim=1)  # (batch_size, embedding_dim)\n",
    "        return pooled\n",
    "\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, index=None):\n",
    "        embeddings = self.esm(x, attention_mask=attention_mask)\n",
    "        outputs = embeddings.last_hidden_state\n",
    "        \n",
    "        return {index: head(self.attention_pooling(outputs, index)) for index, head in self.heads.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fede89-9b77-4f1e-951b-3f6a2600d5cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 766.336998 M\n",
      "Total parameters: 1418.693499 M\n",
      "ESM1b(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-32): 33 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (heads): ModuleDict(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=2, bias=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=33, bias=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=1298, bias=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=5633, bias=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=3733, bias=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=5181, bias=True)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=5444, bias=True)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=15383, bias=True)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=65228, bias=True)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=59006, bias=True)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=53852, bias=True)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=141571, bias=True)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=20369, bias=True)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=13615, bias=True)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=17388, bias=True)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=32162, bias=True)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=35564, bias=True)\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=34333, bias=True)\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=33100, bias=True)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=43499, bias=True)\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=61507, bias=True)\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=57902, bias=True)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=31211, bias=True)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=32827, bias=True)\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=107510, bias=True)\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=82470, bias=True)\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=96454, bias=True)\n",
      "    )\n",
      "    (27): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=90631, bias=True)\n",
      "    )\n",
      "    (28): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=85202, bias=True)\n",
      "    )\n",
      "    (29): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=70506, bias=True)\n",
      "    )\n",
      "    (30): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=71899, bias=True)\n",
      "    )\n",
      "    (31): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=26726, bias=True)\n",
      "    )\n",
      "    (32): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=11716, bias=True)\n",
      "    )\n",
      "    (33): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=6444, bias=True)\n",
      "    )\n",
      "    (34): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=2510, bias=True)\n",
      "    )\n",
      "    (35): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=872, bias=True)\n",
      "    )\n",
      "    (36): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=512, bias=True)\n",
      "      (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.1, inplace=False)\n",
      "      (4): Linear(in_features=512, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (attentions): ModuleDict(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (8): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (9): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (10): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (11): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (12): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (13): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (14): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (15): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (16): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (17): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (18): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (19): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (20): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (21): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (22): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (23): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (24): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (25): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (26): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (27): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (28): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (29): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (30): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (31): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (32): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (33): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (34): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (35): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "    (36): Sequential(\n",
      "      (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ESM1b(tax_vocab_sizes).to(device)\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable parameters: {trainable/ 1e6} M')\n",
    "print(f'Total parameters: {total/ 1e6} M')\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Initial restart interval\n",
    "    T_mult=2,  # Multiply interval by 2 after each restart\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d5aae5-876d-48ad-be0c-31c7acba9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "val_dir = f\"val_results/{model_name}\"\n",
    "if not os.path.exists(val_dir):\n",
    "    os.makedirs(val_dir)\n",
    "    \n",
    "val_batches = [virus_da.get_batch() for _ in range(num_val // 2)] + [cellular_da.get_batch() for _ in range(num_val // 2)]\n",
    "\n",
    "input_sequences = [e['Sequence'] for b in val_batches for e in b]\n",
    "labels_ = [encode_lineage(e['Taxonomic_lineage__ALL_'])  for b in val_batches for e in b]\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    df = {\n",
    "        i : {\n",
    "            \"sequence\": [],\n",
    "            \"label\": [],\n",
    "            \"pred\": [],\n",
    "            \"loss\": []\n",
    "        } for i in tax_vocab_sizes.keys()\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        i : {\n",
    "            \"loss\": 0,\n",
    "            \"accuracy\": 0,\n",
    "            \"f1 macro\": 0,\n",
    "            \"f1 micro\": 0\n",
    "        } for i in tax_vocab_sizes.keys()\n",
    "    }\n",
    "    \n",
    "    # Process each sequence\n",
    "    for sequence, label in zip(input_sequences, labels_):\n",
    "        inputs = tokenizer_(\n",
    "            [sequence],\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len\n",
    "        ).to(device)\n",
    "    \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "        for k in tax_vocab_sizes.keys():\n",
    "            pred = output[str(k)].argmax(dim=-1).cpu().item()\n",
    "            loss = criterion(output[str(k)], torch.tensor([label[k]]).to(device))\n",
    "            df[k][\"sequence\"].append(sequence)\n",
    "            df[k][\"label\"].append(level_decoder[k][label[k]])\n",
    "            df[k][\"pred\"].append(level_decoder[k][pred])\n",
    "            df[k][\"loss\"].append(round(loss.cpu().item(), 4))\n",
    "\n",
    "    for k in tax_vocab_sizes.keys():\n",
    "        # Convert to DataFrame\n",
    "        new_df = pd.DataFrame(df[k])\n",
    "        new_df['is_incorrect'] = new_df['label'] != new_df['pred']\n",
    "        new_df = new_df.sort_values(['is_incorrect', 'loss'], ascending=[False, False])\n",
    "        new_df.to_csv(f'val_results/{model_name}/classification_results_{k}.csv', index=False)\n",
    "\n",
    "        metrics[k][\"loss\"] = round(np.array(df[k][\"loss\"]).mean(), 4)\n",
    "        metrics[k][\"accuracy\"] = accuracy_score(np.array(df[k][\"label\"]), np.array(df[k][\"pred\"]))\n",
    "        metrics[k][\"f1 macro\"] = f1_score(np.array(df[k][\"label\"]), np.array(df[k][\"pred\"]), average='macro')  # F1-score for multi-label classification\n",
    "        metrics[k][\"f1 micro\"] = f1_score(np.array(df[k][\"label\"]), np.array(df[k][\"pred\"]), average='micro') \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686acc3d-51ee-42a4-908f-cd988f426daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5cfdd29-cb15-4367-8804-5e730c99d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_ratio(epoch, decay_epochs=100000):\n",
    "    \"\"\"\n",
    "    Calculate partition ratio that decreases from 8/16 to 1/16 in steps\n",
    "    \"\"\"\n",
    "    # Calculate how many epochs before each step down\n",
    "    epochs_per_step = decay_epochs // 7  # 7 steps from 8/16 down to 1/16\n",
    "    \n",
    "    # Calculate current step based on epoch\n",
    "    step = min(epoch // epochs_per_step, 7)  # Max 7 steps down from 8\n",
    "    \n",
    "    # Map step to fraction\n",
    "    fraction = (8 - step) / 16\n",
    "    \n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b487e-69ce-4c41-be78-a1b6dcb58e97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 199/100000 [04:20<36:15:46,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/100000]\n",
      "Train Loss: 57.2345\n",
      "Val Loss: 45.5851\n",
      "{0: 2.8985, 1: 0.4674, 2: 0.9905, 3: 1.5665, 4: 2.0715, 5: 2.6607, 6: 3.2483, 7: 4.4959, 8: 5.6275, 9: 4.3652, 10: 2.5532, 11: 2.0157, 12: 0.8898, 13: 0.852, 14: 0.8094, 15: 0.808, 16: 0.6794, 17: 0.7341, 18: 0.7761, 19: 0.8093, 20: 0.7883, 21: 0.8302, 22: 0.7332, 23: 0.5961, 24: 0.5347, 25: 0.5451, 26: 0.449, 27: 0.4818, 28: 0.3968, 29: 0.3775, 30: 0.2898, 31: 0.1423, 32: 0.0963, 33: 0.0039, 34: 0.0009, 35: 0.0001, 36: 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 399/100000 [09:51<36:01:13,  1.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/100000]\n",
      "Train Loss: 41.3255\n",
      "Val Loss: 43.3288\n",
      "{0: 3.8725, 1: 0.4358, 2: 0.8479, 3: 1.3272, 4: 1.8291, 5: 2.3748, 6: 2.9434, 7: 4.1658, 8: 5.2943, 9: 4.2358, 10: 2.4412, 11: 1.9265, 12: 0.8063, 13: 0.7607, 14: 0.7486, 15: 0.7592, 16: 0.6334, 17: 0.6453, 18: 0.6986, 19: 0.7177, 20: 0.6982, 21: 0.7574, 22: 0.6856, 23: 0.5541, 24: 0.5082, 25: 0.5095, 26: 0.4214, 27: 0.4566, 28: 0.3809, 29: 0.367, 30: 0.2894, 31: 0.1416, 32: 0.092, 33: 0.0021, 34: 0.0006, 35: 0.0001, 36: 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 599/100000 [15:22<36:03:42,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [600/100000]\n",
      "Train Loss: 39.2807\n",
      "Val Loss: 41.8149\n",
      "{0: 4.1397, 1: 0.4336, 2: 0.8546, 3: 1.3674, 4: 1.8533, 5: 2.305, 6: 2.8843, 7: 4.029, 8: 5.0816, 9: 4.1286, 10: 2.3739, 11: 1.8479, 12: 0.7536, 13: 0.706, 14: 0.698, 15: 0.7226, 16: 0.5622, 17: 0.5717, 18: 0.6549, 19: 0.6509, 20: 0.6329, 21: 0.6576, 22: 0.5895, 23: 0.4668, 24: 0.4096, 25: 0.4401, 26: 0.3679, 27: 0.406, 28: 0.363, 29: 0.3566, 30: 0.2785, 31: 0.1336, 32: 0.0901, 33: 0.0024, 34: 0.0012, 35: 0.0003, 36: 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 799/100000 [20:54<35:59:56,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [800/100000]\n",
      "Train Loss: 38.6702\n",
      "Val Loss: 42.4349\n",
      "{0: 5.4021, 1: 0.4523, 2: 0.8887, 3: 1.3269, 4: 1.7663, 5: 2.2407, 6: 2.7953, 7: 3.9614, 8: 5.0403, 9: 4.1548, 10: 2.333, 11: 1.7782, 12: 0.7186, 13: 0.6844, 14: 0.6652, 15: 0.7184, 16: 0.5487, 17: 0.5524, 18: 0.6123, 19: 0.5984, 20: 0.6291, 21: 0.6727, 22: 0.5986, 23: 0.475, 24: 0.4093, 25: 0.442, 26: 0.3862, 27: 0.4172, 28: 0.3414, 29: 0.3432, 30: 0.2669, 31: 0.1187, 32: 0.0887, 33: 0.0057, 34: 0.0013, 35: 0.0005, 36: 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 999/100000 [26:17<36:20:08,  1.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/100000]\n",
      "Train Loss: 36.6787\n",
      "Val Loss: 38.2178\n",
      "{0: 3.1063, 1: 0.4172, 2: 0.8363, 3: 1.2727, 4: 1.7062, 5: 2.1648, 6: 2.7158, 7: 3.8015, 8: 4.8759, 9: 4.0554, 10: 2.2155, 11: 1.7142, 12: 0.6322, 13: 0.6207, 14: 0.6159, 15: 0.6266, 16: 0.4941, 17: 0.4998, 18: 0.6219, 19: 0.5569, 20: 0.5622, 21: 0.6145, 22: 0.5404, 23: 0.415, 24: 0.3667, 25: 0.4036, 26: 0.3058, 27: 0.3707, 28: 0.3083, 29: 0.3137, 30: 0.2573, 31: 0.1174, 32: 0.0918, 33: 0.0003, 34: 0.0002, 35: 0.0, 36: 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1199/100000 [31:40<35:50:41,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1200/100000]\n",
      "Train Loss: 35.3446\n",
      "Val Loss: 37.6746\n",
      "{0: 3.644, 1: 0.3878, 2: 0.7922, 3: 1.1989, 4: 1.5885, 5: 2.0473, 6: 2.6033, 7: 3.7307, 8: 4.8197, 9: 4.0141, 10: 2.1574, 11: 1.6996, 12: 0.6093, 13: 0.5959, 14: 0.5897, 15: 0.614, 16: 0.4683, 17: 0.4869, 18: 0.5739, 19: 0.5375, 20: 0.5428, 21: 0.5736, 22: 0.5174, 23: 0.3953, 24: 0.348, 25: 0.3807, 26: 0.3025, 27: 0.3569, 28: 0.3011, 29: 0.3188, 30: 0.2686, 31: 0.1168, 32: 0.0899, 33: 0.0026, 34: 0.0006, 35: 0.0, 36: 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1341/100000 [36:08<35:46:19,  1.31s/it] "
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "current_lr = lr\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "\n",
    "    tensor_batch = mix_data_to_tensor_batch(virus_da.get_batch(), cellular_da.get_batch(), partition = get_partition_ratio(epoch+1))\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    labels = tensor_batch.taxes\n",
    "    output = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "\n",
    "    batch_loss = 0\n",
    "    for index in tax_vocab_sizes.keys():\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output[str(index)], labels[int(index)])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_loss += loss\n",
    "    \n",
    "    running_loss += batch_loss.item()\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        train_loss = running_loss / val_epoch\n",
    "        val_metrics = evaluate(model)\n",
    "        val_losses = {k: v[\"loss\"] for k, v in val_metrics.items()} \n",
    "        val_loss = sum([entry['loss'] for entry in val_metrics.values()]) \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(val_losses)\n",
    "        \n",
    "        # Create metrics dictionary for saving\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"lr\": current_lr,\n",
    "            \"partition\": get_partition_ratio(epoch+1)\n",
    "        }\n",
    "        for k, v in val_losses.items():\n",
    "            metrics[f\"val loss head {k}\"] = v\n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch + batch_loss.item())\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Reset training metrics\n",
    "        running_loss = 0\n",
    "        \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e11fb7d-2d37-4e03-be2d-52d1faac30b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4777ee-67b9-405f-b633-5e2320984387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cd1b0e-e0e4-4fd8-a680-ccdf0e1918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      " WORLD_SIZE=1 , LOCAL_WORLD_SIZE=1,RANK =0,LOCAL_RANK = 0 \n",
      "../checkpoints/TwoLevelLoss_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "git root error: Cmd('git') failed due to: exit code(128)\n",
      "  cmdline: git rev-parse --show-toplevel\n",
      "  stderr: 'fatal: detected dubious ownership in repository at '/home/aac/Alireza'\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory /home/aac/Alireza'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malirezanor\u001b[0m (\u001b[33malireza_noroozi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20250107_140418-l7jl5dnx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alireza_noroozi/TwoLevelLoss/runs/l7jl5dnx' target=\"_blank\">crimson-brook-4</a></strong> to <a href='https://wandb.ai/alireza_noroozi/TwoLevelLoss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alireza_noroozi/TwoLevelLoss' target=\"_blank\">https://wandb.ai/alireza_noroozi/TwoLevelLoss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alireza_noroozi/TwoLevelLoss/runs/l7jl5dnx' target=\"_blank\">https://wandb.ai/alireza_noroozi/TwoLevelLoss/runs/l7jl5dnx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alireza_noroozi/TwoLevelLoss/runs/l7jl5dnx?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d0a530a1690>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import json\n",
    "from transformers import EsmModel, AutoTokenizer, AutoModel\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from batch import Batch\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 100_000\n",
    "val_epoch = 100\n",
    "num_val = 10\n",
    "batch_size = 64\n",
    "dataset_name = \"corpus_1000_Viruses_cellular\"\n",
    "lr = 0.001\n",
    "model_name = \"TwoLevelLoss\"\n",
    "max_seq_len = 1000\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{dataset_name}\", batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": model_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_szie\": batch_size,\n",
    "        \"max_seq_len\": max_seq_len\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ff77cd-c629-47a6-a31e-2912d4656498",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2name_file = \"../data/taxonomy_index.json\"\n",
    "\n",
    "if os.path.exists(index2name_file):\n",
    "    with open(index2name_file, \"rb\") as f:\n",
    "        index2name = json.load(f)\n",
    "\n",
    "index2name = {k:v for k, v in index2name.items() if k in [\"0\", \"1\"]}\n",
    "\n",
    "tax_vocab_sizes = {\n",
    "    int(k): len(v) + 1 for k,v in index2name.items()\n",
    "}\n",
    "# print(tax_vocab_sizes)\n",
    "\n",
    "level_encoder = {\n",
    "    int(k): {name: idx + 1 for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "level_decoder = {\n",
    "    int(k): {idx + 1: name for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "for k, v in level_decoder.items():\n",
    "    level_decoder[k][0] = \"NOT DEFINED\"\n",
    "\n",
    "\n",
    "def encode_lineage(lineage_str):\n",
    "    taxes_str = lineage_str.split(\", \")\n",
    "\n",
    "    encoded = {int(k): 0 for k in index2name.keys()}\n",
    "    \n",
    "    for i, tax_str in enumerate(taxes_str):\n",
    "        if i <= 1:\n",
    "            encoded[i] = level_encoder[i][tax_str]\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cb1410-5ec8-4c88-854d-be599e54e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESMHead(nn.Module):\n",
    "    def __init__(self, num_classes_dict):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "        for param in self.esm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.heads = nn.ModuleDict()\n",
    "        self.attentions = nn.ModuleDict()\n",
    "\n",
    "        prev_class = 0\n",
    "        for index_name, num_classes in num_classes_dict.items():\n",
    "            self.heads[str(index_name)] = nn.Sequential(\n",
    "                nn.Linear(1280 + prev_class, 512),\n",
    "                nn.LayerNorm(512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(512, num_classes)\n",
    "            )\n",
    "\n",
    "            self.attentions[str(index_name)] = nn.Sequential(\n",
    "                nn.Linear(1280, 256),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(256, 1)\n",
    "            )\n",
    "\n",
    "            prev_class = num_classes\n",
    "        \n",
    "    def attention_pooling(self, x, index):\n",
    "        # x shape: (batch_size, seq_length, embedding_dim)\n",
    "        attention_weights = self.attentions[index](x)  # (batch_size, seq_length, 1)\n",
    "        attention_weights = torch.softmax(attention_weights.squeeze(-1), dim=1)  # (batch_size, seq_length)\n",
    "        attention_weights = attention_weights.unsqueeze(-1)  # (batch_size, seq_length, 1)\n",
    "        pooled = torch.sum(x * attention_weights, dim=1)  # (batch_size, embedding_dim)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = self.esm(x, attention_mask=attention_mask).last_hidden_state\n",
    "        for index, head in self.heads.items():\n",
    "            pooled = self.attention_pooling(x, index)  # Apply attention pooling\n",
    "            if index != \"0\":\n",
    "                current_pooled = torch.cat([pooled, output], dim=-1)\n",
    "            else:\n",
    "                current_pooled = pooled\n",
    "            output = head(current_pooled)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fede89-9b77-4f1e-951b-3f6a2600d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 1.990695 M\n",
      "Total parameters: 654.344636 M\n"
     ]
    }
   ],
   "source": [
    "model = ESMHead(tax_vocab_sizes).to(device)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable parameters: {trainable/ 1e6} M')\n",
    "print(f'Total parameters: {total/ 1e6} M')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "#     optimizer,\n",
    "#     step_size=10,  # Period of learning rate decay\n",
    "#     gamma=0.1  # Multiplicative factor of decay\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d31e550-ed8f-41a5-942f-90d86c3afe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BatchReader:\n",
    "#     def __init__(self, batch_size):\n",
    "#         self.batch_size = batch_size\n",
    "#         self.current_index = 0 \n",
    "#         self.data = pd.read_csv(\"../embeddings/taxonomic_data.csv\")\n",
    "\n",
    "#     def get_batch(self):\n",
    "#         if self.current_index >= len(self.data):\n",
    "#             print(\"No more data available.\")\n",
    "#             return []\n",
    "\n",
    "#         batch_data = self.data.iloc[self.current_index:self.current_index + self.batch_size]\n",
    "#         self.current_index += self.batch_size\n",
    "\n",
    "#         inputs = []\n",
    "#         tax_ids = []\n",
    "#         for _, row in batch_data.iterrows():\n",
    "#             inputs.append(torch.load(f\"../embeddings/tax_esm_embeddings/{row['index']}.pt\"))\n",
    "#             tax_ids.append(encode_lineage(row['Taxonomic_lineage']))\n",
    "    \n",
    "#         inputs = torch.stack(inputs)\n",
    "    \n",
    "#         combined_dict = {}\n",
    "#         for d in tax_ids:\n",
    "#             for key, value in d.items():\n",
    "#                 combined_dict.setdefault(key, []).append(value)\n",
    "\n",
    "#         tensor_encoded = {k: torch.LongTensor(v) for k,v in combined_dict.items()}\n",
    "#         return Batch(inputs, tensor_encoded)\n",
    "\n",
    "\n",
    "# da = BatchReader(batch_size=batch_size)\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# val_batches = [da.get_batch() for _ in range(num_val)]\n",
    "# end_time = time.time()\n",
    "# elapsed_time = end_time - start_time\n",
    "# print(f\"Execution time: {elapsed_time:.6f} seconds\")\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(f\"facebook/esm1b_t33_650M_UR50S\")\n",
    "\n",
    "def data_to_tensor_batch(b, max_seq_len=max_seq_len):\n",
    "\n",
    "    inputs = tokenizer_(\n",
    "        [e['Sequence'] for e in b],\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len\n",
    "    )\n",
    "\n",
    "    tax_ids = [encode_lineage(e['Taxonomic_lineage__ALL_']) for e in b]\n",
    "\n",
    "    combined_dict = {}\n",
    "    for d in tax_ids:\n",
    "        for key, value in d.items():\n",
    "            combined_dict.setdefault(key, []).append(value)\n",
    "\n",
    "    tensor_encoded = {k: torch.LongTensor(v) for k,v in combined_dict.items()}\n",
    "    return Batch(inputs, tensor_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb03ada3-72a3-4e55-a292-7773574ce08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = [da.get_batch() for _ in range(num_val)]\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():\n",
    "            tensor_batch = data_to_tensor_batch(val_batches[epoch], max_seq_len)\n",
    "            tensor_batch.gpu(device)\n",
    "            labels = tensor_batch.taxes\n",
    "            labels = labels[list(labels.keys())[-1]]\n",
    "            \n",
    "        outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "        loss = criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    f1_macro = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')\n",
    "    f1_micro = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')\n",
    "    conf_matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "    avg_loss = running_loss / num_val\n",
    "\n",
    "    return avg_loss, accuracy, f1_micro, f1_macro, conf_matrix\n",
    "\n",
    "# evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba451553-07c8-4608-821a-28078d720548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "def load_checkpoint(model, optimizer=None, scheduler=None):\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pt'))        \n",
    "    # Extract epoch numbers and find latest\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load optimizer state if provided (for training)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Move optimizer state to GPU if necessary\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get training metadata\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "    \n",
    "    print(f\"Successfully loaded checkpoint from epoch {epoch}\")\n",
    "    # print(\"Metrics at checkpoint:\", metrics)\n",
    "    \n",
    "    return model, optimizer, scheduler, epoch, metrics\n",
    "        \n",
    "\n",
    "# model, optimizer, scheduler, latest_epoch, metrics = load_checkpoint(model, optimizer, scheduler)\n",
    "latest_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b487e-69ce-4c41-be78-a1b6dcb58e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 99/100000 [07:32<127:05:32,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100000]\n",
      "Train Loss: 0.6424\n",
      "Val Loss: 0.8306, Val Accuracy: 0.7719\n",
      "Val F1 (micro): 0.7719, Val F1 (macro): 0.1825\n",
      "Val Confusion Matrix:\n",
      "[[ 39   0   0   0  80   0   0]\n",
      " [  3   0   0   0  11   0   0]\n",
      " [  2   0   0   0  13   0   0]\n",
      " [  0   0   0   0   2   0   0]\n",
      " [ 26   0   0   0 455   0   0]\n",
      " [  1   0   0   0   1   0   0]\n",
      " [  0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 199/100000 [16:01<126:44:08,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/100000]\n",
      "Train Loss: 0.2962\n",
      "Val Loss: 1.0210, Val Accuracy: 0.7656\n",
      "Val F1 (micro): 0.7656, Val F1 (macro): 0.1538\n",
      "Val Confusion Matrix:\n",
      "[[ 15   0   0   0 104   0   0]\n",
      " [  0   0   0   0  14   0   0]\n",
      " [  0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   2   0   0]\n",
      " [  6   0   0   0 475   0   0]\n",
      " [  2   0   0   0   0   0   0]\n",
      " [  0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 299/100000 [24:30<126:30:14,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/100000]\n",
      "Train Loss: 0.1406\n",
      "Val Loss: 1.2323, Val Accuracy: 0.7688\n",
      "Val F1 (micro): 0.7688, Val F1 (macro): 0.1317\n",
      "Val Confusion Matrix:\n",
      "[[ 12   0   0   0   7 100   0   0]\n",
      " [  0   0   0   0   1  13   0   0]\n",
      " [  0   0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0 480   0   0]\n",
      " [  2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 399/100000 [32:59<126:22:50,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/100000]\n",
      "Train Loss: 0.1229\n",
      "Val Loss: 1.2201, Val Accuracy: 0.7625\n",
      "Val F1 (micro): 0.7625, Val F1 (macro): 0.1242\n",
      "Val Confusion Matrix:\n",
      "[[  8   0   0   0   6 105   0   0]\n",
      " [  0   0   0   0   1  13   0   0]\n",
      " [  0   0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0 480   0   0]\n",
      " [  1   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 499/100000 [41:29<126:11:20,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/100000]\n",
      "Train Loss: 0.1300\n",
      "Val Loss: 1.0158, Val Accuracy: 0.7688\n",
      "Val F1 (micro): 0.7688, Val F1 (macro): 0.1422\n",
      "Val Confusion Matrix:\n",
      "[[ 19   0   0   0  15  85   0   0]\n",
      " [  0   0   0   0   3  11   0   0]\n",
      " [  0   0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  8   0   0   0   0 473   0   0]\n",
      " [  2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 599/100000 [49:59<126:03:50,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [600/100000]\n",
      "Train Loss: 0.2430\n",
      "Val Loss: 1.3656, Val Accuracy: 0.7672\n",
      "Val F1 (micro): 0.7672, Val F1 (macro): 0.1178\n",
      "Val Confusion Matrix:\n",
      "[[ 12   0   0   0   1  12  94   0   0]\n",
      " [  0   0   0   0   1   3  10   0   0]\n",
      " [  0   0   0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0   1 479   0   0]\n",
      " [  1   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 699/100000 [58:28<125:58:02,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [700/100000]\n",
      "Train Loss: 0.1236\n",
      "Val Loss: 1.3164, Val Accuracy: 0.7688\n",
      "Val F1 (micro): 0.7688, Val F1 (macro): 0.1349\n",
      "Val Confusion Matrix:\n",
      "[[ 13   0   0   0  18  88   0   0]\n",
      " [  0   0   0   0   5   9   0   0]\n",
      " [  0   0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   1 479   0   0]\n",
      " [  1   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 799/100000 [1:06:57<125:48:12,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [800/100000]\n",
      "Train Loss: 0.0627\n",
      "Val Loss: 1.2201, Val Accuracy: 0.7719\n",
      "Val F1 (micro): 0.7719, Val F1 (macro): 0.1444\n",
      "Val Confusion Matrix:\n",
      "[[ 21   0   0   0   8  90   0   0]\n",
      " [  0   0   0   0   1  13   0   0]\n",
      " [  0   0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  8   0   0   0   0 473   0   0]\n",
      " [  2   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 899/100000 [1:15:26<125:39:22,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [900/100000]\n",
      "Train Loss: 0.1093\n",
      "Val Loss: 1.1847, Val Accuracy: 0.7625\n",
      "Val F1 (micro): 0.7625, Val F1 (macro): 0.1146\n",
      "Val Confusion Matrix:\n",
      "[[ 34   3   0   2   0   0  13  67   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   4   0   0   1   7   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  14   0   1   0]\n",
      " [  0   0   0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0]\n",
      " [ 24   0   0   0   0   0   1 454   0   2   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   7   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 999/100000 [1:23:54<125:39:28,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/100000]\n",
      "Train Loss: 0.1206\n",
      "Val Loss: 1.3157, Val Accuracy: 0.7672\n",
      "Val F1 (micro): 0.7672, Val F1 (macro): 0.1300\n",
      "Val Confusion Matrix:\n",
      "[[ 11   0   0   0   8 100   0   0]\n",
      " [  0   0   0   0   1  13   0   0]\n",
      " [  0   0   0   0   0  15   0   0]\n",
      " [  0   0   0   0   0   2   0   0]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0 480   0   0]\n",
      " [  1   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1099/100000 [1:32:21<125:24:40,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1100/100000]\n",
      "Train Loss: 0.0672\n",
      "Val Loss: 1.2068, Val Accuracy: 0.7703\n",
      "Val F1 (micro): 0.7703, Val F1 (macro): 0.1136\n",
      "Val Confusion Matrix:\n",
      "[[ 19   0   0   0   0   9  91   0   0   0]\n",
      " [  0   0   0   0   0   1  13   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  15   0   0   0]\n",
      " [  0   0   1   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  6   0   0   0   0   0 474   0   1   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   7   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1199/100000 [1:40:50<125:39:15,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1200/100000]\n",
      "Train Loss: 0.2755\n",
      "Val Loss: 1.2789, Val Accuracy: 0.7609\n",
      "Val F1 (micro): 0.7609, Val F1 (macro): 0.1316\n",
      "Val Confusion Matrix:\n",
      "[[ 22   4   0   0   0  23  70   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   4   0   0   0   4   6   0   0]\n",
      " [  0   0   0   0   0   0  15   0   0]\n",
      " [  0   1   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  7   2   0   0   0   7 465   0   0]\n",
      " [  2   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   6   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1299/100000 [1:49:19<125:20:40,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1300/100000]\n",
      "Train Loss: 0.1012\n",
      "Val Loss: 1.3074, Val Accuracy: 0.7594\n",
      "Val F1 (micro): 0.7594, Val F1 (macro): 0.1294\n",
      "Val Confusion Matrix:\n",
      "[[ 22   0   0   0   0  21  76   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   0   5   8   0   0]\n",
      " [  0   0   0   0   0   0  15   0   0]\n",
      " [  0   1   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [ 13   0   0   0   0   4 464   0   0]\n",
      " [  2   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   1   6   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1399/100000 [1:57:49<124:59:01,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1400/100000]\n",
      "Train Loss: 0.2339\n",
      "Val Loss: 1.2792, Val Accuracy: 0.7672\n",
      "Val F1 (micro): 0.7672, Val F1 (macro): 0.1205\n",
      "Val Confusion Matrix:\n",
      "[[ 25   0   0   0   0  13  76   0   5   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3   6   0   5   0]\n",
      " [  0   0   0   0   0   0  14   0   1   0]\n",
      " [  0   1   0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [ 11   0   0   0   0   1 466   0   3   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   7   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1499/100000 [2:06:17<124:59:26,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1500/100000]\n",
      "Train Loss: 0.0691\n",
      "Val Loss: 1.3962, Val Accuracy: 0.7719\n",
      "Val F1 (micro): 0.7719, Val F1 (macro): 0.1221\n",
      "Val Confusion Matrix:\n",
      "[[ 15   0   0   0   0  10  94   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   3  11   0   0]\n",
      " [  0   0   0   0   0   0  15   0   0]\n",
      " [  0   1   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  2   0   0   0   0   0 479   0   0]\n",
      " [  1   0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   0   0   0   7   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1599/100000 [2:14:46<124:45:00,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1600/100000]\n",
      "Train Loss: 0.0998\n",
      "Val Loss: 1.6517, Val Accuracy: 0.6375\n",
      "Val F1 (micro): 0.6375, Val F1 (macro): 0.1153\n",
      "Val Confusion Matrix:\n",
      "[[ 26   5   0   0   0  48  39   0   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   3   0   0   0   7   4   0   0   0]\n",
      " [  0   0   0   0   0   1  12   0   2   0]\n",
      " [  0   1   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [ 14   2   0   0   0  81 382   0   2   0]\n",
      " [  2   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   5   2   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1699/100000 [2:23:15<124:59:51,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1700/100000]\n",
      "Train Loss: 0.2225\n",
      "Val Loss: 2.0615, Val Accuracy: 0.6469\n",
      "Val F1 (micro): 0.6469, Val F1 (macro): 0.0927\n",
      "Val Confusion Matrix:\n",
      "[[  5   0   0   0   0  60  54   0   0   0]\n",
      " [  0   0   1   0   0   9   4   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  14   0   1   0]\n",
      " [  0   0   1   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  72 409   0   0   0]\n",
      " [  0   0   0   0   0   1   1   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   4   3   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1774/100000 [2:29:50<124:45:31,  4.57s/it]"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "current_lr = lr\n",
    "\n",
    "for epoch in tqdm(range(latest_epoch, latest_epoch + epochs)):\n",
    "    model.train()\n",
    "    \n",
    "    tensor_batch = da.get_batch()\n",
    "    tensor_batch = data_to_tensor_batch(tensor_batch, max_seq_len)\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    labels = tensor_batch.taxes\n",
    "    labels = labels[list(labels.keys())[-1]]\n",
    "    \n",
    "    outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        train_loss = running_loss / val_epoch\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_accuracy, val_f1_micro, val_f1_macro, val_cm = evaluate(model)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Val F1 (micro): {val_f1_micro:.4f}, Val F1 (macro): {val_f1_macro:.4f}\")\n",
    "        print(\"Val Confusion Matrix:\")\n",
    "        print(val_cm)\n",
    "        \n",
    "        # Create metrics dictionary for saving\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\":val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_f1_micro\": val_f1_micro,\n",
    "            \"val_f1_macro\": val_f1_macro,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"lr\": current_lr\n",
    "        }\n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            # 'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Step the scheduler\n",
    "        # scheduler.step(epoch + loss.item())\n",
    "        # current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Reset training metrics\n",
    "        running_loss = 0\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38a288-ea78-424e-9f90-995c9b435655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _, latest_epoch, metrics = load_checkpoint(model)\n",
    "\n",
    "val_batches_ = [da.get_batch() for _ in range(num_val)]\n",
    "\n",
    "\n",
    "input_sequences_ = [\"ACACAD\"]\n",
    "labels_ = [{0: 1}]\n",
    "\n",
    "def evaluate_df(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    df = {\n",
    "        \"sequence\": [],\n",
    "        \"label\": [],\n",
    "        \"pred\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": 0,\n",
    "        \"accuracy\": 0,\n",
    "        \"f1 macro\": 0,\n",
    "        \"f1 micro\": 0\n",
    "    }\n",
    "    \n",
    "    # Process each sequence\n",
    "    for sequence, label in zip(input_sequences_, labels_):\n",
    "        inputs = tokenizer_(\n",
    "            [sequence],\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len\n",
    "        ).to(device)\n",
    "    \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "        pred = output.argmax(dim=-1).cpu().item()\n",
    "        loss = criterion(output, torch.tensor([label[0]]).to(device))\n",
    "        df[\"sequence\"].append(sequence)\n",
    "        df[\"label\"].append(level_decoder[0][label[0]])\n",
    "        df[\"pred\"].append(level_decoder[0][pred])\n",
    "        df[\"loss\"].append(round(loss.cpu().item(), 4))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    new_df = pd.DataFrame(df)\n",
    "    new_df['is_incorrect'] = new_df['label'] != new_df['pred']\n",
    "    new_df = new_df.sort_values(['is_incorrect', 'loss'], ascending=[False, False])\n",
    "    new_df.to_csv(f'classification_results__new_att.csv', index=False)\n",
    "\n",
    "    metrics[\"loss\"] = np.array(df[\"loss\"]).mean()\n",
    "    metrics[\"accuracy\"] = accuracy_score(np.array(df[\"label\"]), np.array(df[\"pred\"]))\n",
    "    metrics[\"f1 macro\"] = f1_score(np.array(df[\"label\"]), np.array(df[\"pred\"]), average='macro')  # F1-score for multi-label classification\n",
    "    metrics[\"f1 micro\"] = f1_score(np.array(df[\"label\"]), np.array(df[\"pred\"]), average='micro') \n",
    "    print(metrics)\n",
    "\n",
    "evaluate_df(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb06be5-7f3f-4771-802d-086267ec2df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

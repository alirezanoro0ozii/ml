{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a30dd4f-031e-4586-8bdb-1b51e7ca0010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "../checkpoints/tokenizer_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from data_access import PQDataAccess\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "batch_size = 32\n",
    "da = PQDataAccess(\"/home/aac/Alireza/datasets/taxseq/corpus_1000\", batch_size)\n",
    "epochs= 10_000\n",
    "val_epoch = 50\n",
    "num_val = 25\n",
    "\n",
    "model_name = \"tokenizer\" # \"FNN\", \"hierarchy\", \"T5\"\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "from data_process import *\n",
    "\n",
    "from models.TokenizerClassifier import TokenizerClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95492264-fb61-44a5-b6d3-bec74aaede4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: 8.021248 M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from models.TokenizerClassifier import TokenizerClassifier\n",
    "\n",
    "model = TokenizerClassifier(output_dim=len_tokenizer, max_tax_len=max_tax_len).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print(\"model:\", sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41b8560c-0989-45e1-a340-01bf9c40cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from data_process import *\n",
    "\n",
    "def train_step(model, optimizer, da, device):\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get batch and convert to tensor\n",
    "    tensor_batch = tokenizer_data_to_tensor_batch(da.get_batch())\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    prediction = model(tensor_batch.seq_ids)\n",
    "    labels = tensor_batch.taxes\n",
    "    \n",
    "    loss = nn.BCEWithLogitsLoss()(prediction, labels.float())\n",
    "        \n",
    "    # Backward pass and optimization step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68091252-059a-4f2d-b7b9-a29755201c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from data_process import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def evaluate(model, da, device, num_val_batches):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "        for _ in range(num_val_batches):\n",
    "            tensor_batch = tokenizer_data_to_tensor_batch(da.get_batch())\n",
    "            tensor_batch.gpu(device)\n",
    "            \n",
    "            prediction = model(tensor_batch.seq_ids)\n",
    "            label = tensor_batch.taxes\n",
    "            \n",
    "            loss = nn.BCEWithLogitsLoss()(prediction, label.float())\n",
    "            total_loss += loss\n",
    "\n",
    "            all_preds.append(prediction)\n",
    "            all_labels.append(label)\n",
    "\n",
    "    all_preds = torch.cat(all_preds).cpu()\n",
    "    all_labels = torch.cat(all_labels).cpu()\n",
    "    \n",
    "    thresholds = np.arange(0.0, 1.0, 0.1)\n",
    "    best_threshold = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    # Iterate over thresholds to find the one with the highest F1 score\n",
    "    for threshold in thresholds:\n",
    "        predicted_classes = (torch.sigmoid(all_preds) > threshold).int()\n",
    "        f1 = f1_score(all_labels, predicted_classes, average='micro')\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predicted_classes = (torch.sigmoid(all_preds) > best_threshold).int()\n",
    "    # print(all_labels)\n",
    "    # print(predicted_classes)\n",
    "    # print(all_labels.shape)\n",
    "    # print(predicted_classes.shape)\n",
    "    f1 = f1_score(all_labels, predicted_classes, average='micro')\n",
    "    # accuracy = accuracy_score(all_labels, predicted_classes)\n",
    "\n",
    "    accuracy = (predicted_classes == all_labels).sum().item() / predicted_classes.numel() \n",
    "\n",
    "    # Average losses\n",
    "    val_loss = total_loss / num_val_batches\n",
    "    \n",
    "    model.train()  # Set the model back to training mode\n",
    "    return val_loss, accuracy, f1, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d018a47-e2e0-4313-a257-9db08d161e9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0024, device='cuda:0'),\n",
       " 0.9993202941785074,\n",
       " 0.5497072683135799,\n",
       " 0.30000000000000004)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, da, device, num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fd4d07-5735-438e-9a08-1dafcc487cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_step(model, optimizer, da, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        val_loss, acc, f1, tresh = evaluate(model, da, device, num_val)\n",
    "        \n",
    "        mean_train_loss = sum(train_losses[-val_epoch:]) / val_epoch\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {mean_train_loss:.4f}, Val Loss: {val_loss:.4f}, val acc: {acc:.4f}, val f1: {f1:.4f}\")\n",
    "    \n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_step_{epoch + 1}.pt\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'accuracy': acc,\n",
    "            'f1_score': f1,\n",
    "            'tresh': tresh\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7e31c35-12e7-4883-93b6-7bd6ecdf6391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "len_tokenizer = len(tokenizer.vocab)\n",
    "\n",
    "def encode_lineage_tokenizer(tax_lineage):\n",
    "    return tokenizer.encode(tax_lineage.split(\", \"), add_special_tokens=False, padding='max_length', max_length=max_tax_len, is_split_into_words= True)\n",
    "\n",
    "def tokenizer_data_to_tensor_batch(b):\n",
    "    # if model_name in [\"new_hierarchy\", \"hierarchy\"]:\n",
    "    sequences = [encode_sequence(e['sequence']) for e in b]\n",
    "    tax_ids = [encode_lineage_tokenizer(e['tax_lineage']) for e in b]\n",
    "    encoded_list = [[1 if _ in tax_id else 0 for _ in range(len_tokenizer)] for tax_id in tax_ids]\n",
    "\n",
    "    return Batch(torch.LongTensor(sequences), torch.LongTensor(encoded_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fc682a-6729-43d1-bb9e-2721e53d18ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(checkpoint_dir, model, specific=None):\n",
    "    # List all checkpoint files and sort them by step number\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_step_\")]\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints found in directory.\")\n",
    "        return None\n",
    "\n",
    "    # Find the latest checkpoint based on step number\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]), reverse=True)\n",
    "    if specific is None:\n",
    "        latest_checkpoint_path = os.path.join(checkpoint_dir, checkpoints[0])\n",
    "    else:\n",
    "        latest_checkpoint_path = os.path.join(checkpoint_dir, specific)\n",
    "\n",
    "    # Load the checkpoint\n",
    "    checkpoint = torch.load(latest_checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    train_loss = checkpoint['train_loss']\n",
    "    val_loss = checkpoint['val_loss']\n",
    "    accuracy = checkpoint['accuracy']\n",
    "    f1_score = checkpoint['f1_score']\n",
    "    tresh = checkpoint['tresh']\n",
    "\n",
    "    print(f\"Loaded checkpoint from epoch {epoch+1}\")\n",
    "    \n",
    "    return {\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1_score,\n",
    "        \"tresh\": tresh\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "034768cb-cc6f-44ad-ad94-1438a102168a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint from epoch 5150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'epoch': 5149,\n",
       " 'train_loss': 0.005232478026300669,\n",
       " 'val_loss': tensor(0.0050, device='cuda:0'),\n",
       " 'accuracy': 0.9980904693750862,\n",
       " 'f1_score': 0.36409836771609455,\n",
       " 'tresh': 0.2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_seq = \"MKRLRPSDKFFELLGYKPHHVQLAIHRSTAKRRVACLGRQSGKSEAASVEAVFELFARPGSQGWIIAPTYDQAEIIFGRVVEKVERLSEVFPTTEVQLQRRRLRLLVHHYDRPVNAPGAKRVATSEFRGKSADRPDNLRGATLDFVILDEAAMIPFSVWSEAIEPTLSVRDGWALIISTPKGLNWFYEFFLMGWRGGLKEGIPNSGINQTHPDFESFHAASWDVWPERREWYMERRLYIPDLEFRQEYGAEFVSHSNSVFSGLDMLILLPYERRGTRLVVEDYRPDHIYCIGADFGKNQDYSVFSVLDLDTGAIACLERMNGATWSDQVARLKALSEDYGHAYVVADTWGVGDAIAEELDAQGINYTPLPVKSSSVKEQLISNLALLMEKGQVAVPNDKTILDELRNFRYYRTASGNQVMRAYGRGHDDIVMSLALAYSQYEGKDGYKFELAEERPSKLKHEESVMSLVEDDFTDLELANRAFSA\"\n",
    "tax_lineage = \"cellular organisms, Bacteria, Pseudomonadota, Betaproteobacteria, unclassified Betaproteobacteria, Betaproteobacteria bacterium GR16-43\"\n",
    "\n",
    "model = TokenizerClassifier(output_dim=len_tokenizer, max_tax_len=max_tax_len).to(device)\n",
    "latest_checkpoint = load_latest_checkpoint(checkpoint_dir, model)\n",
    "latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a69d8982-c683-4474-ac29-31401081f1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 113,\n",
       " 114,\n",
       " 118,\n",
       " 119,\n",
       " 139,\n",
       " 140,\n",
       " 142,\n",
       " 144,\n",
       " 153,\n",
       " 155,\n",
       " 156,\n",
       " 159,\n",
       " 188,\n",
       " 1116,\n",
       " 1161,\n",
       " 1162,\n",
       " 1182,\n",
       " 1183,\n",
       " 1279,\n",
       " 1361,\n",
       " 1372,\n",
       " 1465,\n",
       " 1566,\n",
       " 1643,\n",
       " 1776,\n",
       " 1777,\n",
       " 1810,\n",
       " 1874,\n",
       " 2083,\n",
       " 2093,\n",
       " 2173,\n",
       " 2822,\n",
       " 2897,\n",
       " 3052,\n",
       " 4527,\n",
       " 4559,\n",
       " 5096,\n",
       " 5114,\n",
       " 6112,\n",
       " 6140,\n",
       " 6766,\n",
       " 8209,\n",
       " 8362,\n",
       " 8814,\n",
       " 9126,\n",
       " 10548,\n",
       " 10961,\n",
       " 11179,\n",
       " 12023,\n",
       " 12658,\n",
       " 12809,\n",
       " 12985,\n",
       " 14391,\n",
       " 14521,\n",
       " 15447,\n",
       " 15540,\n",
       " 16386,\n",
       " 18484,\n",
       " 18757,\n",
       " 18882,\n",
       " 19415,\n",
       " 19810,\n",
       " 19890,\n",
       " 19891,\n",
       " 21020,\n",
       " 23916,\n",
       " 25857,\n",
       " 26503,\n",
       " 26918]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor = torch.LongTensor([encode_sequence(test_seq)]).to(device)\n",
    "output = model(input_tensor)\n",
    "predicted_labels = (torch.sigmoid(output) > 0.2).int()\n",
    "indexes = [i for i, _ in enumerate(predicted_labels[0]) if _]\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d102c079-b651-4212-9392-dd21ad7ed045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] ( ) -. B C E G P R S V ssaeiyesus groupiatepisttadareterce Actbalestesmyino Prohozoutebraeo unceae Op bacteriaazact organismsukaoboa cellular Neoumeosteriarda Baomi Met Terrakon Gramroteclassifiedcterryoleo'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29999875-8ed1-4486-9164-16fdf96b313e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

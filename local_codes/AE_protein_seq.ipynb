{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 4096\n",
    "max_iters = 2001\n",
    "eval_interval = 200\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 50\n",
    "mask_prob = .15\n",
    "\n",
    "hidden_sizes = [64, 128]\n",
    "latent_size = 256  # Compressed vector size\n",
    "input_size = 22  # Length of the input sequence\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9A2X-AHbCECI",
    "outputId": "c1d7a09b-a916-42cb-fb8a-083ac6a38df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20398 #seqs\n",
      "11151556 tokens\n",
      "4981 max len\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"uniprotkb_length_TO_5000_AND_reviewed_t_2024_09_20 (1).tsv\")\n",
    "seqs = [str(v[0]).split('\\t')[1] for v in df.values[1:]]\n",
    "\n",
    "print(len(seqs), \"#seqs\")\n",
    "print(sum([len(s) for s in seqs]), \"tokens\")\n",
    "print(max([len(s) for s in seqs]), \"max len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\".join(seqs)\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text))) + ['<MASK>']\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "def encode(sequence):\n",
    "    \"\"\"One-hot encode a protein sequence.\"\"\"\n",
    "    one_hot = np.zeros((len(sequence), vocab_size))\n",
    "    for i, aa in enumerate(sequence):\n",
    "        one_hot[i, stoi[aa]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def decode(one_hot_seq):\n",
    "    \"\"\"Decode a one-hot encoded protein sequence.\"\"\"\n",
    "    return \"\".join([itos[idx.item()] for idx in np.argmax(one_hot_seq, axis=1)])\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.float)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    return torch.stack([data[i:i+block_size] for i in ix]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(input_batch):\n",
    "    batch_size, block_size, input_dim = input_batch.shape\n",
    "    \n",
    "    # Create a mask with the same shape as the batch\n",
    "    mask = (torch.rand(batch_size, block_size) < mask_prob).to(device)\n",
    "\n",
    "    mask_token = torch.zeros(input_dim).to(device)  # You can customize this to a unique token\n",
    "    mask_token[-1] = 1\n",
    "\n",
    "    # Apply the mask to the batch (replace masked vectors with the mask token)\n",
    "    masked_batch = input_batch.clone().to(device)\n",
    "    masked_batch[mask] = mask_token  \n",
    "    \n",
    "    return masked_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=22, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=22, bias=True)\n",
       "    (5): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Autoencoder model\n",
    "class SequenceAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, latent_size):\n",
    "        super(SequenceAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_size = input_size\n",
    "        for h in hidden_sizes:\n",
    "            encoder_layers.append(nn.Linear(prev_size, h))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            prev_size = h\n",
    "        encoder_layers.append(nn.Linear(prev_size, latent_size))  # Final layer to latent space\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder\n",
    "        decoder_layers = []\n",
    "        prev_size = latent_size\n",
    "        for h in reversed(hidden_sizes):\n",
    "            decoder_layers.append(nn.Linear(prev_size, h))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            prev_size = h\n",
    "        decoder_layers.append(nn.Linear(prev_size, input_size))  # Final layer to reconstruct input\n",
    "        decoder_layers.append(nn.Softmax(dim=1))  # For one-hot encoded data\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compress the sequence\n",
    "        encoded = self.encoder(x)\n",
    "        # Reconstruct the sequence\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "model = SequenceAutoencoder(input_size, hidden_sizes, latent_size).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # Mean squared error loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    loss_out = {}\n",
    "    acc_out = {}\n",
    "    \n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        total_correct = 0\n",
    "        total_elements = 0\n",
    "        \n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X = get_batch(split)\n",
    "            new_X = model(X)\n",
    "            loss = criterion(new_X, X)\n",
    "            \n",
    "            losses[k] = loss.item()\n",
    "            \n",
    "            for j in range(len(X)):\n",
    "                pred_seq = new_X[j].argmax(dim=-1)  # Predicted sequence using argmax\n",
    "                actual_seq = X[j].argmax(dim=-1)  # Ground truth sequence using argmax (if needed)\n",
    "                correct = (pred_seq == actual_seq).sum().item()  # Count the number of correct predictions\n",
    "                total_correct += correct\n",
    "                total_elements += len(X[j])\n",
    "            \n",
    "        loss_out[split] = losses.mean()\n",
    "        acc_out[split] = total_correct / total_elements\n",
    "        \n",
    "    model.train()\n",
    "    return loss_out, acc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XhVgZotk_NxY",
    "outputId": "86f2cc7f-05fe-4c16-8a70-c004ef66ae9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 0.0497, val loss 0.0497, train acc 0.0138, val acc 0.0157\n",
      "step 200: train loss 0.0410, val loss 0.0409, train acc 0.5967, val acc 0.6030\n",
      "step 400: train loss 0.0259, val loss 0.0265, train acc 0.9258, val acc 0.9190\n",
      "step 600: train loss 0.0216, val loss 0.0217, train acc 0.9657, val acc 0.9644\n",
      "step 800: train loss 0.0198, val loss 0.0199, train acc 0.9891, val acc 0.9885\n",
      "step 1000: train loss 0.0182, val loss 0.0185, train acc 0.9988, val acc 0.9991\n",
      "step 1200: train loss 0.0177, val loss 0.0179, train acc 1.0000, val acc 0.9996\n",
      "step 1400: train loss 0.0177, val loss 0.0175, train acc 1.0000, val acc 0.9998\n",
      "step 1600: train loss 0.0173, val loss 0.0175, train acc 0.9990, val acc 0.9994\n",
      "step 1800: train loss 0.0173, val loss 0.0174, train acc 0.9999, val acc 1.0000\n",
      "step 2000: train loss 0.0175, val loss 0.0174, train acc 0.9999, val acc 1.0000\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0:\n",
    "        losses, accs = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, train acc {accs['train']:.4f}, val acc {accs['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    x = get_batch('train')\n",
    "    masked_x = mask(x)\n",
    "    # evaluate the loss\n",
    "    new_x = model(masked_x)\n",
    "    loss = criterion(new_x, x)\n",
    "    \n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split: train\n",
      "0 SDPQRNFK\n",
      "0 SDPQRNFK\n",
      "===========\n",
      "1 APASPQPP\n",
      "1 APASPQPP\n",
      "===========\n",
      "2 ERDGDRRL\n",
      "2 ERDGDRRL\n",
      "===========\n",
      "3 TASTRGFY\n",
      "3 TASTRGFY\n",
      "===========\n",
      "4 ICPIPKEV\n",
      "4 ICPIPKEV\n",
      "===========\n",
      "5 QFSSYVGR\n",
      "5 QFSSYVGR\n",
      "===========\n",
      "6 PEGPRGAA\n",
      "6 PEGPRGAA\n",
      "===========\n",
      "7 VCPLSWFG\n",
      "7 VCPLSWFG\n",
      "===========\n",
      "8 VYYVGDTF\n",
      "8 VYYVGDTF\n",
      "===========\n",
      "9 LIKARALN\n",
      "9 LIKARALN\n",
      "===========\n",
      "10 KFQLLVQQ\n",
      "10 KFQLLVQQ\n",
      "===========\n",
      "11 EVSSQGRE\n",
      "11 EVSSQGRE\n",
      "===========\n",
      "12 RFMPEPNL\n",
      "12 RFMPEPNL\n",
      "===========\n",
      "13 PAVAESAV\n",
      "13 PAVAESAV\n",
      "===========\n",
      "14 QPTEDNIH\n",
      "14 QPTEDNIH\n",
      "===========\n",
      "15 QHLRIHLG\n",
      "15 QHLRIHLG\n",
      "===========\n",
      "16 MVRMKSMF\n",
      "16 MVRMKSMF\n",
      "===========\n",
      "17 TEALSMAH\n",
      "17 TEALSMAH\n",
      "===========\n",
      "18 GSTVQSVD\n",
      "18 GSTVQSVD\n",
      "===========\n",
      "19 SPGKQASS\n",
      "19 SPGKQASS\n",
      "===========\n",
      "20 DGQRVKLK\n",
      "20 DGQRVKLK\n",
      "===========\n",
      "21 TDLSFDSQ\n",
      "21 TDLSFDSQ\n",
      "===========\n",
      "22 GPGTVCES\n",
      "22 GPGTVCES\n",
      "===========\n",
      "23 DLAEYFRL\n",
      "23 DLAEYFRL\n",
      "===========\n",
      "24 DYSPPLHK\n",
      "24 DYSPPLHK\n",
      "===========\n",
      "25 PVQEELSV\n",
      "25 PVQEELSV\n",
      "===========\n",
      "26 QQEHFVLS\n",
      "26 QQEHFVLS\n",
      "===========\n",
      "27 TEDIRERV\n",
      "27 TEDIRERV\n",
      "===========\n",
      "28 QISGGFLV\n",
      "28 QISGGFLV\n",
      "===========\n",
      "29 RIIKHQKM\n",
      "29 RIIKHQKM\n",
      "===========\n",
      "30 IEPDGAEL\n",
      "30 IEPDGAEL\n",
      "===========\n",
      "31 VNAYSHKF\n",
      "31 VNAYSHKF\n",
      "===========\n",
      "split: val\n",
      "0 GALCAGLG\n",
      "0 GALCAGLG\n",
      "===========\n",
      "1 IPVEQLML\n",
      "1 IPVEQLML\n",
      "===========\n",
      "2 AEKLRKME\n",
      "2 AEKLRKME\n",
      "===========\n",
      "3 RDSGTNAQ\n",
      "3 RDSGTNAQ\n",
      "===========\n",
      "4 PLPAHYRS\n",
      "4 PLPAHYRS\n",
      "===========\n",
      "5 SGAGRATA\n",
      "5 SGAGRATA\n",
      "===========\n",
      "6 EAVAFVVP\n",
      "6 EAVAFVVP\n",
      "===========\n",
      "7 LGDRIITP\n",
      "7 LGDRIITP\n",
      "===========\n",
      "8 PGLAPPQK\n",
      "8 PGLAPPQK\n",
      "===========\n",
      "9 EPGRIQHP\n",
      "9 EPGRIQHP\n",
      "===========\n",
      "10 FKWSSNLT\n",
      "10 FKWSSNLT\n",
      "===========\n",
      "11 VPGSGLSG\n",
      "11 VPGSGLSG\n",
      "===========\n",
      "12 LPTHDSDS\n",
      "12 LPTHDSDS\n",
      "===========\n",
      "13 YQETLIVW\n",
      "13 YQETLIVW\n",
      "===========\n",
      "14 KTIALPAS\n",
      "14 KTIALPAS\n",
      "===========\n",
      "15 NSLFEELK\n",
      "15 NSLFEELK\n",
      "===========\n",
      "16 TKTEKIES\n",
      "16 TKTEKIES\n",
      "===========\n",
      "17 WPKRIHTT\n",
      "17 WPKRIHTT\n",
      "===========\n",
      "18 VIENCICA\n",
      "18 VIENCICA\n",
      "===========\n",
      "19 LKLACGGD\n",
      "19 LKLACGGD\n",
      "===========\n",
      "20 QKFLQQNS\n",
      "20 QKFLQQNS\n",
      "===========\n",
      "21 NIIEIFEN\n",
      "21 NIIEIFEN\n",
      "===========\n",
      "22 EQHIRRRL\n",
      "22 EQHIRRRL\n",
      "===========\n",
      "23 SSLSNHQR\n",
      "23 SSLSNHQR\n",
      "===========\n",
      "24 LLMYLVIL\n",
      "24 LLMYLVIL\n",
      "===========\n",
      "25 QKTIEMKR\n",
      "25 QKTIEMKR\n",
      "===========\n",
      "26 MSFALDNL\n",
      "26 MSFALDNL\n",
      "===========\n",
      "27 EPGSSPPW\n",
      "27 EPGSSPPW\n",
      "===========\n",
      "28 GTSPTEQL\n",
      "28 GTSPTEQL\n",
      "===========\n",
      "29 FTGFSKDG\n",
      "29 FTGFSKDG\n",
      "===========\n",
      "30 PQPQEEQS\n",
      "30 PQPQEEQS\n",
      "===========\n",
      "31 ATLWYAPL\n",
      "31 ATLWYAPL\n",
      "===========\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def show_construction():\n",
    "    for split in ['train', 'val']:\n",
    "        print('split:', split)\n",
    "        X = get_batch(split)\n",
    "        new_X = model(X)\n",
    "        for j in range(len(X)):\n",
    "            print(j, decode(new_X[j].cpu()))\n",
    "            print(j, decode(X[j].cpu()))\n",
    "            print('===========')\n",
    "show_construction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

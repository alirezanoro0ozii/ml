{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cd1b0e-e0e4-4fd8-a680-ccdf0e1918a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      " WORLD_SIZE=1 , LOCAL_WORLD_SIZE=1,RANK =0,LOCAL_RANK = 0 \n",
      "../checkpoints/OnlyFirst_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malirezanor\u001b[0m (\u001b[33malireza_noroozi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aac/Alireza/local_codes/wandb/run-20241128_165812-h3ty8eo7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alireza_noroozi/OnlyFirst/runs/h3ty8eo7' target=\"_blank\">lemon-valley-8</a></strong> to <a href='https://wandb.ai/alireza_noroozi/OnlyFirst' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alireza_noroozi/OnlyFirst' target=\"_blank\">https://wandb.ai/alireza_noroozi/OnlyFirst</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alireza_noroozi/OnlyFirst/runs/h3ty8eo7' target=\"_blank\">https://wandb.ai/alireza_noroozi/OnlyFirst/runs/h3ty8eo7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/alireza_noroozi/OnlyFirst/runs/h3ty8eo7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x774430547070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import sys, os, math\n",
    "import wandb\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "import json\n",
    "from transformers import EsmModel, AutoTokenizer\n",
    "\n",
    "sys.path.insert(0, '../dlp')\n",
    "from batch import Batch\n",
    "\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "epochs = 100_000\n",
    "val_epoch = 10\n",
    "num_val = 10\n",
    "batch_size = 64\n",
    "virus_dataset_name = \"corpus_1000_Viruses\"\n",
    "cellular_dataset_name = \"corpus_1000_cellular\"\n",
    "lr = 0.001\n",
    "model_name = \"OnlyFirst\"\n",
    "max_seq_len = 1000\n",
    "\n",
    "from data_access import PQDataAccess\n",
    "virus_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{virus_dataset_name}\", batch_size)\n",
    "cellular_da = PQDataAccess(f\"/home/aac/Alireza/datasets/export_pqt_4_taxseq_new/{cellular_dataset_name}\", batch_size)\n",
    "\n",
    "checkpoint_dir = f\"../checkpoints/{model_name}_checkpoints\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "print(checkpoint_dir)\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=model_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": model_name,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch_szie\": batch_size,\n",
    "        \"max_seq_len\": max_seq_len\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2cb1410-5ec8-4c88-854d-be599e54e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM1b(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.esm = EsmModel.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")\n",
    "        \n",
    "        # Freeze ESM parameters\n",
    "        for param in self.esm.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(1280, 256),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.layer1 = nn.Linear(1280, 512)\n",
    "        self.layer_norm = nn.LayerNorm(512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.layer2 = nn.Linear(512, 3)\n",
    "\n",
    "    def attention_pooling(self, x):\n",
    "        # x shape: (batch_size, seq_length, embedding_dim)\n",
    "        attention_weights = self.attention(x)  # (batch_size, seq_length, 1)\n",
    "        attention_weights = torch.softmax(attention_weights.squeeze(-1), dim=1)  # (batch_size, seq_length)\n",
    "        attention_weights = attention_weights.unsqueeze(-1)  # (batch_size, seq_length, 1)\n",
    "        pooled = torch.sum(x * attention_weights, dim=1)  # (batch_size, embedding_dim)\n",
    "        return pooled\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        outputs = self.esm(x, attention_mask=attention_mask).last_hidden_state\n",
    "        outputs = self.attention_pooling(outputs)\n",
    "        \n",
    "        outputs = self.layer1(outputs)\n",
    "        outputs = self.layer_norm(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.dropout(outputs)\n",
    "        outputs = self.layer2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37fede89-9b77-4f1e-951b-3f6a2600d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm1b_t33_650M_UR50S and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 0.986628 M\n",
      "Total parameters: 653.343129 M\n",
      "ESM1b(\n",
      "  (esm): EsmModel(\n",
      "    (embeddings): EsmEmbeddings(\n",
      "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
      "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
      "    )\n",
      "    (encoder): EsmEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-32): 33 x EsmLayer(\n",
      "          (attention): EsmAttention(\n",
      "            (self): EsmSelfAttention(\n",
      "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): EsmSelfOutput(\n",
      "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (intermediate): EsmIntermediate(\n",
      "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          )\n",
      "          (output): EsmOutput(\n",
      "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (pooler): EsmPooler(\n",
      "      (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "    (contact_head): EsmContactPredictionHead(\n",
      "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
      "      (activation): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (attention): Sequential(\n",
      "    (0): Linear(in_features=1280, out_features=256, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (layer1): Linear(in_features=1280, out_features=512, bias=True)\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (layer2): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ESM1b().to(device)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable parameters: {trainable/ 1e6} M')\n",
    "print(f'Total parameters: {total/ 1e6} M')\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Cosine annealing with warm restarts\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer,\n",
    "    T_0=10,  # Initial restart interval\n",
    "    T_mult=2,  # Multiply interval by 2 after each restart\n",
    "    eta_min=1e-6  # Minimum learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d31e550-ed8f-41a5-942f-90d86c3afe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index2name_file = \"../data/taxonomy_index.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(index2name_file):\n",
    "    with open(index2name_file, \"rb\") as f:\n",
    "        index2name = json.load(f)\n",
    "\n",
    "tax_vocab_sizes = {\n",
    "    int(k): len(v) for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "# print(tax_vocab_sizes)\n",
    "# # Print tax_vocab_sizes sorted by value (number of taxa per rank)\n",
    "# sorted_sizes = dict(sorted(tax_vocab_sizes.items(), key=lambda x: x[1], reverse=True))\n",
    "# print(\"\\nTaxonomic ranks sorted by number of taxa:\")\n",
    "# for rank, size in sorted_sizes.items():\n",
    "#     print(f\"{rank}: {size}\")\n",
    "\n",
    "level_encoder = {\n",
    "    int(k): {name: idx + 1 for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "level_decoder = {\n",
    "    int(k): {idx + 1: name for idx, name in enumerate(v)} for k,v in index2name.items()\n",
    "}\n",
    "\n",
    "for k, v in level_decoder.items():\n",
    "    level_decoder[k][0] = \"NOT DEFINED\"\n",
    "\n",
    "\n",
    "def encode_lineage(lineage_str):\n",
    "    taxes_str = lineage_str.split(\", \")\n",
    "\n",
    "    encoded = {int(k): 0 for k in index2name.keys()}\n",
    "    \n",
    "    for i, tax_str in enumerate(taxes_str):\n",
    "        encoded[i] = level_encoder[i][tax_str]\n",
    "\n",
    "    return encoded\n",
    "\n",
    "tokenizer_ = AutoTokenizer.from_pretrained(f\"facebook/esm1b_t33_650M_UR50S\")\n",
    "\n",
    "def mix_data_to_tensor_batch(b_virues, b_cellular, max_seq_len=max_seq_len, partition=0.25):\n",
    "    if partition == -1:\n",
    "        b = b_virues + b_cellular\n",
    "    else:\n",
    "        split_point = int(len(b_virues) * partition)\n",
    "        b = b_virues[:split_point] + b_cellular[-len(b_virues) + split_point:]\n",
    "        random.shuffle(b)  # In-place shuffle\n",
    "    \n",
    "    inputs = tokenizer_(\n",
    "        [e['Sequence'] for e in b],\n",
    "        return_tensors=\"pt\", \n",
    "        padding='max_length', \n",
    "        truncation=True, \n",
    "        max_length=max_seq_len\n",
    "    )\n",
    "\n",
    "    tax_ids = [encode_lineage(e['Taxonomic_lineage__ALL_']) for e in b]\n",
    "    combined_dict = {}\n",
    "    for d in tax_ids:\n",
    "        for key, value in d.items():\n",
    "            combined_dict.setdefault(key, []).append(value)\n",
    "\n",
    "    tensor_encoded = {k: torch.LongTensor(v) for k,v in combined_dict.items()}\n",
    "    return Batch(inputs, tensor_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb03ada3-72a3-4e55-a292-7773574ce08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dir = f\"val_results/{model_name}\"\n",
    "if not os.path.exists(val_dir):\n",
    "    os.makedirs(val_dir)\n",
    "    \n",
    "val_batches = [virus_da.get_batch() for _ in range(num_val)]\n",
    "cell_val_batches = [cellular_da.get_batch() for _ in range(num_val)]\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for epoch in range(num_val):\n",
    "        with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "            tensor_batch = mix_data_to_tensor_batch(val_batches[epoch], cell_val_batches[epoch], max_seq_len, partition=-1)\n",
    "            tensor_batch.gpu(device)\n",
    "        \n",
    "            labels = tensor_batch.taxes[0]\n",
    "            outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "    \n",
    "            running_loss += loss.item()\n",
    "                \n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "    \n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    # Concatenate all batches into single tensors\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    # Compute evaluation metrics (example: accuracy, F1 score)\n",
    "    accuracy = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
    "    f1_macro = f1_score(all_labels.numpy(), all_preds.numpy(), average='macro')  # F1-score for multi-label classification\n",
    "    f1_micro = f1_score(all_labels.numpy(), all_preds.numpy(), average='micro')  # F1-score for multi-label classification\n",
    "    conf_matrix = confusion_matrix(all_labels.numpy(), all_preds.numpy())\n",
    "    avg_loss = running_loss / num_val\n",
    "    \n",
    "    return avg_loss, accuracy, f1_micro, f1_macro, conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba451553-07c8-4608-821a-28078d720548",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint from epoch 229\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "def load_checkpoint(model, optimizer=None, scheduler=None):\n",
    "    checkpoints = glob.glob(os.path.join(checkpoint_dir, 'checkpoint_epoch_*.pt'))        \n",
    "    # Extract epoch numbers and find latest\n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "    checkpoint = torch.load(latest_checkpoint)\n",
    "    \n",
    "    # Load model state\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Load optimizer state if provided (for training)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Move optimizer state to GPU if necessary\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "    if scheduler is not None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "    # Get training metadata\n",
    "    epoch = checkpoint['epoch']\n",
    "    metrics = checkpoint['metrics']\n",
    "    \n",
    "    print(f\"Successfully loaded checkpoint from epoch {epoch}\")\n",
    "    # print(\"Metrics at checkpoint:\", metrics)\n",
    "    \n",
    "    return model, optimizer, scheduler, epoch, metrics\n",
    "        \n",
    "\n",
    "model, optimizer, scheduler, latest_epoch, metrics = load_checkpoint(model, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b894a867-bf97-4d90-9092-17acfa8cf6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition_ratio(epoch, decay_epochs=100000):\n",
    "    \"\"\"\n",
    "    Calculate partition ratio that decreases from 8/16 to 1/16 in steps\n",
    "    \"\"\"\n",
    "    # Calculate how many epochs before each step down\n",
    "    epochs_per_step = decay_epochs // 7  # 7 steps from 8/16 down to 1/16\n",
    "    \n",
    "    # Calculate current step based on epoch\n",
    "    step = min(epoch // epochs_per_step, 7)  # Max 7 steps down from 8\n",
    "    \n",
    "    # Map step to fraction\n",
    "    fraction = (8 - step) / 16\n",
    "    \n",
    "    return fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413b487e-69ce-4c41-be78-a1b6dcb58e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/100000]\n",
      "Train Loss: 0.0212\n",
      "Val Loss: 0.2465, Val Accuracy: 0.9047\n",
      "Val F1 (micro): 0.9047, Val F1 (macro): 0.9041\n",
      "Val Confusion Matrix:\n",
      "[[530 110]\n",
      " [ 12 628]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/100000 [02:11<149:37:16,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/100000]\n",
      "Train Loss: 0.2307\n",
      "Val Loss: 0.2489, Val Accuracy: 0.9023\n",
      "Val F1 (micro): 0.9023, Val F1 (macro): 0.9022\n",
      "Val Confusion Matrix:\n",
      "[[601  39]\n",
      " [ 86 554]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/100000 [04:25<150:15:38,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/100000]\n",
      "Train Loss: 0.2191\n",
      "Val Loss: 0.2130, Val Accuracy: 0.9125\n",
      "Val F1 (micro): 0.9125, Val F1 (macro): 0.9123\n",
      "Val Confusion Matrix:\n",
      "[[553  87]\n",
      " [ 25 615]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/100000 [06:38<150:16:08,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/100000]\n",
      "Train Loss: 0.2123\n",
      "Val Loss: 0.2108, Val Accuracy: 0.9164\n",
      "Val F1 (micro): 0.9164, Val F1 (macro): 0.9164\n",
      "Val Confusion Matrix:\n",
      "[[576  64]\n",
      " [ 43 597]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/100000 [08:51<150:20:20,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/100000]\n",
      "Train Loss: 0.2065\n",
      "Val Loss: 0.2086, Val Accuracy: 0.9109\n",
      "Val F1 (micro): 0.9109, Val F1 (macro): 0.9109\n",
      "Val Confusion Matrix:\n",
      "[[565  75]\n",
      " [ 39 601]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/100000 [11:04<149:47:03,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [280/100000]\n",
      "Train Loss: 0.1871\n",
      "Val Loss: 0.2079, Val Accuracy: 0.9117\n",
      "Val F1 (micro): 0.9117, Val F1 (macro): 0.9117\n",
      "Val Confusion Matrix:\n",
      "[[566  74]\n",
      " [ 39 601]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 60/100000 [13:17<150:12:24,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [290/100000]\n",
      "Train Loss: 0.1854\n",
      "Val Loss: 0.2083, Val Accuracy: 0.9180\n",
      "Val F1 (micro): 0.9180, Val F1 (macro): 0.9179\n",
      "Val Confusion Matrix:\n",
      "[[574  66]\n",
      " [ 39 601]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 70/100000 [15:29<149:36:07,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/100000]\n",
      "Train Loss: 0.1911\n",
      "Val Loss: 0.2081, Val Accuracy: 0.9180\n",
      "Val F1 (micro): 0.9180, Val F1 (macro): 0.9179\n",
      "Val Confusion Matrix:\n",
      "[[574  66]\n",
      " [ 39 601]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 80/100000 [17:41<149:41:06,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [310/100000]\n",
      "Train Loss: 0.2336\n",
      "Val Loss: 0.2077, Val Accuracy: 0.9164\n",
      "Val F1 (micro): 0.9164, Val F1 (macro): 0.9164\n",
      "Val Confusion Matrix:\n",
      "[[572  68]\n",
      " [ 39 601]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 90/100000 [19:53<149:35:03,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [320/100000]\n",
      "Train Loss: 0.2065\n",
      "Val Loss: 0.2077, Val Accuracy: 0.9164\n",
      "Val F1 (micro): 0.9164, Val F1 (macro): 0.9164\n",
      "Val Confusion Matrix:\n",
      "[[572  68]\n",
      " [ 39 601]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 100/100000 [22:06<149:53:30,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [330/100000]\n",
      "Train Loss: 0.1917\n",
      "Val Loss: 0.2120, Val Accuracy: 0.9141\n",
      "Val F1 (micro): 0.9141, Val F1 (macro): 0.9141\n",
      "Val Confusion Matrix:\n",
      "[[580  60]\n",
      " [ 50 590]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 110/100000 [24:17<149:31:45,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [340/100000]\n",
      "Train Loss: 0.1793\n",
      "Val Loss: 0.2095, Val Accuracy: 0.9133\n",
      "Val F1 (micro): 0.9133, Val F1 (macro): 0.9131\n",
      "Val Confusion Matrix:\n",
      "[[556  84]\n",
      " [ 27 613]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 120/100000 [26:29<149:25:33,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [350/100000]\n",
      "Train Loss: 0.2192\n",
      "Val Loss: 0.1974, Val Accuracy: 0.9211\n",
      "Val F1 (micro): 0.9211, Val F1 (macro): 0.9210\n",
      "Val Confusion Matrix:\n",
      "[[573  67]\n",
      " [ 34 606]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 130/100000 [28:41<149:34:25,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [360/100000]\n",
      "Train Loss: 0.1879\n",
      "Val Loss: 0.2057, Val Accuracy: 0.9125\n",
      "Val F1 (micro): 0.9125, Val F1 (macro): 0.9125\n",
      "Val Confusion Matrix:\n",
      "[[580  60]\n",
      " [ 52 588]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 140/100000 [30:55<150:03:44,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [370/100000]\n",
      "Train Loss: 0.1932\n",
      "Val Loss: 0.2109, Val Accuracy: 0.9141\n",
      "Val F1 (micro): 0.9141, Val F1 (macro): 0.9139\n",
      "Val Confusion Matrix:\n",
      "[[561  79]\n",
      " [ 31 609]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 150/100000 [33:08<149:46:04,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [380/100000]\n",
      "Train Loss: 0.1995\n",
      "Val Loss: 0.2022, Val Accuracy: 0.9164\n",
      "Val F1 (micro): 0.9164, Val F1 (macro): 0.9163\n",
      "Val Confusion Matrix:\n",
      "[[561  79]\n",
      " [ 28 612]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 160/100000 [35:20<149:47:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [390/100000]\n",
      "Train Loss: 0.2479\n",
      "Val Loss: 0.1961, Val Accuracy: 0.9148\n",
      "Val F1 (micro): 0.9148, Val F1 (macro): 0.9148\n",
      "Val Confusion Matrix:\n",
      "[[566  74]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 170/100000 [37:32<149:19:38,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/100000]\n",
      "Train Loss: 0.1799\n",
      "Val Loss: 0.2058, Val Accuracy: 0.9172\n",
      "Val F1 (micro): 0.9172, Val F1 (macro): 0.9172\n",
      "Val Confusion Matrix:\n",
      "[[592  48]\n",
      " [ 58 582]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 180/100000 [39:44<149:18:21,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [410/100000]\n",
      "Train Loss: 0.2095\n",
      "Val Loss: 0.1956, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9187\n",
      "Val Confusion Matrix:\n",
      "[[571  69]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 190/100000 [41:55<149:10:22,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [420/100000]\n",
      "Train Loss: 0.1721\n",
      "Val Loss: 0.2040, Val Accuracy: 0.9164\n",
      "Val F1 (micro): 0.9164, Val F1 (macro): 0.9162\n",
      "Val Confusion Matrix:\n",
      "[[557  83]\n",
      " [ 24 616]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 200/100000 [44:08<149:38:25,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [430/100000]\n",
      "Train Loss: 0.2186\n",
      "Val Loss: 0.2191, Val Accuracy: 0.9055\n",
      "Val F1 (micro): 0.9055, Val F1 (macro): 0.9053\n",
      "Val Confusion Matrix:\n",
      "[[603  37]\n",
      " [ 84 556]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 210/100000 [46:20<149:35:37,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [440/100000]\n",
      "Train Loss: 0.2218\n",
      "Val Loss: 0.1941, Val Accuracy: 0.9164\n",
      "Val F1 (micro): 0.9164, Val F1 (macro): 0.9164\n",
      "Val Confusion Matrix:\n",
      "[[575  65]\n",
      " [ 42 598]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 220/100000 [48:33<149:38:39,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [450/100000]\n",
      "Train Loss: 0.1920\n",
      "Val Loss: 0.1942, Val Accuracy: 0.9148\n",
      "Val F1 (micro): 0.9148, Val F1 (macro): 0.9147\n",
      "Val Confusion Matrix:\n",
      "[[561  79]\n",
      " [ 30 610]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 230/100000 [50:45<149:56:46,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [460/100000]\n",
      "Train Loss: 0.1882\n",
      "Val Loss: 0.1947, Val Accuracy: 0.9148\n",
      "Val F1 (micro): 0.9148, Val F1 (macro): 0.9148\n",
      "Val Confusion Matrix:\n",
      "[[570  70]\n",
      " [ 39 601]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 240/100000 [52:58<149:39:27,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [470/100000]\n",
      "Train Loss: 0.2246\n",
      "Val Loss: 0.1968, Val Accuracy: 0.9141\n",
      "Val F1 (micro): 0.9141, Val F1 (macro): 0.9141\n",
      "Val Confusion Matrix:\n",
      "[[579  61]\n",
      " [ 49 591]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 250/100000 [55:09<149:17:58,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [480/100000]\n",
      "Train Loss: 0.1634\n",
      "Val Loss: 0.1913, Val Accuracy: 0.9195\n",
      "Val F1 (micro): 0.9195, Val F1 (macro): 0.9195\n",
      "Val Confusion Matrix:\n",
      "[[569  71]\n",
      " [ 32 608]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 260/100000 [57:21<149:10:50,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [490/100000]\n",
      "Train Loss: 0.1883\n",
      "Val Loss: 0.1969, Val Accuracy: 0.9156\n",
      "Val F1 (micro): 0.9156, Val F1 (macro): 0.9156\n",
      "Val Confusion Matrix:\n",
      "[[582  58]\n",
      " [ 50 590]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 270/100000 [59:33<149:21:51,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/100000]\n",
      "Train Loss: 0.2055\n",
      "Val Loss: 0.1903, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9186\n",
      "Val Confusion Matrix:\n",
      "[[565  75]\n",
      " [ 29 611]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 280/100000 [1:01:46<150:20:19,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [510/100000]\n",
      "Train Loss: 0.2053\n",
      "Val Loss: 0.1886, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9187\n",
      "Val Confusion Matrix:\n",
      "[[578  62]\n",
      " [ 42 598]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 290/100000 [1:03:58<149:59:42,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [520/100000]\n",
      "Train Loss: 0.1999\n",
      "Val Loss: 0.1882, Val Accuracy: 0.9227\n",
      "Val F1 (micro): 0.9227, Val F1 (macro): 0.9227\n",
      "Val Confusion Matrix:\n",
      "[[587  53]\n",
      " [ 46 594]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 300/100000 [1:06:13<153:11:03,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [530/100000]\n",
      "Train Loss: 0.1656\n",
      "Val Loss: 0.1863, Val Accuracy: 0.9195\n",
      "Val F1 (micro): 0.9195, Val F1 (macro): 0.9195\n",
      "Val Confusion Matrix:\n",
      "[[571  69]\n",
      " [ 34 606]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 310/100000 [1:08:27<154:18:43,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [540/100000]\n",
      "Train Loss: 0.2138\n",
      "Val Loss: 0.1856, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9187\n",
      "Val Confusion Matrix:\n",
      "[[576  64]\n",
      " [ 40 600]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 320/100000 [1:10:41<153:51:47,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [550/100000]\n",
      "Train Loss: 0.1943\n",
      "Val Loss: 0.1860, Val Accuracy: 0.9203\n",
      "Val F1 (micro): 0.9203, Val F1 (macro): 0.9203\n",
      "Val Confusion Matrix:\n",
      "[[573  67]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 330/100000 [1:12:56<151:24:33,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [560/100000]\n",
      "Train Loss: 0.2218\n",
      "Val Loss: 0.1858, Val Accuracy: 0.9211\n",
      "Val F1 (micro): 0.9211, Val F1 (macro): 0.9211\n",
      "Val Confusion Matrix:\n",
      "[[576  64]\n",
      " [ 37 603]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 340/100000 [1:15:09<149:28:26,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [570/100000]\n",
      "Train Loss: 0.1779\n",
      "Val Loss: 0.1858, Val Accuracy: 0.9219\n",
      "Val F1 (micro): 0.9219, Val F1 (macro): 0.9218\n",
      "Val Confusion Matrix:\n",
      "[[578  62]\n",
      " [ 38 602]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 350/100000 [1:17:23<154:28:13,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [580/100000]\n",
      "Train Loss: 0.1814\n",
      "Val Loss: 0.1856, Val Accuracy: 0.9203\n",
      "Val F1 (micro): 0.9203, Val F1 (macro): 0.9203\n",
      "Val Confusion Matrix:\n",
      "[[574  66]\n",
      " [ 36 604]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 360/100000 [1:19:37<149:47:51,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [590/100000]\n",
      "Train Loss: 0.1917\n",
      "Val Loss: 0.1854, Val Accuracy: 0.9195\n",
      "Val F1 (micro): 0.9195, Val F1 (macro): 0.9195\n",
      "Val Confusion Matrix:\n",
      "[[572  68]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 370/100000 [1:21:49<150:23:42,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [600/100000]\n",
      "Train Loss: 0.1648\n",
      "Val Loss: 0.1854, Val Accuracy: 0.9195\n",
      "Val F1 (micro): 0.9195, Val F1 (macro): 0.9195\n",
      "Val Confusion Matrix:\n",
      "[[572  68]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 380/100000 [1:24:03<154:08:51,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [610/100000]\n",
      "Train Loss: 0.2176\n",
      "Val Loss: 0.1853, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9187\n",
      "Val Confusion Matrix:\n",
      "[[571  69]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 390/100000 [1:26:19<154:40:44,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [620/100000]\n",
      "Train Loss: 0.1812\n",
      "Val Loss: 0.1853, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9187\n",
      "Val Confusion Matrix:\n",
      "[[571  69]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 400/100000 [1:28:37<155:23:53,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [630/100000]\n",
      "Train Loss: 0.1906\n",
      "Val Loss: 0.1853, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9187\n",
      "Val Confusion Matrix:\n",
      "[[571  69]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 410/100000 [1:30:51<154:28:00,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [640/100000]\n",
      "Train Loss: 0.1710\n",
      "Val Loss: 0.1853, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9187\n",
      "Val Confusion Matrix:\n",
      "[[571  69]\n",
      " [ 35 605]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 420/100000 [1:33:05<150:11:05,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [650/100000]\n",
      "Train Loss: 0.1703\n",
      "Val Loss: 0.1928, Val Accuracy: 0.9211\n",
      "Val F1 (micro): 0.9211, Val F1 (macro): 0.9211\n",
      "Val Confusion Matrix:\n",
      "[[583  57]\n",
      " [ 44 596]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 430/100000 [1:35:20<154:08:54,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [660/100000]\n",
      "Train Loss: 0.2660\n",
      "Val Loss: 0.2073, Val Accuracy: 0.9172\n",
      "Val F1 (micro): 0.9172, Val F1 (macro): 0.9172\n",
      "Val Confusion Matrix:\n",
      "[[594  46]\n",
      " [ 60 580]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 440/100000 [1:37:36<154:54:08,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [670/100000]\n",
      "Train Loss: 0.2146\n",
      "Val Loss: 0.2118, Val Accuracy: 0.9148\n",
      "Val F1 (micro): 0.9148, Val F1 (macro): 0.9148\n",
      "Val Confusion Matrix:\n",
      "[[594  46]\n",
      " [ 63 577]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 450/100000 [1:39:51<154:20:33,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [680/100000]\n",
      "Train Loss: 0.2118\n",
      "Val Loss: 0.1884, Val Accuracy: 0.9203\n",
      "Val F1 (micro): 0.9203, Val F1 (macro): 0.9202\n",
      "Val Confusion Matrix:\n",
      "[[570  70]\n",
      " [ 32 608]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 460/100000 [1:42:07<154:53:54,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [690/100000]\n",
      "Train Loss: 0.1840\n",
      "Val Loss: 0.1874, Val Accuracy: 0.9234\n",
      "Val F1 (micro): 0.9234, Val F1 (macro): 0.9233\n",
      "Val Confusion Matrix:\n",
      "[[567  73]\n",
      " [ 25 615]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 470/100000 [1:44:22<150:51:05,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [700/100000]\n",
      "Train Loss: 0.1994\n",
      "Val Loss: 0.1881, Val Accuracy: 0.9187\n",
      "Val F1 (micro): 0.9187, Val F1 (macro): 0.9187\n",
      "Val Confusion Matrix:\n",
      "[[585  55]\n",
      " [ 49 591]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 480/100000 [1:46:38<154:37:20,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [710/100000]\n",
      "Train Loss: 0.1812\n",
      "Val Loss: 0.2006, Val Accuracy: 0.9203\n",
      "Val F1 (micro): 0.9203, Val F1 (macro): 0.9203\n",
      "Val Confusion Matrix:\n",
      "[[594  46]\n",
      " [ 56 584]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 490/100000 [1:48:53<152:20:59,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [720/100000]\n",
      "Train Loss: 0.2150\n",
      "Val Loss: 0.1843, Val Accuracy: 0.9266\n",
      "Val F1 (micro): 0.9266, Val F1 (macro): 0.9265\n",
      "Val Confusion Matrix:\n",
      "[[583  57]\n",
      " [ 37 603]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 500/100000 [1:51:08<154:20:17,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [730/100000]\n",
      "Train Loss: 0.2007\n",
      "Val Loss: 0.1814, Val Accuracy: 0.9258\n",
      "Val F1 (micro): 0.9258, Val F1 (macro): 0.9258\n",
      "Val Confusion Matrix:\n",
      "[[585  55]\n",
      " [ 40 600]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 510/100000 [1:53:23<154:27:34,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [740/100000]\n",
      "Train Loss: 0.1980\n",
      "Val Loss: 0.1816, Val Accuracy: 0.9234\n",
      "Val F1 (micro): 0.9234, Val F1 (macro): 0.9234\n",
      "Val Confusion Matrix:\n",
      "[[572  68]\n",
      " [ 30 610]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 520/100000 [1:55:36<152:04:13,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [750/100000]\n",
      "Train Loss: 0.1685\n",
      "Val Loss: 0.1878, Val Accuracy: 0.9258\n",
      "Val F1 (micro): 0.9258, Val F1 (macro): 0.9257\n",
      "Val Confusion Matrix:\n",
      "[[566  74]\n",
      " [ 21 619]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 530/100000 [1:57:52<154:33:06,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [760/100000]\n",
      "Train Loss: 0.1876\n",
      "Val Loss: 0.1892, Val Accuracy: 0.9234\n",
      "Val F1 (micro): 0.9234, Val F1 (macro): 0.9234\n",
      "Val Confusion Matrix:\n",
      "[[573  67]\n",
      " [ 31 609]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 540/100000 [2:00:07<151:42:07,  5.49s/it]"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "current_lr = lr\n",
    "\n",
    "for epoch in tqdm(range(latest_epoch, latest_epoch + epochs)):\n",
    "    model.train()\n",
    "\n",
    "    current_partition = get_partition_ratio(epoch)\n",
    "    \n",
    "    tensor_batch = mix_data_to_tensor_batch(\n",
    "        virus_da.get_batch(),\n",
    "        cellular_da.get_batch(),\n",
    "        max_seq_len,\n",
    "        partition=current_partition\n",
    "    )\n",
    "    tensor_batch.gpu(device)\n",
    "    \n",
    "    labels = tensor_batch.taxes[0]\n",
    "    outputs = model(tensor_batch.seq_ids['input_ids'], tensor_batch.seq_ids['attention_mask'])\n",
    "    \n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    if (epoch + 1) % val_epoch == 0:\n",
    "        train_loss = running_loss / val_epoch\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_accuracy, val_f1_micro, val_f1_macro, val_cm = evaluate(model)\n",
    "        \n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"Val F1 (micro): {val_f1_micro:.4f}, Val F1 (macro): {val_f1_macro:.4f}\")\n",
    "        print(\"Val Confusion Matrix:\")\n",
    "        print(val_cm)\n",
    "        \n",
    "        # Create metrics dictionary for saving\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_f1_micro\": val_f1_micro,\n",
    "            \"val_f1_macro\": val_f1_macro,\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"current_portion\": current_partition,\n",
    "            \"lr\": current_lr\n",
    "        }\n",
    "\n",
    "        # Save periodic checkpoint\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pt')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        # Log to wandb\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step(epoch + loss.item())\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        # Reset training metrics\n",
    "        running_loss = 0\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa38a288-ea78-424e-9f90-995c9b435655",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, _, latest_epoch, metrics = load_checkpoint(model)\n",
    "\n",
    "val_batches_ = [virus_da.get_batch() for _ in range(num_val // 2)] + [cellular_da.get_batch() for _ in range(num_val // 2)]\n",
    "\n",
    "# input_sequences_ = [e['Sequence'] for b in val_batches_ for e in b]\n",
    "# labels_ = [encode_lineage(e['Taxonomic_lineage__ALL_'])  for b in val_batches_ for e in b]\n",
    "\n",
    "input_sequences_ = [\"ACACAD\"]\n",
    "labels_ = [{0: 1}]\n",
    "\n",
    "def evaluate_df(model):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    df = {\n",
    "        \"sequence\": [],\n",
    "        \"label\": [],\n",
    "        \"pred\": [],\n",
    "        \"loss\": []\n",
    "    }\n",
    "\n",
    "    metrics = {\n",
    "        \"loss\": 0,\n",
    "        \"accuracy\": 0,\n",
    "        \"f1 macro\": 0,\n",
    "        \"f1 micro\": 0\n",
    "    }\n",
    "    \n",
    "    # Process each sequence\n",
    "    for sequence, label in zip(input_sequences_, labels_):\n",
    "        inputs = tokenizer_(\n",
    "            [sequence],\n",
    "            return_tensors=\"pt\",\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len\n",
    "        ).to(device)\n",
    "    \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "        pred = output.argmax(dim=-1).cpu().item()\n",
    "        loss = criterion(output, torch.tensor([label[0]]).to(device))\n",
    "        df[\"sequence\"].append(sequence)\n",
    "        df[\"label\"].append(level_decoder[0][label[0]])\n",
    "        df[\"pred\"].append(level_decoder[0][pred])\n",
    "        df[\"loss\"].append(round(loss.cpu().item(), 4))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    new_df = pd.DataFrame(df)\n",
    "    new_df['is_incorrect'] = new_df['label'] != new_df['pred']\n",
    "    new_df = new_df.sort_values(['is_incorrect', 'loss'], ascending=[False, False])\n",
    "    new_df.to_csv(f'classification_results__new_att.csv', index=False)\n",
    "\n",
    "    metrics[\"loss\"] = np.array(df[\"loss\"]).mean()\n",
    "    metrics[\"accuracy\"] = accuracy_score(np.array(df[\"label\"]), np.array(df[\"pred\"]))\n",
    "    metrics[\"f1 macro\"] = f1_score(np.array(df[\"label\"]), np.array(df[\"pred\"]), average='macro')  # F1-score for multi-label classification\n",
    "    metrics[\"f1 micro\"] = f1_score(np.array(df[\"label\"]), np.array(df[\"pred\"]), average='micro') \n",
    "    print(metrics)\n",
    "\n",
    "evaluate_df(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb06be5-7f3f-4771-802d-086267ec2df6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
